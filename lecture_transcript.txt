 So, you are using my word also. So, online students, are you able to see the green model? So, some formulations, so they are going to be on the screen on the slide right. So, let us motivate about the speech synthesis. I am sure Dr. Chai has already talked about speech synthesis and then did actually classes, but we will go a little bit in detail here. So, in speech synthesis there are two major parts. The first part inspired by like you can get idea from this particular video. I am speaking English, but I am not in the bowl, but I am with Ola Pallare. I can speak French. Those are just a few examples of the different languages that I can speak here. So, now remember when our prime minister once actually used in Koder and decoder to speak in like give a life telecast in multiple languages in Talon, Thaleru, Kandara and all other languages. He does not know either of them, probably right, when he can speak so fluently neither of them, but then in the life telecast he was speaking, probably he was not probably, he was speaking in Hindi, but in the life telecast was happening in multiple languages. So, in almost near real time life telecast was happening and this is like an example that one can speak in near real time in different languages. So, you may not need to know the language. Another example is suppose I am traveling to Japan, I do not know Japanese, but if I have to communicate, currently what happens when you go to Japan or China somewhere, then you use actually your phone and then do that conversation with the native people over there, those who may not know English there. So, you speak in English and then you use a TTS model or speak synthesis model to convert into Japanese. They speak in Japanese, they understand from there, they speak in Japanese and then the TTS model gives or this synthesis model gives we back the English version, right. Now, the field has advanced much much more and here is the second part. So, in the previous part the idea was that I am speaking in English and I am getting translated in Hindi, right, or French or Spanish. It was speech to speech synthesis. The other could be you know that whose image is this? Where is the CMO? Care and not care. So, Tamil language. Tamil language, exactly. So, but the point here is that he is no more. In the Indian elections, while this is an example of defake, but in Indian elections his video was circulated, okay, giving a particular message, okay. So, what would have happened? Let me run the video and then you can guess, okay. It's very long. Video, but then you got the idea, right. Now, the person is no more. We have persons, a lot of video clips and audio signals. I can we can learn actually some how actually someone would actually speak anything. Now, what I can do is if I have using foundation models or using any of these modern AI algorithms, I can type in text whatever I want him to say, I have a trained model and that that trained model takes this text and then pass all this information converts into a voice signal, a waveform, okay. So, this is text to speech. So, that's a second kind of speech synthesis part, okay. So, birds eye view, essentially we have very prevalent examples of the second part when we are providing the text and we are generating the audio sound, okay. While both examples I have videos, but then just for the time being don't look at videos, it's audio is also that like equally important and they are all lip sync and so on. So, now if you have to generate such a software, such a tool, how will we go about it? Simpler is text to speech, slightly tougher is speech to speech and we will talk about both, so if you start with text to speech, the very first thing is we need to have a mapping, some sort of mapping, some sort of understanding of the text word and the speech word. So, unless you are able to see my board, write the time, what I write something. So, if this is my text space, this is my feature space, okay and this is all audio voice, right. What I need actually, I need can I convert from here to here and from here to here. So, I need to learn a mapping broadly, I need to learn a mapping, between text, T1 and speech, in such a way that T1 equal to n times S1, right. So, I need to have, I need to learn a mapping function which can convert my text embeddings into my speech embeddings and from the speech embedding, I can do a decoding and generate a speech out, right. So, if we have let us say a speech, any speech encoder like Risham has taught some of those speech encoders, right. So, once we have the embedding, I can convert that into user decoder to decode it, right. So, so far clear, now what are the challenges, which challenges, what are the challenges, it is not that straightforward. See, if we talk about any into embeddings like text to text. So, okay let us let us with respect to NLU, topic. NLU is not my area, but at least from the very basic fundamental concept. If I have line, this is what text in English, okay. And I have a text in Hindi, right. I can learn very simple mapping between some mapping, between E2H, so that if I provide a text in Hindi, I can generate text in English by processing by mapping. So, it is like a dictionary, okay. So, if we do that, right. So, in the English dictionary, we all have done in our childhood days, this is my English word, what is the Hindi part of it, right or vice versa. So, so if we can use, so this mapping is nothing but that dictionary. So, in text, we have done it many times. Now, if you have to do this in audio, the points like that, what is what is the duration of the audio clip, right. How someone is actually the accent of the person, how someone is speaking, right. And even many intricate details, for instance, how someone is phasing the sentences. And then when you are speaking, right, when we speak, we actually utter a lot of, these kinds of words. And it also happens if your English is not very fluent or the language, the second language is not very fluent, if it is not a mother term, then you have some errors in the translation, because at the back of your mind, you have also a dictionary, God gives the dictionary given, right, in brain. What you are doing essentially, you are speaking in Hindi in your brain and then that dictionary is converting and that after the conversion, when you are speaking, right, broadly. So, so that may have some errors, because your dictionary, some of that translation may not be absolutely correct. So, those errors may also come in, while your Hindi is completely fine, but your English translation may not be very fine. So, how to minimize, so then essentially, this whole thing comes under from the words to utterances. We have to take care of all these variations and that is where the mapping comes into picture. Let how to ensure that this mapping of different accent variations, different style variations can also be understood and mapped, while the correctness also is equally important. So, this is another view, one of the views which actually a paper has talked about, where you have a text module, you give it to your embedding layer, like the Anelu layer, Anelu layer gives you the linguistic features, the embedding essentially. And then from there, you do some procedure modeling, you actually do the, you go through the entire process of generating the waveform and tapetrol is essentially the text to meld, MFCC coefficient extraction. So, if you have a text and you convert that into an MFCC coefficient, the model has been trained essentially that is a dictionary. So, so, the particular style of creating that dictionary, where we are converting everything into a meld frequency spectrum. And then you have a vocoder like a wave net or a wave glow, you can use any of your favorite vocoder which is essentially converts into, it is like a voice decoder and that is the short form of vocoder is essentially short form of voice decoder. So, you decode into a voice signal, where you actually do the end to end synthesis of a voice signal from where input is your text. So, this is the bird's ivy. So, this is the first part of, or the one of the parts of speech synthesis. Any questions so far? Online guys, if not then I will follow it. And I hope I am audible and everything is going good. Okay. So, so, one of the things that we are also working in the similar domain, but we have just added a few steps of corrections. So, what we have essentially done is speech to speech or what we have been trying to do is essentially speech to speech. I think Risham has taught you guys, right? So, he has been working on this. So, what we do is like, suppose I am right now discussing and we will have actually a transcription option in the recording. Then, when we are recording, we can actually do a basket for us. So, whatever I am saying, Google will transcribe it. If you do not do conversion, it will do only transcription. That is a facility available. So, if I am speaking something, an automatic speech recognition module can simply just transcribe it. Now, when I am doing the transcription, either because I have made mistake in speaking the particular sentence or the transcription would have created an error. Suppose, I have said when the moon comes in between the sun and the earth, the solar eclipse happens. But, I may have actually made a mistake and I may have said when the moon come in between sun and this, right? Or the module, Google Translator may have missed me saying comes, it may have just like heard as come. And, this is the transcription. Now, here is an error in the transcription. Either we can actually manually correct it or we create a module of language correction or a grammar correction. So, we can correct the grammar and then the machine translation can take in our module of text to text to text to text coming. Now, from English, the machine translation can do some to Hindi. So, from English to Hindi, we have now a translation. When Chandramar, the sun comes in between the sun, then the sun becomes dark. Now, this translation has made an error. Now, we need a correction module. It can be a manual correction or it can be generally like not manual all the time, but there has to be a automated module of correcting the grammar in the second language also. So, the second language after the grammar correction is available now, right after the correction and then we can add a text to speech module. So, in this approach, what we are doing? So, English to English, right. So, English, why is to English text, then English text to Hindi text, and Hindi text to Hindi, why is right. This was when part one of first read the ratio. If I have to do this, this is typically the path that what can actually think of doing. The most naive approach I would say or the first thought, we can do something like this, what is the problem in this? Latency, what else? Errors, right. So, any step like automatic speech recognition making an error would need to be rectified. The machine translation error needs to be rectified, right. And machine and then like, there is a translation firm again from text to speech. So, many scope or many blocks for chances of more errors, right. So, that is when we are using this Bavohashak project for creating more data. We are using this such a pipeline to create data, not to actually solve the problem of speech to speech synthesis, okay. So, so far what we have understood, one is speech to speech synthesis, one or another is text to speech synthesis. And speech to speech synthesis can be done by a text to speech synthesis. So, here what we have done is exactly that speech to text, then text to text and then text to speech, right. However, as we all said, lot of latency, error scope of error, actually lot of scope of error and error can be propagate and have larger errors. Why can't we directly do speech to speech synthesis? So, so so, first and foremost, latency will go off. We do not need to convert into some other domain, okay. Now, what we have done over here is text to voice, it will be related to learn of particular mapping. Now, what happens if we say that we have both speech on these, we say, for a space of the, this is my voice and correlation, voice and Hindi, okay. So, the problem statement still is going to be something similar or even we are just trying to help you to say that, okay, that text to speech, the concept in text to speech is similar to or the concept in speech to speech is similar to text in speech in mapping. We do not have to go through that whole cycle of convergence from one domain to another, okay. So, what happens now? So, now we have two domains, both the domains are audio signals, okay. Now, my this domain, I need to import, okay. I need to import my voice signal, which is an English, to voice signal, which is an Hindi and battery custom back, both signals are to be doing, right. So, if when I am doing this, what I am trying to do, V of E is equal to N of E in E of H of V H, okay. When I am coming back, what do I, what you are doing? So, this is the forward E conversion, right. So, V of E equal to M of E H into V H, right. Now, when I am doing this, what what we are trying to do? V of H equal to M of E H, H T, right. Yaha, this is a, this can be a different map, right. It is can be different mapping approach, into, sorry. So, when I am coming from here to here, what we have is we need to generate V H equal to M of H E into V E, okay. So, V given as V we are generating V H, okay. Given V H, we are generating V, right. So, this will be V of E equal to M of H E into, sorry, V H into V H, fine. Now, would you like to have two different importers, sorry, two different mappers, a dictionary sufficient, right. You do not need both the dictionaries. Now, what can we do here essentially is envy force. Can we force that the two dictionaries that we are trying to create or the two mappers that we are trying to create, they both actually are same. So, if you have to write the equation, so this is equation number 1, this is equation number 2, with the control that both the mappers are or both the conversion approaches are exactly same. So, then we are not using two dictionaries, right. So, this essentially reduces your latency, this, right. So, you and then this also reduces your overall computational load on the system, memory requirement everything, right, is it just reduces, okay. So, we can translate from one language to another language with the help of a single dictionary, right. So, we will see actually we will go through the entire algorithmic details also like how things happen. So, we will start with the first algorithm, a very simple algorithm, actually one of the early algorithms on removing the noise, okay. So, we will build from there so that we can understand how this speed synthesis is using the dictionary algorithm or in order or in order to go to the model works. Then we will see like how we replace the noise all apart with the language all apart and it works, right. So, let us understand the problem first. This was the base network for building the input or the input or model for speed synthesis, okay. So, we have a particular noise out. So, if you go to a relation, which are the relation, right. Background you are calling on form. It is a you are calling your family members, right. From the relation, the relationship is not there. What is happening at the other end? Your family members are listening all the way. Background, background, background, lots are are are are, right. A lot of background in the machine must be going on. Why USP is also? So, if you if you if you treat USP signal as the foreground, the all other noise are background. Noise at the registration, everything is background. If I if I ask you, can you can we build an architecture which drops the entire background part and preserves only the foreground? What kind of approach you can immediately think of? Trash holding, okay. But remember, you are speaking and suddenly, PCHSA, I am mandor express platform number one, and that is going to be much louder than your voice, right. And if it for sure, it what happens? Mandor express of Garvala Gosulay Dega, your voice will actually be below the threshold. So, some sort of noise cancellation needs to be done, right. So, can you think of a nice cancellation algorithm? STFT, short term related Fourier transform. Right. So, so if if you use our traditional speech processing approach, we convert our audio signals to STFT form and from STFT we convert it to MSCC forms and then we essentially suppress the signal. And of the day, we are trying to like do a thresholding in some other space, some other feature space, right. And and then we just say the okay now, whatever is the left over, after the thresholding, we convert like using a decoder, we convert it back into the audio signal and then pass it. Hopefully again that may work, but we do not know. Okay. So, obviously, when we when we are in the deep learning world, everything is deep learning, STFT and MSCC are now old school, right. So, this this is actually what you are talking about, this particular block diagram that is over here. Essentially talks about that, we can use STFT, MFCC, then from there to actually we send it to a very classical old school kind of decoder, which separates out the signal and the, there is the noise. What I am interested in an encoder decoder approach, right. So, so we we may not need to convert anything into MSCC or STFT and followed by MSCC. Can we so how many of you remember auto encoders? Lockerger has taught auto encoders in the class, right. Can someone tell me in the formulations for auto encoder very quickly. So, I have an input x, okay. I have a hidden layer h and an output x dash, right. And this is also connected, right. So, this part is my encoder, this part is my encoder, what is the last function? Anywhere else? MSC, MSC between. So, MSC of x minus dash, this is the last function, right. So, now we are going to do this, so if this is my h, you say h, h is water, and it is embedding, but as you call do, so here if it is the gates are w and here the gates are w dash, h is equal to w dash, x, right. And then x dash is equal to, w dash into h. Now, here we have h value, we know the h value over here, then x dash equal to w dash w, x, right. If you put this value over here, can we have a, w dash w, x, right. So, given x, we need we are obtaining w dashing w. For the sake of simplicity, what we will do is, we will say that whatever w is over here, here it is a task force. Not a different weight, it is the same weight, generally the physical force here, right. In the dictionary one, in the dictionary name, we will only create one dictionary, right. So, here also, so it is a dictionary, mapping the current, it is a dictionary. So, in place of creating two dictionary, there are two weights sets. If you, if you force it, if this w and w, and here it is w transpose will talk, right. Just weight similarity, you can quote here, h, this is the ultimate weight set equal to garnaji, h where if you, if you can say that, a transpose weight, transpose weight set equal to garnaji, so then we can force w transpose, dupcone into x. So, then I do not need to learn two set of weights. I can learn only one set of weights and use your dupcotransposing can, can, can, then with it, okay. So, this is a classical formulation of autoencoders, encoder decoder, essentially, okay. So, MSc loss, but then this is how this formulation works. So, why did I actually take this? I just, this is noise data, our audio signal has both in noise as well as input signal, right. I have to remove the noise. Up, let us try to understand, the elizion, this is fine. Okay. So, we can think of NFCC or whatever, a melastictrum or whatever, or the raw audio itself, but we need to give a one dimensional embedding, one dimensional signal, as input to my encoder, I have to place that into some embedding space and then convert it back, such that I generate the clean signal up, okay. So, autoencoder and from deep learning class, if you remember autoencoder, excuse me, one of the formulation of autoencoders, one of the same as formulation of autoencoder is de-noising autoencoder. It can be de-noised the data, is the data is noisy, can I reconstruct the non-noised data, okay. So, my problem exactly same, our input is noisy. So, can I construct the de-noised output, can I synthesize and remove the noise out, noise out, out and keep the clean signal. So, if we have my input, let's say, a bias V, which is clean, right, so V clean plus a noise, background noise, okay. So, what we get is V of data, okay. We need to ensure that we are removing this data, the noise happening and we construct V c back, okay. So, far with me, I should take I have used xn over here, so we are with me, let me convert this into x, fine. Now, my encoder job, what is it, so if I have my input as xc, so my, sorry, this is my input, right. So, how should I write a formula or the equation for input, input, what do I do. This xc, right, sorry, right now I have xn, I need to reconstruct xc back, but what I have is xn, the relation of the noise when you are talking, okay. So, xn, I obtain h, means h equal to W times xn, okay. Now, from h, I generate, what back? xn generate, correct. xn generate, correct. De-noise x, right. So, I need to generate xc dash, okay. Now, I have to generate h, this is my encoder, okay. Okay. So, what should happen over here? I have xc dash, okay, but just hold, I want. So, what should be my output of x, xc, kendo, W transpose, remember, I will put that in place of W dash, now we can live with W transpose, fine. So, W transpose into, what? Right, fine. I will be hk value k3, w of xn, here it is, x dashc, So now, this is, what, is that k glass function into which k is phi 6 ? I will change the k matrix to a друзья's limit. I will charge or try to answer that in one language. Okay. then you actually have to move forward, you cannot, So, clean signal, if the rica is constructed clean signal, we have to minimize the different two. Now, let us actually, let us write this xc minus 1 root transpose w into nix. So, given our noisy signal and its clean counterpart, we are trying to learn the weights w and w and w transpose so that I can obtain the clean signal back. This is the time of training, once the training is done, what we need essentially is an xn, it will encode and give my h from h, I will generate xc dash and this xc dash because training is over, this xc dash is actually a d noise. So, this is what actually this particular last function or this particular slide actually highlights. Now, in this slide, there is one more small add-on. What is that small add-on? Can anyone actually look at the small add-on and tell me, what we have here? Noise carrier and what you are saying? Okay, we have skipped connection, it has a net style, it has a skip connection, encoded to decoder. So, that is learning better so we can go much deeper. Not to skip, I am talking about a loss function, focus on the loss function, there is one add-on thing in the loss function. What is that? Now, we need to add one more tone and what is that tone? And why actually we need to add that? So, how do we know that we have just set the noise in the right place? So, what is essentially we have to find to achieve here is, this first tone, it is not I will say, it is not the high-end, it is not the high-end, otherwise it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. What questions are you going to answer? So, what are you going to anart? You are going to make the overlay as write quite correct it. So, how would you do not write that as write a pointer? And how would you have it? Please become easy as time day. And, finally, okay, ok, I understood that he had any details left, we already set it Can someone tell me? Online guys. Can you interpret the second term over here? No. Is there a question? Is there a question? No. What's the name of it? What's the name of it? It's called Furnay. It's a refugee here. Is there a question? Yes. It's a question. No. No. What happened? Everything is done. Okay. This is better for online guys? Yes. Okay. If something happens, tell me. Okay. So, where are we going to go? Online guys. What is the meaning of the significance and what the second term in the last function is doing? It's a regularization term. But what is it doing? Will you tell me? No. Will you give me a good tightening? Okay. Okay. Okay. So, then let me actually tell you how is he? Now, look at his diagram again. And just imagine for a moment, we don't have a voice signal. We don't have a voice signal. And what will we do? What will we do if we have a noise? Okay. For the time being, let's understand from there. What was the name of it? The XM. Okay. We have a constant. I have to change that. What we should do is we should think of, can we have a noise signal there? And noise reconstructed. Means, we are suppressing the actual voice and reconstructing the noise back. What train the Pichayabh and Manur Express are there. That announcement we want to request. So, what would be my last function? We have the original noise. Right? Whatever is my original noise, minus of the reconstructed noise. Right? Or, not the NR, NR, NR, Reconstruction. Okay. So, what this actually tells me? What is this? But same autoencoder. If it starts actually doing the reconstruction of noise. Means, our first term in this...