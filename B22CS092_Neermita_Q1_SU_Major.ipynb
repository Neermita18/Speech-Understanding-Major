{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GoSzRQ6_mNhd",
        "outputId": "77d6f11e-55ee-4eca-e455-ba18ae9bffcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m665.6/800.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-cloud-speech\n",
            "  Downloading google_cloud_speech-2.32.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting google-cloud-texttospeech\n",
            "  Downloading google_cloud_texttospeech-2.26.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: google-cloud-translate in /usr/local/lib/python3.11/dist-packages (3.20.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Collecting pesq\n",
            "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (5.29.4)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-translate) (2.4.3)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-translate) (0.14.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (4.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_speech-2.32.0-py3-none-any.whl (334 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m334.1/334.1 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_texttospeech-2.26.0-py3-none-any.whl (188 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.1/188.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper, pesq\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803405 sha256=07666c097153b71ce7bc18dbef4ca7fcd70f88901a3cc0e5347d70b5f5d0f3f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp311-cp311-linux_x86_64.whl size=274948 sha256=c0c9187a3f0f389c45e092592b80656e6ac09df130715c5c22deb3eb18b6f0da\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/f1/23/2698d0bf31eec2b2aa50623b5d93b6206c49c7155d0e31345d\n",
            "Successfully built openai-whisper pesq\n",
            "Installing collected packages: pesq, rapidfuzz, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jiwer, nvidia-cusolver-cu12, openai-whisper, google-cloud-texttospeech, google-cloud-speech\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed google-cloud-speech-2.32.0 google-cloud-texttospeech-2.26.0 jiwer-3.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 pesq-0.0.4 rapidfuzz-3.13.0 tiktoken-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "ebeca6f33f664b2d9ea250c373180785"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install torch openai-whisper google-cloud-speech google-cloud-texttospeech google-cloud-translate librosa soundfile pesq jiwer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from google.cloud import speech, texttospeech, translate_v2 as translate\n",
        "import whisper\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from pesq import pesq\n",
        "from jiwer import wer, cer"
      ],
      "metadata": {
        "id": "9m7BjHjtnGYN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/speech-understanding-457206-a2485146b69c.json\""
      ],
      "metadata": {
        "id": "x4gk4vv8nMAo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQkRiwM9oDEK",
        "outputId": "e34f5782-271a-4dda-ac4a-0925e52df3f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription with Whisper"
      ],
      "metadata": {
        "id": "MiV9UOFAmiIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os\n",
        "\n",
        "def transcribe_with_whisper(audio_path, model_size=\"base\"):\n",
        "    print(\"Loading Whisper model...\")\n",
        "    model = whisper.load_model(model_size)\n",
        "    print(f\"Transcribing audio file: {audio_path}\")\n",
        "    result = model.transcribe(audio_path, language=None)  # Auto-detects language, supports code-switching\n",
        "    transcript = result[\"text\"]\n",
        "    print(\"Transcription complete!\")\n",
        "\n",
        "    return transcript\n"
      ],
      "metadata": {
        "id": "F_zXxsGdmUnS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQggyF4nonff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using 40 mins of the last Speech Lecture"
      ],
      "metadata": {
        "id": "1bzOCC25_AY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Step 1: Transcribing lecture...\")\n",
        "transcript = transcribe_with_whisper('/content/drive/MyDrive/audio.mp3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtOikX0YpAg7",
        "outputId": "723c8b78-72e0-4bd9-8858-02f41cdbae0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Transcribing lecture...\n",
            "Loading Whisper model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 89.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing audio file: /content/drive/MyDrive/audio.mp3\n",
            "Transcription complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript"
      ],
      "metadata": {
        "id": "bCisryiZpELJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "19307a2b-3bc5-45eb-a566-0f75b9c38b44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" So, you are using my word also. So, online students, are you able to see the green model? So, some formulations, so they are going to be on the screen on the slide right. So, let us motivate about the speech synthesis. I am sure Dr. Chai has already talked about speech synthesis and then did actually classes, but we will go a little bit in detail here. So, in speech synthesis there are two major parts. The first part inspired by like you can get idea from this particular video. I am speaking English, but I am not in the bowl, but I am with Ola Pallare. I can speak French. Those are just a few examples of the different languages that I can speak here. So, now remember when our prime minister once actually used in Koder and decoder to speak in like give a life telecast in multiple languages in Talon, Thaleru, Kandara and all other languages. He does not know either of them, probably right, when he can speak so fluently neither of them, but then in the life telecast he was speaking, probably he was not probably, he was speaking in Hindi, but in the life telecast was happening in multiple languages. So, in almost near real time life telecast was happening and this is like an example that one can speak in near real time in different languages. So, you may not need to know the language. Another example is suppose I am traveling to Japan, I do not know Japanese, but if I have to communicate, currently what happens when you go to Japan or China somewhere, then you use actually your phone and then do that conversation with the native people over there, those who may not know English there. So, you speak in English and then you use a TTS model or speak synthesis model to convert into Japanese. They speak in Japanese, they understand from there, they speak in Japanese and then the TTS model gives or this synthesis model gives we back the English version, right. Now, the field has advanced much much more and here is the second part. So, in the previous part the idea was that I am speaking in English and I am getting translated in Hindi, right, or French or Spanish. It was speech to speech synthesis. The other could be you know that whose image is this? Where is the CMO? Care and not care. So, Tamil language. Tamil language, exactly. So, but the point here is that he is no more. In the Indian elections, while this is an example of defake, but in Indian elections his video was circulated, okay, giving a particular message, okay. So, what would have happened? Let me run the video and then you can guess, okay. It's very long. Video, but then you got the idea, right. Now, the person is no more. We have persons, a lot of video clips and audio signals. I can we can learn actually some how actually someone would actually speak anything. Now, what I can do is if I have using foundation models or using any of these modern AI algorithms, I can type in text whatever I want him to say, I have a trained model and that that trained model takes this text and then pass all this information converts into a voice signal, a waveform, okay. So, this is text to speech. So, that's a second kind of speech synthesis part, okay. So, birds eye view, essentially we have very prevalent examples of the second part when we are providing the text and we are generating the audio sound, okay. While both examples I have videos, but then just for the time being don't look at videos, it's audio is also that like equally important and they are all lip sync and so on. So, now if you have to generate such a software, such a tool, how will we go about it? Simpler is text to speech, slightly tougher is speech to speech and we will talk about both, so if you start with text to speech, the very first thing is we need to have a mapping, some sort of mapping, some sort of understanding of the text word and the speech word. So, unless you are able to see my board, write the time, what I write something. So, if this is my text space, this is my feature space, okay and this is all audio voice, right. What I need actually, I need can I convert from here to here and from here to here. So, I need to learn a mapping broadly, I need to learn a mapping, between text, T1 and speech, in such a way that T1 equal to n times S1, right. So, I need to have, I need to learn a mapping function which can convert my text embeddings into my speech embeddings and from the speech embedding, I can do a decoding and generate a speech out, right. So, if we have let us say a speech, any speech encoder like Risham has taught some of those speech encoders, right. So, once we have the embedding, I can convert that into user decoder to decode it, right. So, so far clear, now what are the challenges, which challenges, what are the challenges, it is not that straightforward. See, if we talk about any into embeddings like text to text. So, okay let us let us with respect to NLU, topic. NLU is not my area, but at least from the very basic fundamental concept. If I have line, this is what text in English, okay. And I have a text in Hindi, right. I can learn very simple mapping between some mapping, between E2H, so that if I provide a text in Hindi, I can generate text in English by processing by mapping. So, it is like a dictionary, okay. So, if we do that, right. So, in the English dictionary, we all have done in our childhood days, this is my English word, what is the Hindi part of it, right or vice versa. So, so if we can use, so this mapping is nothing but that dictionary. So, in text, we have done it many times. Now, if you have to do this in audio, the points like that, what is what is the duration of the audio clip, right. How someone is actually the accent of the person, how someone is speaking, right. And even many intricate details, for instance, how someone is phasing the sentences. And then when you are speaking, right, when we speak, we actually utter a lot of, these kinds of words. And it also happens if your English is not very fluent or the language, the second language is not very fluent, if it is not a mother term, then you have some errors in the translation, because at the back of your mind, you have also a dictionary, God gives the dictionary given, right, in brain. What you are doing essentially, you are speaking in Hindi in your brain and then that dictionary is converting and that after the conversion, when you are speaking, right, broadly. So, so that may have some errors, because your dictionary, some of that translation may not be absolutely correct. So, those errors may also come in, while your Hindi is completely fine, but your English translation may not be very fine. So, how to minimize, so then essentially, this whole thing comes under from the words to utterances. We have to take care of all these variations and that is where the mapping comes into picture. Let how to ensure that this mapping of different accent variations, different style variations can also be understood and mapped, while the correctness also is equally important. So, this is another view, one of the views which actually a paper has talked about, where you have a text module, you give it to your embedding layer, like the Anelu layer, Anelu layer gives you the linguistic features, the embedding essentially. And then from there, you do some procedure modeling, you actually do the, you go through the entire process of generating the waveform and tapetrol is essentially the text to meld, MFCC coefficient extraction. So, if you have a text and you convert that into an MFCC coefficient, the model has been trained essentially that is a dictionary. So, so, the particular style of creating that dictionary, where we are converting everything into a meld frequency spectrum. And then you have a vocoder like a wave net or a wave glow, you can use any of your favorite vocoder which is essentially converts into, it is like a voice decoder and that is the short form of vocoder is essentially short form of voice decoder. So, you decode into a voice signal, where you actually do the end to end synthesis of a voice signal from where input is your text. So, this is the bird's ivy. So, this is the first part of, or the one of the parts of speech synthesis. Any questions so far? Online guys, if not then I will follow it. And I hope I am audible and everything is going good. Okay. So, so, one of the things that we are also working in the similar domain, but we have just added a few steps of corrections. So, what we have essentially done is speech to speech or what we have been trying to do is essentially speech to speech. I think Risham has taught you guys, right? So, he has been working on this. So, what we do is like, suppose I am right now discussing and we will have actually a transcription option in the recording. Then, when we are recording, we can actually do a basket for us. So, whatever I am saying, Google will transcribe it. If you do not do conversion, it will do only transcription. That is a facility available. So, if I am speaking something, an automatic speech recognition module can simply just transcribe it. Now, when I am doing the transcription, either because I have made mistake in speaking the particular sentence or the transcription would have created an error. Suppose, I have said when the moon comes in between the sun and the earth, the solar eclipse happens. But, I may have actually made a mistake and I may have said when the moon come in between sun and this, right? Or the module, Google Translator may have missed me saying comes, it may have just like heard as come. And, this is the transcription. Now, here is an error in the transcription. Either we can actually manually correct it or we create a module of language correction or a grammar correction. So, we can correct the grammar and then the machine translation can take in our module of text to text to text to text coming. Now, from English, the machine translation can do some to Hindi. So, from English to Hindi, we have now a translation. When Chandramar, the sun comes in between the sun, then the sun becomes dark. Now, this translation has made an error. Now, we need a correction module. It can be a manual correction or it can be generally like not manual all the time, but there has to be a automated module of correcting the grammar in the second language also. So, the second language after the grammar correction is available now, right after the correction and then we can add a text to speech module. So, in this approach, what we are doing? So, English to English, right. So, English, why is to English text, then English text to Hindi text, and Hindi text to Hindi, why is right. This was when part one of first read the ratio. If I have to do this, this is typically the path that what can actually think of doing. The most naive approach I would say or the first thought, we can do something like this, what is the problem in this? Latency, what else? Errors, right. So, any step like automatic speech recognition making an error would need to be rectified. The machine translation error needs to be rectified, right. And machine and then like, there is a translation firm again from text to speech. So, many scope or many blocks for chances of more errors, right. So, that is when we are using this Bavohashak project for creating more data. We are using this such a pipeline to create data, not to actually solve the problem of speech to speech synthesis, okay. So, so far what we have understood, one is speech to speech synthesis, one or another is text to speech synthesis. And speech to speech synthesis can be done by a text to speech synthesis. So, here what we have done is exactly that speech to text, then text to text and then text to speech, right. However, as we all said, lot of latency, error scope of error, actually lot of scope of error and error can be propagate and have larger errors. Why can't we directly do speech to speech synthesis? So, so so, first and foremost, latency will go off. We do not need to convert into some other domain, okay. Now, what we have done over here is text to voice, it will be related to learn of particular mapping. Now, what happens if we say that we have both speech on these, we say, for a space of the, this is my voice and correlation, voice and Hindi, okay. So, the problem statement still is going to be something similar or even we are just trying to help you to say that, okay, that text to speech, the concept in text to speech is similar to or the concept in speech to speech is similar to text in speech in mapping. We do not have to go through that whole cycle of convergence from one domain to another, okay. So, what happens now? So, now we have two domains, both the domains are audio signals, okay. Now, my this domain, I need to import, okay. I need to import my voice signal, which is an English, to voice signal, which is an Hindi and battery custom back, both signals are to be doing, right. So, if when I am doing this, what I am trying to do, V of E is equal to N of E in E of H of V H, okay. When I am coming back, what do I, what you are doing? So, this is the forward E conversion, right. So, V of E equal to M of E H into V H, right. Now, when I am doing this, what what we are trying to do? V of H equal to M of E H, H T, right. Yaha, this is a, this can be a different map, right. It is can be different mapping approach, into, sorry. So, when I am coming from here to here, what we have is we need to generate V H equal to M of H E into V E, okay. So, V given as V we are generating V H, okay. Given V H, we are generating V, right. So, this will be V of E equal to M of H E into, sorry, V H into V H, fine. Now, would you like to have two different importers, sorry, two different mappers, a dictionary sufficient, right. You do not need both the dictionaries. Now, what can we do here essentially is envy force. Can we force that the two dictionaries that we are trying to create or the two mappers that we are trying to create, they both actually are same. So, if you have to write the equation, so this is equation number 1, this is equation number 2, with the control that both the mappers are or both the conversion approaches are exactly same. So, then we are not using two dictionaries, right. So, this essentially reduces your latency, this, right. So, you and then this also reduces your overall computational load on the system, memory requirement everything, right, is it just reduces, okay. So, we can translate from one language to another language with the help of a single dictionary, right. So, we will see actually we will go through the entire algorithmic details also like how things happen. So, we will start with the first algorithm, a very simple algorithm, actually one of the early algorithms on removing the noise, okay. So, we will build from there so that we can understand how this speed synthesis is using the dictionary algorithm or in order or in order to go to the model works. Then we will see like how we replace the noise all apart with the language all apart and it works, right. So, let us understand the problem first. This was the base network for building the input or the input or model for speed synthesis, okay. So, we have a particular noise out. So, if you go to a relation, which are the relation, right. Background you are calling on form. It is a you are calling your family members, right. From the relation, the relationship is not there. What is happening at the other end? Your family members are listening all the way. Background, background, background, lots are are are are, right. A lot of background in the machine must be going on. Why USP is also? So, if you if you if you treat USP signal as the foreground, the all other noise are background. Noise at the registration, everything is background. If I if I ask you, can you can we build an architecture which drops the entire background part and preserves only the foreground? What kind of approach you can immediately think of? Trash holding, okay. But remember, you are speaking and suddenly, PCHSA, I am mandor express platform number one, and that is going to be much louder than your voice, right. And if it for sure, it what happens? Mandor express of Garvala Gosulay Dega, your voice will actually be below the threshold. So, some sort of noise cancellation needs to be done, right. So, can you think of a nice cancellation algorithm? STFT, short term related Fourier transform. Right. So, so if if you use our traditional speech processing approach, we convert our audio signals to STFT form and from STFT we convert it to MSCC forms and then we essentially suppress the signal. And of the day, we are trying to like do a thresholding in some other space, some other feature space, right. And and then we just say the okay now, whatever is the left over, after the thresholding, we convert like using a decoder, we convert it back into the audio signal and then pass it. Hopefully again that may work, but we do not know. Okay. So, obviously, when we when we are in the deep learning world, everything is deep learning, STFT and MSCC are now old school, right. So, this this is actually what you are talking about, this particular block diagram that is over here. Essentially talks about that, we can use STFT, MFCC, then from there to actually we send it to a very classical old school kind of decoder, which separates out the signal and the, there is the noise. What I am interested in an encoder decoder approach, right. So, so we we may not need to convert anything into MSCC or STFT and followed by MSCC. Can we so how many of you remember auto encoders? Lockerger has taught auto encoders in the class, right. Can someone tell me in the formulations for auto encoder very quickly. So, I have an input x, okay. I have a hidden layer h and an output x dash, right. And this is also connected, right. So, this part is my encoder, this part is my encoder, what is the last function? Anywhere else? MSC, MSC between. So, MSC of x minus dash, this is the last function, right. So, now we are going to do this, so if this is my h, you say h, h is water, and it is embedding, but as you call do, so here if it is the gates are w and here the gates are w dash, h is equal to w dash, x, right. And then x dash is equal to, w dash into h. Now, here we have h value, we know the h value over here, then x dash equal to w dash w, x, right. If you put this value over here, can we have a, w dash w, x, right. So, given x, we need we are obtaining w dashing w. For the sake of simplicity, what we will do is, we will say that whatever w is over here, here it is a task force. Not a different weight, it is the same weight, generally the physical force here, right. In the dictionary one, in the dictionary name, we will only create one dictionary, right. So, here also, so it is a dictionary, mapping the current, it is a dictionary. So, in place of creating two dictionary, there are two weights sets. If you, if you force it, if this w and w, and here it is w transpose will talk, right. Just weight similarity, you can quote here, h, this is the ultimate weight set equal to garnaji, h where if you, if you can say that, a transpose weight, transpose weight set equal to garnaji, so then we can force w transpose, dupcone into x. So, then I do not need to learn two set of weights. I can learn only one set of weights and use your dupcotransposing can, can, can, then with it, okay. So, this is a classical formulation of autoencoders, encoder decoder, essentially, okay. So, MSc loss, but then this is how this formulation works. So, why did I actually take this? I just, this is noise data, our audio signal has both in noise as well as input signal, right. I have to remove the noise. Up, let us try to understand, the elizion, this is fine. Okay. So, we can think of NFCC or whatever, a melastictrum or whatever, or the raw audio itself, but we need to give a one dimensional embedding, one dimensional signal, as input to my encoder, I have to place that into some embedding space and then convert it back, such that I generate the clean signal up, okay. So, autoencoder and from deep learning class, if you remember autoencoder, excuse me, one of the formulation of autoencoders, one of the same as formulation of autoencoder is de-noising autoencoder. It can be de-noised the data, is the data is noisy, can I reconstruct the non-noised data, okay. So, my problem exactly same, our input is noisy. So, can I construct the de-noised output, can I synthesize and remove the noise out, noise out, out and keep the clean signal. So, if we have my input, let's say, a bias V, which is clean, right, so V clean plus a noise, background noise, okay. So, what we get is V of data, okay. We need to ensure that we are removing this data, the noise happening and we construct V c back, okay. So, far with me, I should take I have used xn over here, so we are with me, let me convert this into x, fine. Now, my encoder job, what is it, so if I have my input as xc, so my, sorry, this is my input, right. So, how should I write a formula or the equation for input, input, what do I do. This xc, right, sorry, right now I have xn, I need to reconstruct xc back, but what I have is xn, the relation of the noise when you are talking, okay. So, xn, I obtain h, means h equal to W times xn, okay. Now, from h, I generate, what back? xn generate, correct. xn generate, correct. De-noise x, right. So, I need to generate xc dash, okay. Now, I have to generate h, this is my encoder, okay. Okay. So, what should happen over here? I have xc dash, okay, but just hold, I want. So, what should be my output of x, xc, kendo, W transpose, remember, I will put that in place of W dash, now we can live with W transpose, fine. So, W transpose into, what? Right, fine. I will be hk value k3, w of xn, here it is, x dashc, So now, this is, what, is that k glass function into which k is phi 6 ? I will change the k matrix to a друзья's limit. I will charge or try to answer that in one language. Okay. then you actually have to move forward, you cannot, So, clean signal, if the rica is constructed clean signal, we have to minimize the different two. Now, let us actually, let us write this xc minus 1 root transpose w into nix. So, given our noisy signal and its clean counterpart, we are trying to learn the weights w and w and w transpose so that I can obtain the clean signal back. This is the time of training, once the training is done, what we need essentially is an xn, it will encode and give my h from h, I will generate xc dash and this xc dash because training is over, this xc dash is actually a d noise. So, this is what actually this particular last function or this particular slide actually highlights. Now, in this slide, there is one more small add-on. What is that small add-on? Can anyone actually look at the small add-on and tell me, what we have here? Noise carrier and what you are saying? Okay, we have skipped connection, it has a net style, it has a skip connection, encoded to decoder. So, that is learning better so we can go much deeper. Not to skip, I am talking about a loss function, focus on the loss function, there is one add-on thing in the loss function. What is that? Now, we need to add one more tone and what is that tone? And why actually we need to add that? So, how do we know that we have just set the noise in the right place? So, what is essentially we have to find to achieve here is, this first tone, it is not I will say, it is not the high-end, it is not the high-end, otherwise it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. What questions are you going to answer? So, what are you going to anart? You are going to make the overlay as write quite correct it. So, how would you do not write that as write a pointer? And how would you have it? Please become easy as time day. And, finally, okay, ok, I understood that he had any details left, we already set it Can someone tell me? Online guys. Can you interpret the second term over here? No. Is there a question? Is there a question? No. What's the name of it? What's the name of it? It's called Furnay. It's a refugee here. Is there a question? Yes. It's a question. No. No. What happened? Everything is done. Okay. This is better for online guys? Yes. Okay. If something happens, tell me. Okay. So, where are we going to go? Online guys. What is the meaning of the significance and what the second term in the last function is doing? It's a regularization term. But what is it doing? Will you tell me? No. Will you give me a good tightening? Okay. Okay. Okay. So, then let me actually tell you how is he? Now, look at his diagram again. And just imagine for a moment, we don't have a voice signal. We don't have a voice signal. And what will we do? What will we do if we have a noise? Okay. For the time being, let's understand from there. What was the name of it? The XM. Okay. We have a constant. I have to change that. What we should do is we should think of, can we have a noise signal there? And noise reconstructed. Means, we are suppressing the actual voice and reconstructing the noise back. What train the Pichayabh and Manur Express are there. That announcement we want to request. So, what would be my last function? We have the original noise. Right? Whatever is my original noise, minus of the reconstructed noise. Right? Or, not the NR, NR, NR, Reconstruction. Okay. So, what this actually tells me? What is this? But same autoencoder. If it starts actually doing the reconstruction of noise. Means, our first term in this...\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/lecture_transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(transcript)"
      ],
      "metadata": {
        "id": "4QxIHhty_PKW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_filler_words(transcript):\n",
        "    english_fillers = r'\\b(um|uh|er|like|you know|so|basically|actually|literally|Um|Uh|So|Umm|Uhh|Like|Eh)\\b'\n",
        "    hindi_fillers = r'\\b(मतलब|ऐसे|वो|तो|ना|हां|अच्छा)\\b'\n",
        "    # Remove filler words\n",
        "    cleaned_transcript = re.sub(english_fillers, '', transcript, flags=re.IGNORECASE)\n",
        "    cleaned_transcript = re.sub(hindi_fillers, '', cleaned_transcript)\n",
        "    cleaned_transcript = re.sub(r'\\s+', ' ', cleaned_transcript).strip()\n",
        "    return cleaned_transcript"
      ],
      "metadata": {
        "id": "7kuJEBe9_Ycc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_transcript=remove_filler_words(transcript)"
      ],
      "metadata": {
        "id": "uNiUrqx7C76h"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_transcript"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "r0VjMiE8DCXc",
        "outputId": "7a16e33e-13dd-4d25-b362-b8251dee4f52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\", you are using my word also. , online students, are you able to see the green model? , some formulations, they are going to be on the screen on the slide right. , let us motivate about the speech synthesis. I am sure Dr. Chai has already talked about speech synthesis and then did classes, but we will go a little bit in detail here. , in speech synthesis there are two major parts. The first part inspired by you can get idea from this particular video. I am speaking English, but I am not in the bowl, but I am with Ola Pallare. I can speak French. Those are just a few examples of the different languages that I can speak here. , now remember when our prime minister once used in Koder and decoder to speak in give a life telecast in multiple languages in Talon, Thaleru, Kandara and all other languages. He does not know either of them, probably right, when he can speak fluently neither of them, but then in the life telecast he was speaking, probably he was not probably, he was speaking in Hindi, but in the life telecast was happening in multiple languages. , in almost near real time life telecast was happening and this is an example that one can speak in near real time in different languages. , you may not need to know the language. Another example is suppose I am traveling to Japan, I do not know Japanese, but if I have to communicate, currently what happens when you go to Japan or China somewhere, then you use your phone and then do that conversation with the native people over there, those who may not know English there. , you speak in English and then you use a TTS model or speak synthesis model to convert into Japanese. They speak in Japanese, they understand from there, they speak in Japanese and then the TTS model gives or this synthesis model gives we back the English version, right. Now, the field has advanced much much more and here is the second part. , in the previous part the idea was that I am speaking in English and I am getting translated in Hindi, right, or French or Spanish. It was speech to speech synthesis. The other could be that whose image is this? Where is the CMO? Care and not care. , Tamil language. Tamil language, exactly. , but the point here is that he is no more. In the Indian elections, while this is an example of defake, but in Indian elections his video was circulated, okay, giving a particular message, okay. , what would have happened? Let me run the video and then you can guess, okay. It's very long. Video, but then you got the idea, right. Now, the person is no more. We have persons, a lot of video clips and audio signals. I can we can learn some how someone would speak anything. Now, what I can do is if I have using foundation models or using any of these modern AI algorithms, I can type in text whatever I want him to say, I have a trained model and that that trained model takes this text and then pass all this information converts into a voice signal, a waveform, okay. , this is text to speech. , that's a second kind of speech synthesis part, okay. , birds eye view, essentially we have very prevalent examples of the second part when we are providing the text and we are generating the audio sound, okay. While both examples I have videos, but then just for the time being don't look at videos, it's audio is also that equally important and they are all lip sync and on. , now if you have to generate such a software, such a tool, how will we go about it? Simpler is text to speech, slightly tougher is speech to speech and we will talk about both, if you start with text to speech, the very first thing is we need to have a mapping, some sort of mapping, some sort of understanding of the text word and the speech word. , unless you are able to see my board, write the time, what I write something. , if this is my text space, this is my feature space, okay and this is all audio voice, right. What I need , I need can I convert from here to here and from here to here. , I need to learn a mapping broadly, I need to learn a mapping, between text, T1 and speech, in such a way that T1 equal to n times S1, right. , I need to have, I need to learn a mapping function which can convert my text embeddings into my speech embeddings and from the speech embedding, I can do a decoding and generate a speech out, right. , if we have let us say a speech, any speech encoder Risham has taught some of those speech encoders, right. , once we have the embedding, I can convert that into user decoder to decode it, right. , far clear, now what are the challenges, which challenges, what are the challenges, it is not that straightforward. See, if we talk about any into embeddings text to text. , okay let us let us with respect to NLU, topic. NLU is not my area, but at least from the very basic fundamental concept. If I have line, this is what text in English, okay. And I have a text in Hindi, right. I can learn very simple mapping between some mapping, between E2H, that if I provide a text in Hindi, I can generate text in English by processing by mapping. , it is a dictionary, okay. , if we do that, right. , in the English dictionary, we all have done in our childhood days, this is my English word, what is the Hindi part of it, right or vice versa. , if we can use, this mapping is nothing but that dictionary. , in text, we have done it many times. Now, if you have to do this in audio, the points that, what is what is the duration of the audio clip, right. How someone is the accent of the person, how someone is speaking, right. And even many intricate details, for instance, how someone is phasing the sentences. And then when you are speaking, right, when we speak, we utter a lot of, these kinds of words. And it also happens if your English is not very fluent or the language, the second language is not very fluent, if it is not a mother term, then you have some errors in the translation, because at the back of your mind, you have also a dictionary, God gives the dictionary given, right, in brain. What you are doing essentially, you are speaking in Hindi in your brain and then that dictionary is converting and that after the conversion, when you are speaking, right, broadly. , that may have some errors, because your dictionary, some of that translation may not be absolutely correct. , those errors may also come in, while your Hindi is completely fine, but your English translation may not be very fine. , how to minimize, then essentially, this whole thing comes under from the words to utterances. We have to take care of all these variations and that is where the mapping comes into picture. Let how to ensure that this mapping of different accent variations, different style variations can also be understood and mapped, while the correctness also is equally important. , this is another view, one of the views which a paper has talked about, where you have a text module, you give it to your embedding layer, the Anelu layer, Anelu layer gives you the linguistic features, the embedding essentially. And then from there, you do some procedure modeling, you do the, you go through the entire process of generating the waveform and tapetrol is essentially the text to meld, MFCC coefficient extraction. , if you have a text and you convert that into an MFCC coefficient, the model has been trained essentially that is a dictionary. , , the particular style of creating that dictionary, where we are converting everything into a meld frequency spectrum. And then you have a vocoder a wave net or a wave glow, you can use any of your favorite vocoder which is essentially converts into, it is a voice decoder and that is the short form of vocoder is essentially short form of voice decoder. , you decode into a voice signal, where you do the end to end synthesis of a voice signal from where input is your text. , this is the bird's ivy. , this is the first part of, or the one of the parts of speech synthesis. Any questions far? Online guys, if not then I will follow it. And I hope I am audible and everything is going good. Okay. , , one of the things that we are also working in the similar domain, but we have just added a few steps of corrections. , what we have essentially done is speech to speech or what we have been trying to do is essentially speech to speech. I think Risham has taught you guys, right? , he has been working on this. , what we do is , suppose I am right now discussing and we will have a transcription option in the recording. Then, when we are recording, we can do a basket for us. , whatever I am saying, Google will transcribe it. If you do not do conversion, it will do only transcription. That is a facility available. , if I am speaking something, an automatic speech recognition module can simply just transcribe it. Now, when I am doing the transcription, either because I have made mistake in speaking the particular sentence or the transcription would have created an error. Suppose, I have said when the moon comes in between the sun and the earth, the solar eclipse happens. But, I may have made a mistake and I may have said when the moon come in between sun and this, right? Or the module, Google Translator may have missed me saying comes, it may have just heard as come. And, this is the transcription. Now, here is an error in the transcription. Either we can manually correct it or we create a module of language correction or a grammar correction. , we can correct the grammar and then the machine translation can take in our module of text to text to text to text coming. Now, from English, the machine translation can do some to Hindi. , from English to Hindi, we have now a translation. When Chandramar, the sun comes in between the sun, then the sun becomes dark. Now, this translation has made an error. Now, we need a correction module. It can be a manual correction or it can be generally not manual all the time, but there has to be a automated module of correcting the grammar in the second language also. , the second language after the grammar correction is available now, right after the correction and then we can add a text to speech module. , in this approach, what we are doing? , English to English, right. , English, why is to English text, then English text to Hindi text, and Hindi text to Hindi, why is right. This was when part one of first read the ratio. If I have to do this, this is typically the path that what can think of doing. The most naive approach I would say or the first thought, we can do something this, what is the problem in this? Latency, what else? Errors, right. , any step automatic speech recognition making an error would need to be rectified. The machine translation error needs to be rectified, right. And machine and then , there is a translation firm again from text to speech. , many scope or many blocks for chances of more errors, right. , that is when we are using this Bavohashak project for creating more data. We are using this such a pipeline to create data, not to solve the problem of speech to speech synthesis, okay. , far what we have understood, one is speech to speech synthesis, one or another is text to speech synthesis. And speech to speech synthesis can be done by a text to speech synthesis. , here what we have done is exactly that speech to text, then text to text and then text to speech, right. However, as we all said, lot of latency, error scope of error, lot of scope of error and error can be propagate and have larger errors. Why can't we directly do speech to speech synthesis? , , first and foremost, latency will go off. We do not need to convert into some other domain, okay. Now, what we have done over here is text to voice, it will be related to learn of particular mapping. Now, what happens if we say that we have both speech on these, we say, for a space of the, this is my voice and correlation, voice and Hindi, okay. , the problem statement still is going to be something similar or even we are just trying to help you to say that, okay, that text to speech, the concept in text to speech is similar to or the concept in speech to speech is similar to text in speech in mapping. We do not have to go through that whole cycle of convergence from one domain to another, okay. , what happens now? , now we have two domains, both the domains are audio signals, okay. Now, my this domain, I need to import, okay. I need to import my voice signal, which is an English, to voice signal, which is an Hindi and battery custom back, both signals are to be doing, right. , if when I am doing this, what I am trying to do, V of E is equal to N of E in E of H of V H, okay. When I am coming back, what do I, what you are doing? , this is the forward E conversion, right. , V of E equal to M of E H into V H, right. Now, when I am doing this, what what we are trying to do? V of H equal to M of E H, H T, right. Yaha, this is a, this can be a different map, right. It is can be different mapping approach, into, sorry. , when I am coming from here to here, what we have is we need to generate V H equal to M of H E into V E, okay. , V given as V we are generating V H, okay. Given V H, we are generating V, right. , this will be V of E equal to M of H E into, sorry, V H into V H, fine. Now, would you to have two different importers, sorry, two different mappers, a dictionary sufficient, right. You do not need both the dictionaries. Now, what can we do here essentially is envy force. Can we force that the two dictionaries that we are trying to create or the two mappers that we are trying to create, they both are same. , if you have to write the equation, this is equation number 1, this is equation number 2, with the control that both the mappers are or both the conversion approaches are exactly same. , then we are not using two dictionaries, right. , this essentially reduces your latency, this, right. , you and then this also reduces your overall computational load on the system, memory requirement everything, right, is it just reduces, okay. , we can translate from one language to another language with the help of a single dictionary, right. , we will see we will go through the entire algorithmic details also how things happen. , we will start with the first algorithm, a very simple algorithm, one of the early algorithms on removing the noise, okay. , we will build from there that we can understand how this speed synthesis is using the dictionary algorithm or in order or in order to go to the model works. Then we will see how we replace the noise all apart with the language all apart and it works, right. , let us understand the problem first. This was the base network for building the input or the input or model for speed synthesis, okay. , we have a particular noise out. , if you go to a relation, which are the relation, right. Background you are calling on form. It is a you are calling your family members, right. From the relation, the relationship is not there. What is happening at the other end? Your family members are listening all the way. Background, background, background, lots are are are are, right. A lot of background in the machine must be going on. Why USP is also? , if you if you if you treat USP signal as the foreground, the all other noise are background. Noise at the registration, everything is background. If I if I ask you, can you can we build an architecture which drops the entire background part and preserves only the foreground? What kind of approach you can immediately think of? Trash holding, okay. But remember, you are speaking and suddenly, PCHSA, I am mandor express platform number one, and that is going to be much louder than your voice, right. And if it for sure, it what happens? Mandor express of Garvala Gosulay Dega, your voice will be below the threshold. , some sort of noise cancellation needs to be done, right. , can you think of a nice cancellation algorithm? STFT, short term related Fourier transform. Right. , if if you use our traditional speech processing approach, we convert our audio signals to STFT form and from STFT we convert it to MSCC forms and then we essentially suppress the signal. And of the day, we are trying to do a thresholding in some other space, some other feature space, right. And and then we just say the okay now, whatever is the left over, after the thresholding, we convert using a decoder, we convert it back into the audio signal and then pass it. Hopefully again that may work, but we do not know. Okay. , obviously, when we when we are in the deep learning world, everything is deep learning, STFT and MSCC are now old school, right. , this this is what you are talking about, this particular block diagram that is over here. Essentially talks about that, we can use STFT, MFCC, then from there to we send it to a very classical old school kind of decoder, which separates out the signal and the, there is the noise. What I am interested in an encoder decoder approach, right. , we we may not need to convert anything into MSCC or STFT and followed by MSCC. Can we how many of you remember auto encoders? Lockerger has taught auto encoders in the class, right. Can someone tell me in the formulations for auto encoder very quickly. , I have an input x, okay. I have a hidden layer h and an output x dash, right. And this is also connected, right. , this part is my encoder, this part is my encoder, what is the last function? Anywhere else? MSC, MSC between. , MSC of x minus dash, this is the last function, right. , now we are going to do this, if this is my h, you say h, h is water, and it is embedding, but as you call do, here if it is the gates are w and here the gates are w dash, h is equal to w dash, x, right. And then x dash is equal to, w dash into h. Now, here we have h value, we know the h value over here, then x dash equal to w dash w, x, right. If you put this value over here, can we have a, w dash w, x, right. , given x, we need we are obtaining w dashing w. For the sake of simplicity, what we will do is, we will say that whatever w is over here, here it is a task force. Not a different weight, it is the same weight, generally the physical force here, right. In the dictionary one, in the dictionary name, we will only create one dictionary, right. , here also, it is a dictionary, mapping the current, it is a dictionary. , in place of creating two dictionary, there are two weights sets. If you, if you force it, if this w and w, and here it is w transpose will talk, right. Just weight similarity, you can quote here, h, this is the ultimate weight set equal to garnaji, h where if you, if you can say that, a transpose weight, transpose weight set equal to garnaji, then we can force w transpose, dupcone into x. , then I do not need to learn two set of weights. I can learn only one set of weights and use your dupcotransposing can, can, can, then with it, okay. , this is a classical formulation of autoencoders, encoder decoder, essentially, okay. , MSc loss, but then this is how this formulation works. , why did I take this? I just, this is noise data, our audio signal has both in noise as well as input signal, right. I have to remove the noise. Up, let us try to understand, the elizion, this is fine. Okay. , we can think of NFCC or whatever, a melastictrum or whatever, or the raw audio itself, but we need to give a one dimensional embedding, one dimensional signal, as input to my encoder, I have to place that into some embedding space and then convert it back, such that I generate the clean signal up, okay. , autoencoder and from deep learning class, if you remember autoencoder, excuse me, one of the formulation of autoencoders, one of the same as formulation of autoencoder is de-noising autoencoder. It can be de-noised the data, is the data is noisy, can I reconstruct the non-noised data, okay. , my problem exactly same, our input is noisy. , can I construct the de-noised output, can I synthesize and remove the noise out, noise out, out and keep the clean signal. , if we have my input, let's say, a bias V, which is clean, right, V clean plus a noise, background noise, okay. , what we get is V of data, okay. We need to ensure that we are removing this data, the noise happening and we construct V c back, okay. , far with me, I should take I have used xn over here, we are with me, let me convert this into x, fine. Now, my encoder job, what is it, if I have my input as xc, my, sorry, this is my input, right. , how should I write a formula or the equation for input, input, what do I do. This xc, right, sorry, right now I have xn, I need to reconstruct xc back, but what I have is xn, the relation of the noise when you are talking, okay. , xn, I obtain h, means h equal to W times xn, okay. Now, from h, I generate, what back? xn generate, correct. xn generate, correct. De-noise x, right. , I need to generate xc dash, okay. Now, I have to generate h, this is my encoder, okay. Okay. , what should happen over here? I have xc dash, okay, but just hold, I want. , what should be my output of x, xc, kendo, W transpose, remember, I will put that in place of W dash, now we can live with W transpose, fine. , W transpose into, what? Right, fine. I will be hk value k3, w of xn, here it is, x dashc, now, this is, what, is that k glass function into which k is phi 6 ? I will change the k matrix to a друзья's limit. I will charge or try to answer that in one language. Okay. then you have to move forward, you cannot, , clean signal, if the rica is constructed clean signal, we have to minimize the different two. Now, let us , let us write this xc minus 1 root transpose w into nix. , given our noisy signal and its clean counterpart, we are trying to learn the weights w and w and w transpose that I can obtain the clean signal back. This is the time of training, once the training is done, what we need essentially is an xn, it will encode and give my h from h, I will generate xc dash and this xc dash because training is over, this xc dash is a d noise. , this is what this particular last function or this particular slide highlights. Now, in this slide, there is one more small add-on. What is that small add-on? Can anyone look at the small add-on and tell me, what we have here? Noise carrier and what you are saying? Okay, we have skipped connection, it has a net style, it has a skip connection, encoded to decoder. , that is learning better we can go much deeper. Not to skip, I am talking about a loss function, focus on the loss function, there is one add-on thing in the loss function. What is that? Now, we need to add one more tone and what is that tone? And why we need to add that? , how do we know that we have just set the noise in the right place? , what is essentially we have to find to achieve here is, this first tone, it is not I will say, it is not the high-end, it is not the high-end, otherwise it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. Otherwise, it will break. What questions are you going to answer? , what are you going to anart? You are going to make the overlay as write quite correct it. , how would you do not write that as write a pointer? And how would you have it? Please become easy as time day. And, finally, okay, ok, I understood that he had any details left, we already set it Can someone tell me? Online guys. Can you interpret the second term over here? No. Is there a question? Is there a question? No. What's the name of it? What's the name of it? It's called Furnay. It's a refugee here. Is there a question? Yes. It's a question. No. No. What happened? Everything is done. Okay. This is better for online guys? Yes. Okay. If something happens, tell me. Okay. , where are we going to go? Online guys. What is the meaning of the significance and what the second term in the last function is doing? It's a regularization term. But what is it doing? Will you tell me? No. Will you give me a good tightening? Okay. Okay. Okay. , then let me tell you how is he? Now, look at his diagram again. And just imagine for a moment, we don't have a voice signal. We don't have a voice signal. And what will we do? What will we do if we have a noise? Okay. For the time being, let's understand from there. What was the name of it? The XM. Okay. We have a constant. I have to change that. What we should do is we should think of, can we have a noise signal there? And noise reconstructed. Means, we are suppressing the actual voice and reconstructing the noise back. What train the Pichayabh and Manur Express are there. That announcement we want to request. , what would be my last function? We have the original noise. Right? Whatever is my original noise, minus of the reconstructed noise. Right? Or, not the NR, NR, NR, Reconstruction. Okay. , what this tells me? What is this? But same autoencoder. If it starts doing the reconstruction of noise. Means, our first term in this...\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/cleaned_lecture_transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(cleaned_transcript)"
      ],
      "metadata": {
        "id": "XzynvDo5DDjr"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio Generation"
      ],
      "metadata": {
        "id": "98weZomsaChl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub gtts googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M4rEGuf8GnfM",
        "outputId": "cd9c4746-21e7-443e-831b-2ef014a6bac9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting gtts\n",
            "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.1.31)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gtts) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gtts) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gtts) (2.3.0)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
            "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=1ff6d7a2ef88b10eb4c49f249fdfd1a3492d1d0f00ce90a3277a154fad7e4c64\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, pydub, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, gtts, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.14.0\n",
            "    Uninstalling h11-0.14.0:\n",
            "      Successfully uninstalled h11-0.14.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.7\n",
            "    Uninstalling httpcore-1.0.7:\n",
            "      Successfully uninstalled httpcore-1.0.7\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.3.28 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.72.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 gtts-2.5.4 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 pydub-0.25.1 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "f2fe612e48584cbeb8d773a8b33a5ccf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bengali samples in my own voice are extracted from a zip file to a folder"
      ],
      "metadata": {
        "id": "rK5zzmQ5aMQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "def extract_voice_samples(zip_file_path, extract_to=\"newer_voice_samples\"):\n",
        "    if not os.path.exists(extract_to):\n",
        "        os.makedirs(extract_to)\n",
        "\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            for zip_info in zip_ref.infolist():\n",
        "                if zip_info.is_dir():\n",
        "                    continue\n",
        "\n",
        "                filename = os.path.basename(zip_info.filename)\n",
        "                if not filename:\n",
        "                    continue\n",
        "\n",
        "                if filename.lower().endswith(('.mp4', '.m4a', '.wav', '.mp3')):\n",
        "                    temp_path = os.path.join(extract_to, \"temp_\" + filename)\n",
        "\n",
        "                    with zip_ref.open(zip_info) as source, open(temp_path, \"wb\") as target:\n",
        "                        target.write(source.read())\n",
        "\n",
        "                    try:\n",
        "                        mp3_filename = os.path.splitext(filename)[0] + \".mp3\"\n",
        "                        mp3_path = os.path.join(extract_to, mp3_filename)\n",
        "                        print(f\"Converting {filename} to MP3...\")\n",
        "                        audio = AudioSegment.from_file(temp_path)\n",
        "                        audio.export(mp3_path, format=\"mp3\")\n",
        "\n",
        "                        print(f\"Converted: {mp3_path}\")\n",
        "\n",
        "                        os.remove(temp_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error converting {filename}: {e}\")\n",
        "                else:\n",
        "                    target_path = os.path.join(extract_to, filename)\n",
        "                    with zip_ref.open(zip_info) as source, open(target_path, \"wb\") as target:\n",
        "                        target.write(source.read())\n",
        "\n",
        "        print(f\"All files extracted and converted to MP3 in: {extract_to}\")\n",
        "        return extract_to\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting voice samples: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ITSXZA-FDOFn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def extract_voice_features(voice_samples_folder):\n",
        "    mp3_files = [os.path.join(voice_samples_folder, f) for f in os.listdir(voice_samples_folder)\n",
        "                 if f.endswith('.mp3')]\n",
        "\n",
        "    pitch_values = []\n",
        "    speech_rate_values = []\n",
        "    for mp3_file in mp3_files:\n",
        "        print(f\"Processing file: {mp3_file}\")\n",
        "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_wav_file:\n",
        "            temp_wav = temp_wav_file.name\n",
        "\n",
        "        audio = AudioSegment.from_mp3(mp3_file)\n",
        "        audio.export(temp_wav, format=\"wav\")\n",
        "\n",
        "        try:\n",
        "            y, sr = librosa.load(temp_wav, sr=None)\n",
        "\n",
        "            #  PYIN for more reliable pitch estimation\n",
        "            # This is better than piptrack for speech\n",
        "            f0, voiced_flag, voiced_probs = librosa.pyin(\n",
        "                y,\n",
        "                fmin=librosa.note_to_hz('C2'),\n",
        "                fmax=librosa.note_to_hz('C7'),\n",
        "                sr=sr\n",
        "            )\n",
        "\n",
        "            valid_f0 = f0[voiced_flag & (f0 > 0)]\n",
        "\n",
        "            if len(valid_f0) > 0:\n",
        "                pitch_avg = np.mean(valid_f0)\n",
        "                pitch_values.append(pitch_avg)\n",
        "                print(f\"Extracted pitch for {os.path.basename(mp3_file)}: {pitch_avg:.2f} Hz\")\n",
        "            else:\n",
        "                print(f\"No valid pitch detected for {os.path.basename(mp3_file)}\")\n",
        "\n",
        "            # Calculate speech rate based on syllable-like energy variations\n",
        "            # First, compute the envelope of the signal\n",
        "            envelope = np.abs(y)\n",
        "\n",
        "            #  low pass filter to smooth the envelope\n",
        "            envelope = librosa.util.normalize(envelope)\n",
        "            envelope = librosa.effects.preemphasis(envelope)\n",
        "\n",
        "            # Find peaks in the envelope that could correspond to syllables\n",
        "            peaks = librosa.util.peak_pick(envelope, pre_max=3, post_max=3,\n",
        "                                         pre_avg=3, post_avg=5, delta=0.1, wait=10)\n",
        "\n",
        "            # Calculate syllables per second\n",
        "            duration = len(y) / sr  # Duration in seconds\n",
        "            if duration > 0:\n",
        "                speech_rate = len(peaks) / duration\n",
        "                speech_rate_values.append(speech_rate)\n",
        "                print(f\"Extracted speech rate for {os.path.basename(mp3_file)}: {speech_rate:.2f} syllables/sec\")\n",
        "            else:\n",
        "                print(f\"Invalid duration for {os.path.basename(mp3_file)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {os.path.basename(mp3_file)}: {e}\")\n",
        "\n",
        "        finally:\n",
        "            if os.path.exists(temp_wav):\n",
        "                os.remove(temp_wav)\n",
        "\n",
        "    if pitch_values:\n",
        "        avg_pitch = np.mean(pitch_values)\n",
        "        min_pitch = np.min(pitch_values)\n",
        "        max_pitch = np.max(pitch_values)\n",
        "        pitch_variation = max_pitch - min_pitch\n",
        "        print(f\"\\nPitch statistics across {len(pitch_values)} files:\")\n",
        "        print(f\"Average: {avg_pitch:.2f} Hz\")\n",
        "        print(f\"Range: {min_pitch:.2f} - {max_pitch:.2f} Hz\")\n",
        "        print(f\"Variation: {pitch_variation:.2f} Hz\")\n",
        "    else:\n",
        "        print(\"No valid pitch measurements obtained\")\n",
        "        avg_pitch = 160  # Typical default for adult human speech\n",
        "\n",
        "    if speech_rate_values:\n",
        "        avg_speech_rate = np.mean(speech_rate_values)\n",
        "        min_rate = np.min(speech_rate_values)\n",
        "        max_rate = np.max(speech_rate_values)\n",
        "        rate_variation = max_rate - min_rate\n",
        "        print(f\"\\nSpeech rate statistics across {len(speech_rate_values)} files:\")\n",
        "        print(f\"Average: {avg_speech_rate:.2f} syllables/sec\")\n",
        "        print(f\"Range: {min_rate:.2f} - {max_rate:.2f} syllables/sec\")\n",
        "        print(f\"Variation: {rate_variation:.2f} syllables/sec\")\n",
        "    else:\n",
        "        print(\"No valid speech rate measurements obtained\")\n",
        "        avg_speech_rate = 4.0  # Typical default syllables per second\n",
        "\n",
        "\n",
        "    pitch_factor = avg_pitch /160.0\n",
        "    speed_factor = avg_speech_rate /4.0\n",
        "\n",
        "    print(f\"\\nFinal factors:\")\n",
        "    print(f\"Pitch factor: {pitch_factor:.3f}\")\n",
        "    print(f\"Speed factor: {speed_factor:.3f}\")\n",
        "\n",
        "    return {\n",
        "        \"pitch_factor\": pitch_factor,\n",
        "        \"speed_factor\": speed_factor,\n",
        "        \"raw_pitch\": avg_pitch,\n",
        "        \"raw_speech_rate\": avg_speech_rate,\n",
        "        \"pitch_variation\": pitch_variation if 'pitch_variation' in locals() else 0,\n",
        "        \"rate_variation\": rate_variation if 'rate_variation' in locals() else 0\n",
        "    }"
      ],
      "metadata": {
        "id": "QBVIFLHOJDSZ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gTTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUTwF8NCMbKN",
        "outputId": "81ffb391-4b3f-4a54-9baa-b8eca2c59cab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gTTS in /usr/local/lib/python3.11/dist-packages (2.5.4)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.32.3)\n",
            "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-cloud-translate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24VgmJMCSPNB",
        "outputId": "265e32d1-f6ba-46fb-94c8-cb0e6d5a87cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-translate in /usr/local/lib/python3.11/dist-packages (3.20.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-translate) (2.38.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-translate) (2.4.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-translate) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-translate) (5.29.4)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-translate) (0.14.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-translate) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-translate) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert bengali text to speech"
      ],
      "metadata": {
        "id": "lZ18qx3EtavY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "from gtts import gTTS\n",
        "from google.cloud import translate_v2 as translate  # Import Google Cloud Translate\n",
        "\n",
        "\n",
        "def basic_bengali_tts(bengali_text, output_file=\"basic_bengali_tts.mp3\"):\n",
        "    tts = gTTS(text=bengali_text, lang='bn', slow=False)\n",
        "    tts.save(output_file)\n",
        "    print(f\"Basic Bengali TTS saved to {output_file}\")\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "6el8Sq7gJhhp"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform to my vioce"
      ],
      "metadata": {
        "id": "tMaDk_vttXv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import tempfile\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import soundfile as sf\n",
        "\n",
        "def adapt_voice_simple(tts_audio_path, voice_features, output_file, chunk_sec=30):\n",
        "    print(f\"Adapting voice with features: {voice_features}\")\n",
        "\n",
        "    y, sr = librosa.load(tts_audio_path, sr=None)\n",
        "\n",
        "    # Calculate pitch shift in semitones based on the pitch factor\n",
        "    raw_steps = 12 * np.log2(voice_features[\"pitch_factor\"])\n",
        "    pitch_semitones = float(np.clip(raw_steps, -3, 3))  # Clamp to a reasonable range\n",
        "    print(f\"Shifting pitch by {pitch_semitones:.2f} semitones\")\n",
        "\n",
        "    # Apply pitch shift\n",
        "    y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=pitch_semitones)\n",
        "\n",
        "    # Apply speed adjustment (adjusting speed without changing pitch)\n",
        "    speed_factor = float(np.clip(voice_features[\"speed_factor\"], 0.8, 1.2))\n",
        "    print(f\"Applying speed factor: {speed_factor:.2f}×\")\n",
        "    if abs(speed_factor - 1.0)> 0.02:\n",
        "        y_shifted = librosa.effects.time_stretch(y_shifted, rate=speed_factor)\n",
        "\n",
        "    sf.write(\"temp.wav\", y_shifted, sr)\n",
        "\n",
        "    AudioSegment.from_wav(\"temp.wav\").export(output_file, format=\"mp3\")\n",
        "    print(f\"Voice-adapted audio saved to {output_file}\")\n",
        "    os.remove(\"temp.wav\")\n",
        "\n",
        "    return output_file"
      ],
      "metadata": {
        "id": "fbrluU6gJoxP"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This translates the text to bengali only"
      ],
      "metadata": {
        "id": "STJjdTvDtSGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_to_bengali(text_to_translate):\n",
        "    translate_client = translate.Client()\n",
        "    try:\n",
        "        result = translate_client.translate(text_to_translate, target_language=\"bn\")\n",
        "        bengali_text = result[\"translatedText\"]\n",
        "        print(f\"Bengali translation: {bengali_text}\")\n",
        "        return bengali_text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        print(\"Using original text as fallback.\")\n",
        "        return text_to_translate"
      ],
      "metadata": {
        "id": "lP3mbVSrKhLb"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_bengali_tts(text_to_translate, zip_file_path):\n",
        "    voice_samples_folder = extract_voice_samples(zip_file_path)\n",
        "    if not voice_samples_folder:\n",
        "        print(\"Failed to extract voice samples. Using basic TTS without adaptation.\")\n",
        "        return basic_bengali_tts(text_to_translate)\n",
        "\n",
        "    # Translate to Bengali\n",
        "    bengali_text = translate_to_bengali(text_to_translate)\n",
        "\n",
        "    # Generate basic Bengali TTS\n",
        "    print(\"Generating basic Bengali TTS...\")\n",
        "    temp_tts = basic_bengali_tts(bengali_text, \"temp_bengali_tts.mp3\")\n",
        "\n",
        "    # Extract voice features from samples\n",
        "    print(\"Extracting voice features from your recordings...\")\n",
        "    voice_features = extract_voice_features(voice_samples_folder)\n",
        "    print(f\"Extracted voice features: {voice_features}\")\n",
        ""
      ],
      "metadata": {
        "id": "BTe9rg89bjsK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/drive/MyDrive/bengali_samples.zip\"\n",
        "output_audio = process_bengali_tts(cleaned_transcript, zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JrkgC6YuKpRE",
        "outputId": "3ebb5db8-7c4a-4518-bccb-37ce8b59bc79"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting b1.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b1.mp3\n",
            "Converting b10.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b10.mp3\n",
            "Converting b2.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b2.mp3\n",
            "Converting b3.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b3.mp3\n",
            "Converting b4.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b4.mp3\n",
            "Converting b5.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b5.mp3\n",
            "Converting b6.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b6.mp3\n",
            "Converting b7.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b7.mp3\n",
            "Converting b8.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b8.mp3\n",
            "Converting b9.mp4 to MP3...\n",
            "Converted: newer_voice_samples/b9.mp3\n",
            "Converting WhatsApp Audio 2025-04-18 at 15.33.25.mp4 to MP3...\n",
            "Converted: newer_voice_samples/WhatsApp Audio 2025-04-18 at 15.33.25.mp3\n",
            "Converting WhatsApp Audio 2025-04-18 at 15.33.26.mp4 to MP3...\n",
            "Converted: newer_voice_samples/WhatsApp Audio 2025-04-18 at 15.33.26.mp3\n",
            "All files extracted and converted to MP3 in: newer_voice_samples\n",
            "Bengali translation: , তুমি আমার কথাও ব্যবহার করছো। , অনলাইন শিক্ষার্থীরা, তুমি কি সবুজ মডেলটি দেখতে পাচ্ছো? , কিছু সূত্র, সেগুলো স্লাইডে স্ক্রিনে থাকবে। , আসুন আমরা বক্তৃতা সংশ্লেষণ সম্পর্কে অনুপ্রাণিত হই। আমি নিশ্চিত ডঃ চাই ইতিমধ্যে বক্তৃতা সংশ্লেষণ সম্পর্কে কথা বলেছেন এবং তারপর ক্লাস করেছেন, তবে আমরা এখানে একটু বিস্তারিত আলোচনা করব। , বক্তৃতা সংশ্লেষণে দুটি প্রধান অংশ রয়েছে। প্রথম অংশটি অনুপ্রাণিত হয়ে আপনি এই বিশেষ ভিডিও থেকে ধারণা পেতে পারেন। আমি ইংরেজি বলছি, কিন্তু আমি বাউলে নেই, তবে আমি ওলা প্যালারের সাথে আছি। আমি ফরাসি বলতে পারি। আমি এখানে যে বিভিন্ন ভাষায় কথা বলতে পারি তার কয়েকটি উদাহরণ মাত্র। , এখন মনে আছে যখন আমাদের প্রধানমন্ত্রী একবার ট্যালন, থ্যালেরু, কান্দারা এবং অন্যান্য সমস্ত ভাষায় একাধিক ভাষায় গিভ আ লাইফ টেলিকাস্টে কথা বলতে কোডার এবং ডিকোডার ব্যবহার করেছিলেন। সে তাদের কাউকেই চেনে না, সম্ভবত ঠিক, যখন সে তাদের দুজনের কাউকেই সাবলীলভাবে বলতে পারে না, কিন্তু তখন লাইফ টেলিকাস্টে সে কথা বলছিল, সম্ভবত সে ছিল না, সে হিন্দিতে কথা বলছিল, কিন্তু লাইফ টেলিকাস্টে একাধিক ভাষায় হচ্ছিল। , প্রায় বাস্তব সময়ে লাইফ টেলিকাস্ট হচ্ছিল এবং এটি একটি উদাহরণ যে কেউ বিভিন্ন ভাষায় প্রায় বাস্তব সময়ে কথা বলতে পারে। , আপনার ভাষা জানার প্রয়োজন নাও হতে পারে। আরেকটি উদাহরণ হল ধরুন আমি জাপান ভ্রমণ করছি, আমি জাপানি জানি না, কিন্তু যদি আমাকে যোগাযোগ করতে হয়, বর্তমানে যখন আপনি জাপান বা চীন কোথাও যান, তখন আপনি আপনার ফোন ব্যবহার করেন এবং তারপর সেখানকার স্থানীয় মানুষদের সাথে সেই কথোপকথন করেন, যারা সেখানে ইংরেজি জানেন না। , আপনি ইংরেজিতে কথা বলেন এবং তারপর আপনি একটি TTS মডেল ব্যবহার করেন বা জাপানি ভাষায় রূপান্তর করার জন্য সংশ্লেষণ মডেল বলেন। তারা জাপানি ভাষায় কথা বলে, তারা সেখান থেকে বোঝে, তারা জাপানি ভাষায় কথা বলে এবং তারপর TTS মডেল দেয় অথবা এই সংশ্লেষণ মডেল দেয় আমরা ইংরেজি সংস্করণ ফিরিয়ে দেই, ঠিক আছে। এখন, ক্ষেত্রটি অনেক এগিয়ে গেছে এবং এখানে দ্বিতীয় অংশ। , আগের অংশে ধারণাটি ছিল যে আমি ইংরেজিতে কথা বলছি এবং আমাকে হিন্দিতে অনুবাদ করা হচ্ছে, তাই না, অথবা ফরাসি বা স্প্যানিশ ভাষায়। এটি ছিল বক্তৃতা থেকে বক্তৃতা সংশ্লেষণ। অন্যটি হতে পারে যে এটি কার চিত্র? সিএমও কোথায়? যত্ন এবং যত্ন নয়। , তামিল ভাষা। তামিল ভাষা, ঠিক আছে। , কিন্তু এখানে মূল কথা হল তিনি আর নেই। ভারতীয় নির্বাচনে, যদিও এটি ডিফেকের একটি উদাহরণ, কিন্তু ভারতীয় নির্বাচনে তার ভিডিও প্রচারিত হয়েছিল, ঠিক আছে, একটি নির্দিষ্ট বার্তা দিচ্ছিল, ঠিক আছে। , কী হত? আমাকে ভিডিওটি চালাতে দিন এবং তারপর আপনি অনুমান করতে পারেন, ঠিক আছে। এটি অনেক দীর্ঘ। ভিডিও, কিন্তু তারপর আপনি ধারণাটি পেয়েছেন, ঠিক আছে। এখন, ব্যক্তিটি আর নেই। আমাদের কাছে ব্যক্তি, প্রচুর ভিডিও ক্লিপ এবং অডিও সংকেত রয়েছে। আমি শিখতে পারি যে কেউ কীভাবে কিছু বলতে পারে। এখন, আমি যা করতে পারি তা হল, যদি আমার কাছে ফাউন্ডেশন মডেল ব্যবহার করা হয় অথবা এই আধুনিক AI অ্যালগরিদমগুলির যেকোনো একটি ব্যবহার করা হয়, তাহলে আমি তাকে যা বলতে চাই তা টাইপ করতে পারি। আমার একটি প্রশিক্ষিত মডেল আছে এবং সেই প্রশিক্ষিত মডেল এই টেক্সটটি নেয় এবং তারপর এই সমস্ত তথ্য একটি ভয়েস সিগন্যালে রূপান্তরিত করে, একটি তরঙ্গরূপ, ঠিক আছে। , এটি টেক্সট টু স্পিচ। , এটি দ্বিতীয় ধরণের স্পিচ সংশ্লেষণ অংশ, ঠিক আছে। , পাখির চোখের দৃষ্টি, মূলত আমাদের কাছে দ্বিতীয় অংশের খুব প্রচলিত উদাহরণ রয়েছে যখন আমরা টেক্সট প্রদান করছি এবং আমরা অডিও সাউন্ড তৈরি করছি, ঠিক আছে। যদিও উভয় উদাহরণই আমার কাছে ভিডিও আছে, কিন্তু আপাতত ভিডিওগুলি দেখবেন না, এর অডিওও সমানভাবে গুরুত্বপূর্ণ এবং সেগুলি সবই লিপ সিঙ্ক এবং অন। , এখন যদি আপনাকে এমন একটি সফ্টওয়্যার, এমন একটি টুল তৈরি করতে হয়, তাহলে আমরা কীভাবে এটি করব? টেক্সট টু স্পিচ সহজতর, স্পিচ টু স্পিচ কিছুটা কঠিন এবং আমরা উভয় সম্পর্কেই কথা বলব, যদি আপনি টেক্সট টু স্পিচ দিয়ে শুরু করেন, তাহলে প্রথম জিনিসটি হল আমাদের একটি ম্যাপিং, কিছু ধরণের ম্যাপিং, টেক্সট ওয়ার্ড এবং স্পিচ ওয়ার্ড সম্পর্কে কিছু ধরণের বোঝাপড়া থাকা দরকার। , যদি না তুমি আমার বোর্ড দেখতে পাও, সময় লিখতে পারো, আমি কী লিখছি। , যদি এটা আমার টেক্সট স্পেস হয়, এটা আমার ফিচার স্পেস, ঠিক আছে এবং এটা সব অডিও ভয়েস, তাই না। আমার যা দরকার, আমি কি এখান থেকে এখানে এবং এখান থেকে এখানে রূপান্তর করতে পারি? , আমার একটি ম্যাপিং শিখতে হবে, আমাকে টেক্সট, T1 এবং স্পিচের মধ্যে একটি ম্যাপিং শিখতে হবে, এমনভাবে যাতে T1 n গুণ S1 এর সমান হয়, তাই না। , আমার একটি ম্যাপিং ফাংশন শিখতে হবে যা আমার টেক্সট এম্বেডিংগুলিকে আমার স্পিচ এম্বেডিংয়ে রূপান্তর করতে পারে এবং স্পিচ এম্বেডিং থেকে, আমি একটি ডিকোডিং করতে পারি এবং একটি স্পিচ তৈরি করতে পারি, তাই না। , যদি আমরা আমাদের একটি স্পিচ বলতে দেই, রিশাম সেই স্পিচ এনকোডারগুলির কিছু শিখিয়েছে, তাই না। , একবার আমাদের এম্বেডিং হয়ে গেলে, আমি এটিকে ডিকোড করার জন্য ব্যবহারকারী ডিকোডারে রূপান্তর করতে পারি, তাই না। , অনেক স্পষ্ট, এখন চ্যালেঞ্জগুলি কী, কোন চ্যালেঞ্জগুলি, চ্যালেঞ্জগুলি কী, এটি এত সহজ নয়। দেখুন, যদি আমরা টেক্সট টু টেক্সট এম্বেডিং সম্পর্কে কথা বলি। ঠিক আছে, NLU সম্পর্কে, বিষয় সম্পর্কে কথা বলি। NLU আমার এলাকা নয়, তবে অন্তত খুব মৌলিক ধারণা থেকে। যদি আমার লাইন থাকে, তাহলে ইংরেজিতে এটাই টেক্সট, ঠিক আছে। আর আমার হিন্দিতে একটা টেক্সট আছে, তাই না। আমি E2H এর মধ্যে কিছু ম্যাপিংয়ের মধ্যে খুব সহজ ম্যাপিং শিখতে পারি, যদি আমি হিন্দিতে একটা টেক্সট প্রদান করি, তাহলে ম্যাপিং দ্বারা প্রক্রিয়াকরণ করে ইংরেজিতে টেক্সট তৈরি করতে পারি। , এটি একটি অভিধান, ঠিক আছে। , যদি আমরা এটা করি, তাই না। , ইংরেজি অভিধানে, আমরা সবাই আমাদের শৈশবের দিনগুলিতে করেছি, এটি আমার ইংরেজি শব্দ, এর হিন্দি অংশ কী, ঠিক আছে বা বিপরীতভাবে। , যদি আমরা ব্যবহার করতে পারি, তাহলে এই ম্যাপিংটি সেই অভিধান ছাড়া আর কিছুই নয়। , টেক্সটে, আমরা এটি অনেকবার করেছি। এখন, যদি আপনাকে অডিওতে এটি করতে হয়, তাহলে পয়েন্টগুলি হল, অডিও ক্লিপের সময়কাল কত, ঠিক আছে। কেউ ব্যক্তির উচ্চারণ কীভাবে, কেউ কীভাবে কথা বলছে, ঠিক আছে। এমনকি অনেক জটিল বিবরণ, উদাহরণস্বরূপ, কেউ বাক্যগুলিকে কীভাবে পর্যায়ক্রমে বর্ণনা করছে। এবং তারপর যখন আপনি কথা বলছেন, ঠিক আছে, আমরা যখন কথা বলি, তখন আমরা এই ধরণের অনেক শব্দ উচ্চারণ করি। এবং এটিও ঘটে যদি আপনার ইংরেজি খুব সাবলীল না হয় বা ভাষা, দ্বিতীয় ভাষা খুব সাবলীল না হয়, যদি এটি মাতৃভাষা না হয়, তবে আপনার অনুবাদে কিছু ত্রুটি রয়েছে, কারণ আপনার মনের পিছনে, আপনার একটি অভিধানও রয়েছে, ঈশ্বর অভিধানটি দিয়েছেন, ঠিক আছে, মস্তিষ্কে। আপনি যা করছেন তা মূলত, আপনি আপনার মস্তিষ্কে হিন্দিতে কথা বলছেন এবং তারপরে সেই অভিধানটি রূপান্তরিত হচ্ছে এবং রূপান্তরের পরে, যখন আপনি কথা বলছেন, ঠিক আছে, বিস্তৃতভাবে। , এতে কিছু ত্রুটি থাকতে পারে, কারণ আপনার অভিধানে, সেই অনুবাদের কিছু অংশ একেবারে সঠিক নাও হতে পারে। , সেই ত্রুটিগুলিও আসতে পারে, যখন আপনার হিন্দি সম্পূর্ণরূপে ঠিক আছে, তবে আপনার ইংরেজি অনুবাদ খুব সূক্ষ্ম নাও হতে পারে। , কীভাবে কমানো যায়, তাহলে মূলত, এই পুরো বিষয়টি শব্দ থেকে উচ্চারণ পর্যন্ত আসে। আমাদের এই সমস্ত বৈচিত্র্যের যত্ন নিতে হবে এবং সেখানেই ম্যাপিং চিত্রিত হয়। আসুন কিভাবে নিশ্চিত করা যায় যে বিভিন্ন উচ্চারণের বৈচিত্র্য, বিভিন্ন স্টাইলের বৈচিত্র্যের এই ম্যাপিংটিও বোঝা যায় এবং ম্যাপ করা যায়, অন্যদিকে সঠিকতাও সমানভাবে গুরুত্বপূর্ণ। , এটি আরেকটি দৃষ্টিভঙ্গি, একটি গবেষণাপত্রে উল্লেখিত দৃষ্টিভঙ্গিগুলির মধ্যে একটি, যেখানে আপনার একটি টেক্সট মডিউল আছে, আপনি এটি আপনার এম্বেডিং লেয়ারে দেন, অ্যানেলু লেয়ার, অ্যানেলু লেয়ার আপনাকে ভাষাগত বৈশিষ্ট্য দেয়, মূলত এম্বেডিং। এবং তারপর সেখান থেকে, আপনি কিছু পদ্ধতি মডেলিং করেন, আপনি তরঙ্গরূপ তৈরির পুরো প্রক্রিয়াটি অতিক্রম করেন এবং ট্যাপেট্রোল মূলত টেক্সটকে মেল্ড করার জন্য, MFCC সহগ নিষ্কাশন। , যদি আপনার একটি টেক্সট থাকে এবং আপনি এটিকে একটি MFCC সহগে রূপান্তর করেন, তাহলে মডেলটিকে মূলত একটি অভিধান হিসেবে প্রশিক্ষণ দেওয়া হয়েছে। , , সেই অভিধান তৈরির বিশেষ স্টাইল, যেখানে আমরা সবকিছুকে একটি মেল্ড ফ্রিকোয়েন্সি বর্ণালীতে রূপান্তর করছি। আর তারপর তোমার কাছে একটা ভোকোডার আছে, একটা ওয়েভ নেট অথবা একটা ওয়েভ গ্লো। তুমি তোমার পছন্দের যেকোনো ভোকোডার ব্যবহার করতে পারো যা মূলত কনভার্ট হয়, এটা একটা ভয়েস ডিকোডার এবং সেটা হলো ভোকোডারের সংক্ষিপ্ত রূপ হলো ভয়েস ডিকোডার। তুমি একটা ভয়েস সিগন্যালে ডিকোড করো, যেখানে তুমি একটা ভয়েস সিগন্যালের এন্ড টু এন্ড সংশ্লেষণ করো যেখান থেকে ইনপুট তোমার টেক্সট। , এটা পাখির আইভি। , এটা স্পিচ সিন্থেসিসের প্রথম অংশ, অথবা একটা অংশ। কোন প্রশ্ন আছে? অনলাইন বন্ধুরা, যদি না হয় তাহলে আমি এটা অনুসরণ করব। আর আমি আশা করি আমি শ্রবণযোগ্য এবং সবকিছু ঠিকঠাক চলছে। ঠিক আছে। , , আমরা একই ধরণের ক্ষেত্রেও কাজ করছি এমন একটি জিনিস, কিন্তু আমরা সংশোধনের কয়েকটি ধাপ যোগ করেছি। , আমরা মূলত যা করেছি তা হল স্পিচ টু স্পিচ অথবা আমরা যা করার চেষ্টা করছি তা হল স্পিচ টু স্পিচ। আমার মনে হয় রিশাম তোমাদের শিখিয়েছে, তাই না? , সে এই বিষয়ে কাজ করছে। , আমরা যা করব তা হল, ধরুন আমি এখন আলোচনা করছি এবং রেকর্ডিংয়ে আমাদের কাছে একটি ট্রান্সক্রিপশন বিকল্প থাকবে। তারপর, যখন আমরা রেকর্ডিং করছি, তখন আমরা আমাদের জন্য একটি ঝুড়ি তৈরি করতে পারি। , আমি যা বলছি, গুগল তা ট্রান্সক্রাইব করবে। যদি আপনি রূপান্তর না করেন, তবে এটি কেবল ট্রান্সক্রিপশন করবে। এটি একটি সুবিধা উপলব্ধ। , যদি আমি কিছু বলি, একটি স্বয়ংক্রিয় বক্তৃতা স্বীকৃতি মডিউল কেবল এটি ট্রান্সক্রাইব করতে পারে। এখন, যখন আমি ট্রান্সক্রিপশন করছি, হয় আমি নির্দিষ্ট বাক্যটি বলতে ভুল করেছি অথবা ট্রান্সক্রিপশন একটি ত্রুটি তৈরি করেছে। ধরুন, আমি বলেছি যখন চাঁদ সূর্য এবং পৃথিবীর মধ্যে আসে, তখন সূর্যগ্রহণ ঘটে। কিন্তু, আমি হয়তো ভুল করেছি এবং আমি বলেছি যখন চাঁদ সূর্য এবং এর মধ্যে আসে, তাই না? অথবা মডিউল, গুগল ট্রান্সলেটর হয়তো আমাকে &quot;আসা&quot; বলতে মিস করেছে, এটি হয়তো &quot;আসা&quot; শুনেছে। এবং, এটি হল ট্রান্সক্রিপশন। এখন, এখানে ট্রান্সক্রিপশনে একটি ত্রুটি রয়েছে। হয় আমরা এটি ম্যানুয়ালি সংশোধন করতে পারি অথবা আমরা ভাষা সংশোধনের একটি মডিউল তৈরি করি অথবা ব্যাকরণ সংশোধন করি। , আমরা ব্যাকরণ সংশোধন করতে পারি এবং তারপর মেশিন অনুবাদ আমাদের টেক্সট থেকে টেক্সট থেকে টেক্সটে টেক্সটে আসার মডিউলটি গ্রহণ করতে পারে। এখন, ইংরেজি থেকে, মেশিন অনুবাদ হিন্দিতে কিছু করতে পারে। , ইংরেজি থেকে হিন্দিতে, আমাদের এখন একটি অনুবাদ আছে। যখন চন্দ্রমার, সূর্য সূর্যের মাঝখানে আসে, তখন সূর্য অন্ধকার হয়ে যায়। এখন, এই অনুবাদটি একটি ত্রুটি করেছে। এখন, আমাদের একটি সংশোধন মডিউল প্রয়োজন। এটি একটি ম্যানুয়াল সংশোধন হতে পারে অথবা এটি সাধারণত সর্বদা ম্যানুয়াল নাও হতে পারে, তবে দ্বিতীয় ভাষায় ব্যাকরণ সংশোধন করার জন্য একটি স্বয়ংক্রিয় মডিউল থাকা উচিত। , ব্যাকরণ সংশোধনের পরে দ্বিতীয় ভাষা এখন উপলব্ধ, সংশোধনের ঠিক পরে এবং তারপরে আমরা একটি টেক্সট টু স্পিচ মডিউল যোগ করতে পারি। , এই পদ্ধতিতে, আমরা কী করছি? , ইংরেজি থেকে ইংরেজি, তাই না। , ইংরেজি, কেন ইংরেজি টেক্সট, তারপর ইংরেজি টেক্সট থেকে হিন্দি টেক্সট, এবং হিন্দি টেক্সট থেকে হিন্দি, কেন সঠিক। এটি ছিল প্রথম পঠন অনুপাতের অংশ এক। যদি আমাকে এটি করতে হয়, তবে এটি সাধারণত সেই পথ যা করার কথা ভাবতে পারে। সবচেয়ে সহজ পদ্ধতি যা আমি বলব অথবা প্রথম চিন্তা, আমরা এটি করতে পারি, এতে সমস্যা কী? লেটেন্সি, আর কী? ত্রুটি, তাই না। , যেকোনো ধাপে স্বয়ংক্রিয় বক্তৃতা স্বীকৃতি ত্রুটি তৈরি করলে তা সংশোধন করা প্রয়োজন। মেশিন অনুবাদ ত্রুটি সংশোধন করা প্রয়োজন, তাই না। এবং মেশিন এবং তারপর, আবার টেক্সট থেকে স্পিচে একটি অনুবাদ সংস্থা আছে। , আরও ত্রুটির সম্ভাবনার জন্য অনেক সুযোগ বা অনেক ব্লক, তাই না। , যখন আমরা আরও ডেটা তৈরির জন্য এই বাভোহাশাক প্রকল্পটি ব্যবহার করছি। আমরা ডেটা তৈরি করার জন্য এই ধরণের পাইপলাইন ব্যবহার করছি, স্পিচ থেকে স্পিচ সংশ্লেষণের সমস্যা সমাধানের জন্য নয়, ঠিক আছে। , আমরা যা বুঝতে পেরেছি, একটি হল স্পিচ থেকে স্পিচ সংশ্লেষণ, একটি বা অন্যটি হল টেক্সট থেকে স্পিচ সংশ্লেষণ। এবং স্পিচ থেকে স্পিচ সংশ্লেষণ একটি টেক্সট থেকে স্পিচ সংশ্লেষণ দ্বারা করা যেতে পারে। , এখানে আমরা যা করেছি তা হল স্পিচ থেকে টেক্সট, তারপর টেক্সট থেকে টেক্সট এবং তারপর টেক্সট থেকে স্পিচ, ঠিক আছে। যাইহোক, আমরা সবাই যেমন বলেছি, প্রচুর ল্যাটেন্সি, ত্রুটির সুযোগ, ত্রুটির সুযোগ এবং ত্রুটি প্রচারিত হতে পারে এবং আরও বড় ত্রুটি থাকতে পারে। কেন আমরা সরাসরি স্পিচ টু স্পিচ সিনথেসিস করতে পারি না? , , প্রথমত, ল্যাটেন্সি বন্ধ হয়ে যাবে। আমাদের অন্য কোনও ডোমেইনে রূপান্তর করার দরকার নেই, ঠিক আছে। এখন, আমরা এখানে যা করেছি তা হল টেক্সট টু ভয়েস, এটি নির্দিষ্ট ম্যাপিং শেখার সাথে সম্পর্কিত হবে। এখন, যদি আমরা বলি যে আমাদের এই দুটি স্পিচ আছে, আমরা বলি, একটি স্পেসের জন্য, এটি আমার ভয়েস এবং পারস্পরিক সম্পর্ক, ভয়েস এবং হিন্দি, ঠিক আছে। , সমস্যা বিবৃতিটি এখনও একই রকম হতে চলেছে অথবা এমনকি আমরা আপনাকে বলতে সাহায্য করার চেষ্টা করছি যে, ঠিক আছে, সেই টেক্সট টু স্পিচ, টেক্সট টু স্পিচের ধারণাটি টেক্সট টু স্পিচের ধারণার অনুরূপ অথবা স্পিচ টু স্পিচের ধারণাটি ম্যাপিংয়ে টেক্সট ইন স্পিচের অনুরূপ। আমাদের এক ডোমেইন থেকে অন্য ডোমেইন রূপান্তরের পুরো চক্রের মধ্য দিয়ে যেতে হবে না, ঠিক আছে। , এখন কি হবে? , এখন আমাদের দুটি ডোমেইন আছে, উভয় ডোমেইনই অডিও সিগন্যাল, ঠিক আছে। এখন, আমার এই ডোমেইন, আমাকে আমদানি করতে হবে, ঠিক আছে। আমার ভয়েস সিগন্যাল, যা একটি ইংরেজি, ভয়েস সিগন্যালে আমদানি করতে হবে, যা একটি হিন্দি এবং ব্যাটারি কাস্টম ব্যাক, উভয় সিগন্যালই করতে হবে, তাই না। , যদি আমি এটি করার সময়, আমি যা করার চেষ্টা করছি, E এর V হল N এর E এর সমান, V এর H এর E এর সমান, ঠিক আছে। যখন আমি ফিরে আসছি, আমি কী করব, তুমি কি করছো? , এটি হল ফরোয়ার্ড E রূপান্তর, তাই না। , V এর E সমান M এর EH এর VH তে, তাই না। এখন, যখন আমি এটি করছি, আমরা কী করার চেষ্টা করছি? V এর H সমান M এর EH, HT, তাই না। ইয়াহা, এটি a, এটি একটি ভিন্ন মানচিত্র হতে পারে, তাই না। এটি ভিন্ন ম্যাপিং পদ্ধতি হতে পারে, দুঃখিত। , যখন আমি এখান থেকে এখানে আসছি, আমাদের যা আছে তা হল আমাদের VH এর সমান M এর HE কে VE তে জেনারেট করতে হবে, ঠিক আছে। , V কে V হিসাবে দেওয়া হয়েছে আমরা VH জেনারেট করছি, ঠিক আছে। VH দেওয়া হয়েছে, আমরা V জেনারেট করছি, তাই না। , এটা হবে E এর V এর সমান M এর সমান HE এর মধ্যে, দুঃখিত, VH থেকে VH, ঠিক আছে। এখন, আপনার কি দুটি ভিন্ন আমদানিকারক থাকতে হবে, দুঃখিত, দুটি ভিন্ন ম্যাপার, যথেষ্ট একটি অভিধান, তাই না? আপনার উভয় অভিধানের প্রয়োজন নেই। এখন, আমরা এখানে মূলত ঈর্ষা শক্তি। আমরা কি জোর করে বলতে পারি যে আমরা যে দুটি অভিধান তৈরি করার চেষ্টা করছি অথবা যে দুটি ম্যাপার তৈরি করার চেষ্টা করছি, তারা উভয়ই একই। , যদি আপনাকে সমীকরণটি লিখতে হয়, এটি সমীকরণ নম্বর 1, এটি সমীকরণ নম্বর 2, নিয়ন্ত্রণের সাথে যে উভয় ম্যাপার বা উভয় রূপান্তর পদ্ধতি ঠিক একই। , তাহলে আমরা দুটি অভিধান ব্যবহার করছি না, তাই না। , এটি মূলত আপনার ল্যাটেন্সি হ্রাস করে, এই, তাই না। , আপনি এবং তারপর এটি সিস্টেমের উপর আপনার সামগ্রিক গণনামূলক লোড, মেমরির প্রয়োজনীয়তা সবকিছু হ্রাস করে, তাই না, এটি কি কেবল হ্রাস করে, ঠিক আছে। , আমরা একটি একক অভিধানের সাহায্যে এক ভাষা থেকে অন্য ভাষায় অনুবাদ করতে পারি, তাই না। , আমরা দেখব আমরা সম্পূর্ণ অ্যালগরিদমিক বিবরণের মধ্য দিয়ে যাব এবং কীভাবে জিনিসগুলি ঘটে। , আমরা প্রথম অ্যালগরিদম দিয়ে শুরু করব, খুব সহজ অ্যালগরিদম, শব্দ অপসারণের প্রাথমিক অ্যালগরিদমগুলির মধ্যে একটি, ঠিক আছে। , আমরা সেখান থেকে তৈরি করব যাতে আমরা বুঝতে পারি যে এই গতি সংশ্লেষণটি অভিধান অ্যালগরিদম ব্যবহার করে বা অর্ডার বা অর্ডার মডেলে যাওয়ার জন্য কীভাবে কাজ করে। তারপর আমরা দেখব কিভাবে আমরা শব্দকে ভাষা সম্পূর্ণ আলাদা করে প্রতিস্থাপন করি এবং এটি কাজ করে, তাই না। , প্রথমে সমস্যাটি বুঝতে পারি। এটি ইনপুট তৈরির জন্য বেস নেটওয়ার্ক ছিল অথবা গতি সংশ্লেষণের জন্য ইনপুট বা মডেল, ঠিক আছে। , আমাদের একটি নির্দিষ্ট শব্দ আছে। , যদি আপনি একটি সম্পর্কে যান, যা সম্পর্ক, তাই না। ব্যাকগ্রাউন্ড ফর্মে আপনি কল করছেন। এটি এমন একটি যা আপনি আপনার পরিবারের সদস্যদের কল করছেন, তাই না। সম্পর্ক থেকে, সম্পর্কটি সেখানে নেই। অন্য প্রান্তে কী ঘটছে? আপনার পরিবারের সদস্যরা পুরো পথ শুনছেন। ব্যাকগ্রাউন্ড, ব্যাকগ্রাউন্ড, ব্যাকগ্রাউন্ড, লটগুলি আছে, আছে, তাই না। মেশিনে অবশ্যই অনেক ব্যাকগ্রাউন্ড চলছে। কেন ইউএসপিও আছে? , যদি আপনি যদি ইউএসপি সিগন্যালকে ফোরগ্রাউন্ড হিসাবে বিবেচনা করেন, তাহলে অন্যান্য সমস্ত শব্দ ব্যাকগ্রাউন্ড। রেজিস্ট্রেশনের সময় শব্দ, সবকিছুই ব্যাকগ্রাউন্ড। যদি আমি আপনাকে জিজ্ঞাসা করি, আপনি কি এমন একটি স্থাপত্য তৈরি করতে পারেন যা পুরো ব্যাকগ্রাউন্ড অংশটি বাদ দেয় এবং কেবল অগ্রভাগ সংরক্ষণ করে? আপনি তাৎক্ষণিকভাবে কী ধরণের পদ্ধতির কথা ভাবতে পারেন? ট্র্যাশ হোল্ডিং, ঠিক আছে। কিন্তু মনে রাখবেন, আপনি কথা বলছেন এবং হঠাৎ, PCHSA, আমি ম্যান্ডোর এক্সপ্রেস প্ল্যাটফর্ম নম্বর ওয়ান, এবং এটি আপনার কণ্ঠস্বরের চেয়ে অনেক বেশি জোরে হতে চলেছে, তাই না। এবং যদি এটি নিশ্চিত হয়, তবে কী হবে? গারভালা গোসুলে দেগার ম্যান্ডোর এক্সপ্রেস, আপনার কণ্ঠস্বর থ্রেশহোল্ডের নীচে থাকবে। , এক ধরণের শব্দ বাতিল করা দরকার, তাই না। , আপনি কি একটি সুন্দর বাতিলকরণ অ্যালগরিদমের কথা ভাবতে পারেন? STFT, স্বল্পমেয়াদী সম্পর্কিত ফুরিয়ার রূপান্তর। ঠিক আছে। , যদি আপনি আমাদের ঐতিহ্যবাহী বক্তৃতা প্রক্রিয়াকরণ পদ্ধতি ব্যবহার করেন, আমরা আমাদের অডিও সংকেতগুলিকে STFT ফর্মে রূপান্তর করি এবং STFT থেকে আমরা এটিকে MSCC ফর্মে রূপান্তর করি এবং তারপরে আমরা মূলত সংকেতটি দমন করি। এবং দিনের কথা, আমরা অন্য কোনও স্থানে, অন্য কোনও বৈশিষ্ট্য স্থানে একটি থ্রেশহোল্ডিং করার চেষ্টা করছি, তাই না। এবং তারপর আমরা শুধু বলি ঠিক আছে এখন, থ্রেশহোল্ডিংয়ের পরে যা বাকি আছে, আমরা একটি ডিকোডার ব্যবহার করে রূপান্তর করি, আমরা এটিকে অডিও সিগন্যালে রূপান্তর করি এবং তারপর এটি পাস করি। আশা করি এটি আবার কাজ করবে, কিন্তু আমরা জানি না। ঠিক আছে। স্পষ্টতই, যখন আমরা যখন গভীর শিক্ষার জগতে থাকি, তখন সবকিছুই গভীর শিক্ষা, STFT এবং MSCC এখন পুরানো স্কুল, তাই না। , আপনি এটি সম্পর্কে কথা বলছেন, এই বিশেষ ব্লক ডায়াগ্রামটি এখানে। মূলত এটি সম্পর্কে কথা বলে, আমরা STFT, MFCC ব্যবহার করতে পারি, তারপর সেখান থেকে আমরা এটি একটি খুব ধ্রুপদী পুরানো স্কুল ধরণের ডিকোডারে পাঠাতে পারি, যা সংকেতকে আলাদা করে এবং, সেখানে শব্দ হয়। আমি একটি এনকোডার ডিকোডার পদ্ধতিতে যা আগ্রহী, তাই না। , আমাদের হয়তো MSCC বা STFT তে কিছু রূপান্তর করার প্রয়োজন হবে না এবং তারপরে MSCC অনুসরণ করতে হবে। তোমাদের মধ্যে কতজন অটো এনকোডার মনে রাখতে পারে? লকারগার ক্লাসে অটো এনকোডার শিখিয়েছে, তাই না। কেউ কি আমাকে অটো এনকোডারের ফর্মুলেশনে খুব দ্রুত বলতে পারেন। , আমার কাছে একটি ইনপুট x আছে, ঠিক আছে। আমার কাছে একটি লুকানো স্তর h এবং একটি আউটপুট x ড্যাশ আছে, তাই না। এবং এটিও সংযুক্ত, তাই না। , এই অংশটি আমার এনকোডার, এই অংশটি আমার এনকোডার, শেষ ফাংশনটি কী? অন্য কোথাও? MSC, MSC এর মধ্যে। , x বিয়োগ ড্যাশের MSC, এটি শেষ ফাংশন, তাই না। , এখন আমরা এটি করতে যাচ্ছি, যদি এটি আমার h হয়, আপনি বলবেন h, h হল জল, এবং এটি এমবেডিং, কিন্তু আপনি যেমনটি করবেন, এখানে যদি এটি হয় তবে গেটগুলি w এবং এখানে গেটগুলি w dash, h সমান w dash, x, ডান। এবং তারপর x dash সমান, w dash into h। এখন, এখানে আমাদের h মান আছে, আমরা এখানে h মান জানি, তারপর x dash সমান w dash w, x, ডান। যদি আপনি এই মানটি এখানে রাখেন, তাহলে কি আমরা a, w dash w, x, ডান পেতে পারি? , x দেওয়া, আমাদের w ড্যাশিং w পেতে হবে। সরলতার জন্য, আমরা যা করব তা হল, আমরা বলব যে এখানে w যাই হোক না কেন, এখানে এটি একটি টাস্ক ফোর্স। ভিন্ন ওজন নয়, এটি একই ওজন, সাধারণত এখানে ভৌত বল, তাই না। অভিধান এক, অভিধানের নামে, আমরা কেবল একটি অভিধান তৈরি করব, তাই না। , এখানেও, এটি একটি অভিধান, বর্তমান ম্যাপিং, এটি একটি অভিধান। , দুটি অভিধান তৈরির জায়গায়, দুটি ওজন সেট রয়েছে। যদি আপনি, যদি আপনি এটি জোর করেন, যদি এই w এবং w, এবং এখানে এটি w ট্রান্সপোজ হয়, তাহলে কথা বলবে, তাই না। শুধু ওজনের মিল, আপনি এখানে উদ্ধৃত করতে পারেন, h, এটি হল চূড়ান্ত ওজন সেট সমান garnaji, h যেখানে আপনি, যদি আপনি বলতে পারেন যে, একটি ট্রান্সপোজ ওজন, ট্রান্সপোজ ওজন সেট সমান garnaji, তারপর আমরা জোর করতে পারি w ট্রান্সপোজ, ডুপকোনকে x এ রূপান্তর করতে পারি। , তাহলে আমার দুটি ওজন সেট শেখার দরকার নেই। আমি কেবল একটি ওজন সেট শিখতে পারি এবং আপনার ডুপকোট্রান্সপোজিং ব্যবহার করতে পারি can, can, can, then with it, okay. , এটি অটোএনকোডার, এনকোডার ডিকোডার, মূলত, ঠিক আছে। , MSc লস, কিন্তু তারপর এই ফর্মুলেশনটি এইভাবে কাজ করে। , আমি এটা কেন নিলাম? আমি শুধু বলছি, এটা নয়েজ ডেটা, আমাদের অডিও সিগন্যালে নয়েজ এবং ইনপুট সিগন্যাল দুটোই আছে, তাই না। আমাকে নয়েজটি সরিয়ে ফেলতে হবে। ওঠো, বোঝার চেষ্টা করি, এলিজিয়ন, এটা ঠিক আছে। ঠিক আছে। , আমরা NFCC বা যাই হোক না কেন, একটি মেলাস্টিক্ট্রাম বা যাই হোক না কেন, অথবা কাঁচা অডিও নিজেই ভাবতে পারি, কিন্তু আমাদের এনকোডারে ইনপুট হিসেবে একটি এক মাত্রিক এম্বেডিং, এক মাত্রিক সিগন্যাল দিতে হবে, আমাকে এটিকে কিছু এম্বেডিং স্পেসে রাখতে হবে এবং তারপর এটিকে আবার রূপান্তর করতে হবে, যাতে আমি পরিষ্কার সিগন্যাল তৈরি করি, ঠিক আছে। , অটোএনকোডার এবং ডিপ লার্নিং ক্লাস থেকে, যদি আপনার অটোএনকোডার মনে থাকে, তাহলে ক্ষমা করবেন, অটোএনকোডারের ফর্মুলেশনের একটি, অটোএনকোডারের ফর্মুলেশনের মতোই একটি হল ডি-নয়েজিং অটোএনকোডার। এটি ডেটা ডি-নয়েজ করা যেতে পারে, যদি ডেটা নয়েজ হয়, আমি কি নন-নয়েজড ডেটা পুনর্গঠন করতে পারি, ঠিক আছে। , আমার সমস্যা ঠিক একই, আমাদের ইনপুট নয়েজড। , আমি কি ডি-নয়েজড আউটপুট তৈরি করতে পারি, আমি কি নয়েজ আউটপুট সিন্থেসাইজ করে রিমুভ করতে পারি, নয়েজ আউট, আউট এবং ক্লিন সিগন্যাল রাখতে পারি। , যদি আমাদের ইনপুট থাকে, ধরুন, একটি বায়াস V, যা পরিষ্কার, তাই না, V ক্লিন প্লাস একটি নয়েজ, ব্যাকগ্রাউন্ড নয়েজ, ঠিক আছে। , আমরা যা পাই তা হল V ডেটা, ঠিক আছে। আমাদের নিশ্চিত করতে হবে যে আমরা এই ডেটাটি সরিয়ে দিচ্ছি, নয়েজ ঘটছে এবং আমরা V c পিছনে তৈরি করছি, ঠিক আছে। , আমার সাথে অনেক দূরে, আমার মনে রাখা উচিত আমি এখানে xn ব্যবহার করেছি, আমরা আমার সাথে আছি, আমাকে এটিকে x এ রূপান্তর করতে দিন, ঠিক আছে। এখন, আমার এনকোডারের কাজ, এটা কী, যদি আমার ইনপুট xc হিসাবে থাকে, আমার, দুঃখিত, এটি আমার ইনপুট, তাই না। , আমি কীভাবে একটি সূত্র বা ইনপুটের সমীকরণ লিখব, ইনপুট, আমি কী করব। এই xc, ঠিক আছে, দুঃখিত, এখন আমার কাছে xn আছে, আমার xc পিছনে পুনর্গঠন করতে হবে, কিন্তু আমার কাছে যা আছে তা হল xn, আপনি যখন কথা বলছেন তখন নয়েজের সম্পর্ক, ঠিক আছে। , xn, আমি h পাই, মানে h সমান W গুণ xn, ঠিক আছে। এখন, h থেকে, আমি জেনারেট করি, কী ফিরে? xn জেনারেট, ঠিক আছে। xn জেনারেট, ঠিক আছে। ডি-নয়েজ x, ঠিক আছে। , আমাকে xc ড্যাশ জেনারেট করতে হবে, ঠিক আছে। এখন, আমাকে h জেনারেট করতে হবে, এটি আমার এনকোডার, ঠিক আছে। ঠিক আছে। , এখানে কি হওয়া উচিত? আমার কাছে xc ড্যাশ আছে, ঠিক আছে, কিন্তু শুধু ধরে রাখো, আমি চাই। , আমার x, xc, kendo, W ট্রান্সপোজের আউটপুট কী হওয়া উচিত, মনে রাখবেন, আমি এটি W ড্যাশের জায়গায় রাখব, এখন আমরা W ট্রান্সপোজের সাথে থাকতে পারি, ঠিক আছে। , W ট্রান্সপোজ, কী? ঠিক আছে, ঠিক আছে। আমি hk মান k3, xn এর w, এখানে এটি, x dashc, এখন, এটি, কি, সেই k গ্লাস ফাংশন যা k হল phi 6? আমি k ম্যাট্রিক্সকে друзья এর সীমাতে পরিবর্তন করব। আমি চার্জ করব বা এক ভাষায় উত্তর দেওয়ার চেষ্টা করব। ঠিক আছে। তাহলে আপনাকে এগিয়ে যেতে হবে, আপনি পারবেন না, , clean signal, যদি rica clean signal তৈরি করা হয়, তাহলে আমাদের দুটি ভিন্ন মিনিমাইজ করতে হবে। এখন, আসুন, এই xc মাইনাস 1 রুট ট্রান্সপোজ w কে nix এ লিখি। , আমাদের noise signal এবং এর clean counterpart দেওয়া হলে, আমরা w এবং w এবং w ট্রান্সপোজের ওজন শেখার চেষ্টা করছি যাতে আমি clean signal ফিরে পেতে পারি। এটি প্রশিক্ষণের সময়, প্রশিক্ষণ শেষ হয়ে গেলে, আমাদের মূলত একটি xn প্রয়োজন, এটি এনকোড করবে এবং h থেকে আমার h দেবে, আমি xc dash এবং এই xc dash তৈরি করব কারণ প্রশিক্ষণ শেষ হয়ে গেছে, এই xc dash হল ad noise। , এই বিশেষ শেষ ফাংশন বা এই বিশেষ স্লাইডটি হাইলাইট করে। এখন, এই স্লাইডে, আরও একটি ছোট অ্যাড-অন আছে। সেই ছোট অ্যাড-অনটি কী? কেউ কি ছোট অ্যাড-অনটি দেখে আমাকে বলতে পারেন, আমাদের এখানে কী আছে? noise carrier এবং আপনি কী বলছেন? ঠিক আছে, আমরা সংযোগ স্কিপ করেছি, এর একটি নেট স্টাইল আছে, এর একটি স্কিপ সংযোগ আছে, ডিকোডারে এনকোড করা আছে। , এটা হলো ভালোভাবে শেখা, আমরা আরও গভীরে যেতে পারি। বাদ দেওয়ার কথা নয়, আমি একটি লস ফাংশনের কথা বলছি, লস ফাংশনের উপর মনোযোগ দিন, লস ফাংশনে একটি অ্যাড-অন জিনিস আছে। সেটা কী? এখন, আমাদের আরও একটি টোন যোগ করতে হবে এবং সেই টোনটি কী? এবং কেন আমাদের এটি যোগ করতে হবে? , আমরা কীভাবে জানব যে আমরা সঠিক জায়গায় নয়েজ সেট করেছি? , এখানে অর্জন করার জন্য আমাদের মূলত যা খুঁজে বের করতে হবে তা হল, এই প্রথম টোন, এটি নয়, আমি বলব, এটি হাই-এন্ড নয়, এটি হাই-এন্ড নয়, অন্যথায় এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। অন্যথায়, এটি ভেঙে যাবে। আপনি কোন প্রশ্নের উত্তর দিতে যাচ্ছেন? , আপনি কী করতে যাচ্ছেন? আপনি ওভারলেটিকে লেখার মতো বেশ সঠিক করে তুলবেন। , আপনি কীভাবে এটি লিখবেন না এবং একটি পয়েন্টার লিখবেন? এবং আপনি কীভাবে এটি পাবেন? দয়া করে সময়ের মতো সহজ হয়ে উঠুন। আর, অবশেষে, ঠিক আছে, ঠিক আছে, আমি বুঝতে পেরেছি যে তার কাছে কোনও বিবরণ বাকি আছে, আমরা ইতিমধ্যেই এটি সেট করেছি। কেউ কি আমাকে বলতে পারেন? অনলাইন বন্ধুরা। এখানে দ্বিতীয় পদটির ব্যাখ্যা করতে পারেন? না। কোন প্রশ্ন আছে? কোন প্রশ্ন আছে? না। এর নাম কী? এর নাম কী? এর নাম ফার্নে। এটি এখানে একটি শরণার্থী। কোন প্রশ্ন আছে? হ্যাঁ। এটি একটি প্রশ্ন। না। না। কী হয়েছে? সবকিছু হয়ে গেছে। ঠিক আছে। এটি অনলাইন বন্ধুদের জন্য ভালো? হ্যাঁ। ঠিক আছে। যদি কিছু ঘটে, আমাকে বলো। ঠিক আছে। , আমরা কোথায় যাব? অনলাইন বন্ধুরা। তাৎপর্যের অর্থ কী এবং শেষ ফাংশনের দ্বিতীয় পদটি কী করছে? এটি একটি নিয়মিতকরণ শব্দ। কিন্তু এটি কী করছে? তুমি কি আমাকে বলবে? না। তুমি কি আমাকে একটি ভাল আঁটসাঁট শব্দ দেবে? ঠিক আছে। ঠিক আছে। ঠিক আছে। , তাহলে আমি তোমাকে বলি সে কেমন আছে? এখন, তার চিত্রটি আবার দেখুন। এবং এক মুহূর্তের জন্য কল্পনা করুন, আমাদের কাছে কোনও ভয়েস সিগন্যাল নেই। আমাদের কোনও ভয়েস সিগন্যাল নেই। আর আমরা কী করব? যদি আমাদের কোনও শব্দ হয় তাহলে আমরা কী করব? ঠিক আছে। আপাতত, সেখান থেকে বুঝতে পারি। এর নাম কী ছিল? XM। ঠিক আছে। আমাদের একটি ধ্রুবক আছে। আমাকে এটি পরিবর্তন করতে হবে। আমাদের যা করা উচিত তা হল আমাদের চিন্তা করা উচিত, আমাদের কি সেখানে একটি শব্দ সংকেত থাকতে পারে? এবং শব্দ পুনর্গঠিত। মানে, আমরা আসল শব্দ দমন করছি এবং শব্দ পুনর্গঠন করছি। পিচায়ভ এবং মানুর এক্সপ্রেস কোন ট্রেনে আছে। আমরা সেই ঘোষণাটি অনুরোধ করতে চাই। , আমার শেষ কাজটি কী হবে? আমাদের আসল শব্দ আছে। তাই না? আমার আসল শব্দ যাই হোক না কেন, পুনর্গঠিত শব্দ বাদ দিয়ে। তাই না? নাকি, NR, NR, NR, পুনর্গঠন নয়। ঠিক আছে। , এটি আমাকে কী বলে? এটি কী? কিন্তু একই অটোএনকোডার। যদি এটি শব্দ পুনর্গঠন শুরু করে। মানে, এই ক্ষেত্রে আমাদের প্রথম শব্দ...\n",
            "Generating basic Bengali TTS...\n",
            "Basic Bengali TTS saved to temp_bengali_tts.mp3\n",
            "Extracting voice features from your recordings...\n",
            "Processing file: newer_voice_samples/b4.mp3\n",
            "Extracted pitch for b4.mp3: 259.37 Hz\n",
            "Extracted speech rate for b4.mp3: 8.55 syllables/sec\n",
            "Processing file: newer_voice_samples/WhatsApp Audio 2025-04-18 at 15.33.26.mp3\n",
            "Extracted pitch for WhatsApp Audio 2025-04-18 at 15.33.26.mp3: 204.75 Hz\n",
            "Extracted speech rate for WhatsApp Audio 2025-04-18 at 15.33.26.mp3: 0.00 syllables/sec\n",
            "Processing file: newer_voice_samples/b2.mp3\n",
            "Extracted pitch for b2.mp3: 220.90 Hz\n",
            "Extracted speech rate for b2.mp3: 29.36 syllables/sec\n",
            "Processing file: newer_voice_samples/b7.mp3\n",
            "Extracted pitch for b7.mp3: 245.79 Hz\n",
            "Extracted speech rate for b7.mp3: 46.18 syllables/sec\n",
            "Processing file: newer_voice_samples/b6.mp3\n",
            "Extracted pitch for b6.mp3: 257.54 Hz\n",
            "Extracted speech rate for b6.mp3: 9.34 syllables/sec\n",
            "Processing file: newer_voice_samples/b1.mp3\n",
            "Extracted pitch for b1.mp3: 264.93 Hz\n",
            "Extracted speech rate for b1.mp3: 1.25 syllables/sec\n",
            "Processing file: newer_voice_samples/b9.mp3\n",
            "Extracted pitch for b9.mp3: 248.24 Hz\n",
            "Extracted speech rate for b9.mp3: 1.37 syllables/sec\n",
            "Processing file: newer_voice_samples/WhatsApp Audio 2025-04-18 at 15.33.25.mp3\n",
            "Extracted pitch for WhatsApp Audio 2025-04-18 at 15.33.25.mp3: 345.70 Hz\n",
            "Extracted speech rate for WhatsApp Audio 2025-04-18 at 15.33.25.mp3: 0.00 syllables/sec\n",
            "Processing file: newer_voice_samples/b3.mp3\n",
            "Extracted pitch for b3.mp3: 276.74 Hz\n",
            "Extracted speech rate for b3.mp3: 1.10 syllables/sec\n",
            "Processing file: newer_voice_samples/b10.mp3\n",
            "Extracted pitch for b10.mp3: 248.58 Hz\n",
            "Extracted speech rate for b10.mp3: 71.18 syllables/sec\n",
            "Processing file: newer_voice_samples/b5.mp3\n",
            "Extracted pitch for b5.mp3: 269.71 Hz\n",
            "Extracted speech rate for b5.mp3: 30.34 syllables/sec\n",
            "Processing file: newer_voice_samples/b8.mp3\n",
            "Extracted pitch for b8.mp3: 229.77 Hz\n",
            "Extracted speech rate for b8.mp3: 0.91 syllables/sec\n",
            "\n",
            "Pitch statistics across 12 files:\n",
            "Average: 256.00 Hz\n",
            "Range: 204.75 - 345.70 Hz\n",
            "Variation: 140.95 Hz\n",
            "\n",
            "Speech rate statistics across 12 files:\n",
            "Average: 16.63 syllables/sec\n",
            "Range: 0.00 - 71.18 syllables/sec\n",
            "Variation: 71.18 syllables/sec\n",
            "\n",
            "Final factors:\n",
            "Pitch factor: 1.600\n",
            "Speed factor: 4.158\n",
            "Extracted voice features: {'pitch_factor': np.float64(1.6000061269452797), 'speed_factor': np.float64(4.1582922440622845), 'raw_pitch': np.float64(256.00098031124475), 'raw_speech_rate': np.float64(16.633168976249138), 'pitch_variation': np.float64(140.94959476173085), 'rate_variation': np.float64(71.18414256198348)}\n",
            "Applying voice adaptation...\n",
            "Adapting voice with features: {'pitch_factor': np.float64(1.6000061269452797), 'speed_factor': np.float64(4.1582922440622845), 'raw_pitch': np.float64(256.00098031124475), 'raw_speech_rate': np.float64(16.633168976249138), 'pitch_variation': np.float64(140.94959476173085), 'rate_variation': np.float64(71.18414256198348)}\n",
            "Shifting pitch by 8.14 semitones\n",
            "Adjusting speed with factor: 0.70\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-1eb4db5dfbf0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mzip_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/bengali_samples.zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_bengali_tts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_transcript\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-4edc5a80e113>\u001b[0m in \u001b[0;36mprocess_bengali_tts\u001b[0;34m(text_to_translate, zip_file_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Adapt the voice and return final output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapt_voice_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_tts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoice_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Clean up temp file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-2fc3b5ad8ab5>\u001b[0m in \u001b[0;36madapt_voice_simple\u001b[0;34m(tts_audio_path, voice_features, output_file)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspeed_factor\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Slow down\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0madjusted_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjusted_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeedup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayback_speed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspeed_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0;31m# Speed up (using soundstretch-like algorithm for better quality)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/effects.py\u001b[0m in \u001b[0;36mspeedup\u001b[0;34m(seg, playback_speed, chunk_size, crossfade)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrossfade\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcrossfade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlast_chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, seg, crossfade)\u001b[0m\n\u001b[1;32m   1262\u001b[0m             ))\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m         \u001b[0mxf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcrossfade\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_gain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1265\u001b[0m         \u001b[0mxf\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mseg2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcrossfade\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_gain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfade\u001b[0;34m(self, to_gain, from_gain, start, end, duration)\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m                 \u001b[0mvolume_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_power\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale_step\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m                 \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m                 chunk = audioop.mul(chunk._data,\n\u001b[1;32m   1349\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_width\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, millisecond)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msilence\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmissing_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_sample_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, data, overrides)\u001b[0m\n\u001b[1;32m    430\u001b[0m         }\n\u001b[1;32m    431\u001b[0m         \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just doing it again here"
      ],
      "metadata": {
        "id": "rY3WO_3Btjcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Extract voice features from samples\n",
        "print(\"Extracting voice features from your recordings...\")\n",
        "voice_features = extract_voice_features('newer_voice_samples/')\n",
        "print(f\"Extracted voice features: {voice_features}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYrQ1v2xsOh1",
        "outputId": "722de5e7-63ac-4a1a-d105-a1ada07bd5ec"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting voice features from your recordings...\n",
            "Processing file: newer_voice_samples/b4.mp3\n",
            "Extracted pitch for b4.mp3: 259.37 Hz\n",
            "Extracted speech rate for b4.mp3: 8.55 syllables/sec\n",
            "Processing file: newer_voice_samples/b2.mp3\n",
            "Extracted pitch for b2.mp3: 220.90 Hz\n",
            "Extracted speech rate for b2.mp3: 29.36 syllables/sec\n",
            "Processing file: newer_voice_samples/b7.mp3\n",
            "Extracted pitch for b7.mp3: 245.79 Hz\n",
            "Extracted speech rate for b7.mp3: 46.18 syllables/sec\n",
            "Processing file: newer_voice_samples/b6.mp3\n",
            "Extracted pitch for b6.mp3: 257.54 Hz\n",
            "Extracted speech rate for b6.mp3: 9.34 syllables/sec\n",
            "Processing file: newer_voice_samples/b1.mp3\n",
            "Extracted pitch for b1.mp3: 264.93 Hz\n",
            "Extracted speech rate for b1.mp3: 1.25 syllables/sec\n",
            "Processing file: newer_voice_samples/b9.mp3\n",
            "Extracted pitch for b9.mp3: 248.24 Hz\n",
            "Extracted speech rate for b9.mp3: 1.37 syllables/sec\n",
            "Processing file: newer_voice_samples/b3.mp3\n",
            "Extracted pitch for b3.mp3: 276.74 Hz\n",
            "Extracted speech rate for b3.mp3: 1.10 syllables/sec\n",
            "Processing file: newer_voice_samples/b10.mp3\n",
            "Extracted pitch for b10.mp3: 248.58 Hz\n",
            "Extracted speech rate for b10.mp3: 71.18 syllables/sec\n",
            "Processing file: newer_voice_samples/b5.mp3\n",
            "Extracted pitch for b5.mp3: 269.71 Hz\n",
            "Extracted speech rate for b5.mp3: 30.34 syllables/sec\n",
            "Processing file: newer_voice_samples/b8.mp3\n",
            "Extracted pitch for b8.mp3: 229.77 Hz\n",
            "Extracted speech rate for b8.mp3: 0.91 syllables/sec\n",
            "\n",
            "Pitch statistics across 10 files:\n",
            "Average: 252.16 Hz\n",
            "Range: 220.90 - 276.74 Hz\n",
            "Variation: 55.85 Hz\n",
            "\n",
            "Speech rate statistics across 10 files:\n",
            "Average: 19.96 syllables/sec\n",
            "Range: 0.91 - 71.18 syllables/sec\n",
            "Variation: 70.27 syllables/sec\n",
            "\n",
            "Final factors:\n",
            "Pitch factor: 1.576\n",
            "Speed factor: 4.990\n",
            "Extracted voice features: {'pitch_factor': np.float64(1.5759762598470133), 'speed_factor': np.float64(4.989950692874741), 'raw_pitch': np.float64(252.15620157552212), 'raw_speech_rate': np.float64(19.959802771498964), 'pitch_variation': np.float64(55.84692576956067), 'rate_variation': np.float64(70.27428890881447)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Applying voice adaptation...\")\n",
        "output_file = \"/content/bengali_with_my_voice.mp3\"\n",
        "final_output = adapt_voice_simple('/content/temp_bengali_tts.mp3', voice_features, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjHK_jrPLPLm",
        "outputId": "7618213b-03be-4db8-9343-f5565c354d2c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying voice adaptation...\n",
            "Adapting voice with features: {'pitch_factor': np.float64(1.5759762598470133), 'speed_factor': np.float64(4.989950692874741), 'raw_pitch': np.float64(252.15620157552212), 'raw_speech_rate': np.float64(19.959802771498964), 'pitch_variation': np.float64(55.84692576956067), 'rate_variation': np.float64(70.27428890881447)}\n",
            "Shifting pitch by 3.00 semitones\n",
            "Applying speed factor: 1.20×\n",
            "Voice-adapted audio saved to /content/bengali_with_my_voice.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pitch_stats = {\n",
        "    \"Average\": 252.16,\n",
        "    \"Min\": 220.90,\n",
        "    \"Max\": 276.74,\n",
        "    \"Variation\": 55.85\n",
        "}\n",
        "\n",
        "speech_rate_stats = {\n",
        "    \"Average\": 19.96,\n",
        "    \"Min\": 0.91,\n",
        "    \"Max\": 71.18,\n",
        "    \"Variation\": 70.27\n",
        "}\n",
        "\n",
        "final_factors = {\n",
        "    \"Pitch factor\": 1.576,\n",
        "    \"Speed factor\": 4.990\n",
        "}\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
        "axs[0].bar([\"Min\", \"Average\", \"Max\"], [pitch_stats[\"Min\"], pitch_stats[\"Average\"], pitch_stats[\"Max\"]], color=[\"#77b5fe\", \"#4f94cd\", \"#1c3f95\"])\n",
        "axs[0].set_title(\"Pitch Statistics (Hz)\")\n",
        "axs[0].set_ylabel(\"Frequency (Hz)\")\n",
        "axs[0].annotate(f'Variation:\\n{pitch_stats[\"Variation\"]:.2f} Hz', xy=(1, pitch_stats[\"Average\"] + 5), ha='center', fontsize=10)\n",
        "\n",
        "axs[1].bar([\"Min\", \"Average\", \"Max\"], [speech_rate_stats[\"Min\"], speech_rate_stats[\"Average\"], speech_rate_stats[\"Max\"]], color=[\"#ff9999\", \"#ff4d4d\", \"#cc0000\"])\n",
        "axs[1].set_title(\"Speech Rate Statistics (syllables/sec)\")\n",
        "axs[1].set_ylabel(\"Rate\")\n",
        "axs[1].annotate(f'Variation:\\n{speech_rate_stats[\"Variation\"]:.2f}', xy=(1, speech_rate_stats[\"Average\"] + 5), ha='center', fontsize=10)\n",
        "\n",
        "plt.suptitle(\"Pitch & Speech Rate Statistics Across 10 Files\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "KStBMwoJuUAP",
        "outputId": "2c896d52-1af5-4a18-e47c-fc661c54e1ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAHvCAYAAACFRmzmAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAjbBJREFUeJzs3XdcVvX///HnhUyZggLiwL3RFE1x4kg0t6Q5cqcNNEc2LCtHZlpqlqv6GGiJluXMcuAq98itmZKKCzfgSEA5vz/8cX29AhQMLhyP++12bjfO+/0+7/M6h4vrOteL93kfk2EYhgAAAAAAAAArssntAAAAAAAAAPDkISkFAAAAAAAAqyMpBQAAAAAAAKsjKQUAAAAAAACrIykFAAAAAAAAqyMpBQAAAAAAAKsjKQUAAAAAAACrIykFAAAAAAAAqyMpBQAAAAAAAKsjKQUAT7hixYqpWLFiOdL38ePHZTKZ1LNnzxzpH1nTs2dPmUwmHT9+PLdDQSbl5O8sIiJCJpNJERER2d43Hn8mk0nBwcEWZSNGjJDJZNK6detyJSYAwKOHpBQAPGZSE0F3L/b29ipSpIi6dOmivXv33rePh/mLxZEjR/TCCy+oSJEicnJyUvHixdWtWzf98ccfD9TfrVu3NGXKFAUFBcnd3V329vYqWLCgatasqcGDB2vXrl3ZfASPh3+/xmxtbeXj46OWLVsqKirqP/dvzYTm9evX9dFHH6latWpycXGRg4ODChcurHr16mnYsGGKjo62aJ+didycTAw9aknh3377zfx6mj9/fm6Hk6t2796td955RyEhISpQoEC6CaD0zJkzR08//bScnZ2VL18+tWzZMsvvjcWKFUvz9333smjRogc7KAAA0mGb2wEAAHJGyZIl9cILL0iSrl27pi1btmju3LlasGCBVq9erTp16kiSVq9enZthZsmZM2dUs2ZNXblyRc8884yqVq2qM2fOaM2aNSpZsqSqVauWpf5u376t5s2bKyoqSn5+furQoYN8fHwUFxenP/74Q59//rmcnZ1VtWrVHDqiR5uXl5f69+8vSbp586YOHDigZcuWadmyZYqMjFTnzp1zOcL7u3r1qurWrau9e/eqVKlSeuGFF+Tl5aWLFy9q27Zt+vjjj1WyZEmVLFkyV+IbO3as3n77bRUqVCjb+27Xrp1q1aqlggULZnvfD2LmzJmS7iQ8v/nmG3Xo0CGXI8o9ixYt0tixY2Vvb68yZcro4sWL991mzJgxGj58uPz9/fXyyy/r6tWrmjdvnmrXrm3xnp8ZefLk0fDhw9OtK1eunCTp0KFDyps3b6b7BAAgPSSlAOAxVapUKY0YMcKibPjw4RozZozeffdd8yio3Pqy/SDmz5+vK1eu6OWXX9b06dPN5SkpKZn60vZvkZGRioqKUrNmzbRkyRLZ2dlZ1MfGxurMmTP/Oe7HVf78+dO8xubNm6fOnTtr2LBhj0RS6rPPPtPevXv14osv6quvvpLJZLKoP3bsmBITE3MpOqlgwYI5ljRyd3eXu7t7jvSdVQkJCfrxxx9VuXJl+fj4aOXKlTp58qSKFCmS26Hlig4dOqh169YKCAjQpUuX7vsaOHLkiEaMGKEyZcpo27Zt5t/rq6++qlq1aqlv377av3+/bGwyd5OEra1tmr/tf0tNTgEA8F9w+x4APEEGDBggSdq+fbu57N+3IgUHB2vkyJGSpIYNG5pv2fj37Urnz5/X66+/rrJly8rJyUmenp6qWbOmPv3003T3ffToUbVr10758uWTs7OzmjRpoj179mQpfhcXF0lS5cqVLcptbGzk7e2dpb4kafPmzZKkl156KU1CSpJ8fX3TjL4KDg6WyWTSzZs39fbbb6to0aJydHRU+fLl9cUXX8gwjHT3tXjxYjVu3Fj58uWTo6OjKlWqpE8//VS3b9/OtvZNmzaVl5eXHB0dVaxYMXXr1k379+9P09YwDH3++ecqV66cHBwc5O/vr5EjRyolJeWe5ysznn/+eTk7O+vEiRNpEoULFy5U586dVapUKeXNm1fu7u6qV6+efvrpJ4t2ERERKl68uCRp1qxZFrcO3X1LqWEY+uabb1SnTh25ubkpb968ql69ur755ptMx5v6GggLC0uTkJKk4sWLm798p94Od+LECZ04ccIirtQv8ElJSfriiy8UEhKiIkWKyMHBQd7e3mrfvn2aW0F79uypXr16SZJ69epl0d/dbdKbU+qnn35SgwYN5O3tLUdHR/n5+alJkybmc5mZc3ivWwf//vtv9evXT8WLFzcfQ3BwcJq294sjs+bOnasbN26oe/fu6t69u1JSUu55S2Nm3n/uvn3x0KFDateunby8vCzO561btzRx4kRVqVJFTk5Ocnd3V8OGDbV06dI0+0xJSdH//vc/Pf300/L09JSTk5MKFy6sVq1apbnV+b+el4oVK6patWrpvi+lJzw8XLdu3dK7775rkWh86qmn1LlzZx06dEgbNmzIVF+ZldlbClPt3btXnTp1UsGCBWVvby9/f38NGDBAly5dStN27dq1at68ufz8/OTg4CAfHx/Vq1dPX331VTYeAQDgYcBIKQB4AqX35TtV6vwz69evV48ePczJKA8PD3Obw4cPq2HDhjp79qzq1q2rtm3b6vr16zpw4IA++ugjDR061KLP48ePq1atWqpYsaJ69+6t6OhoLV68WA0bNtShQ4fk4+OTqbhDQ0P1wQcfaNKkSerUqZPy5cuXpeP+Ny8vL0nSX3/9leVtO3bsqF27dik0NFTSnS+hr732mo4fP64JEyZYtB02bJg+/vhjFSpUSO3bt5e7u7t+//13vfHGG9q6dWua+XOy2v7111/XxIkT5enpqbZt28rb21snT55UVFSUAgMDValSJYv2b7zxhtavX6+WLVsqJCREixYt0ogRI5SUlKQxY8Zk+VxkxNbW8jJj2LBhsre3V926dVWwYEFduHBBS5Ys0XPPPafPP//cnDR96qmnNHDgQE2ePFlVqlRR27ZtzX2kvh4Nw1DXrl01d+5clS5dWl26dJG9vb1WrVqlPn366ODBgxkmSO9292vgqaeeumdbDw8PffDBB/rss88kSYMGDTLXpX45v3z5sgYNGqR69erp2WefVb58+fT3339ryZIl+vXXX/Xbb7+pRo0akqS2bdsqLi5OixcvVps2be67/1TTp0/Xq6++qoIFC5oTLbGxsdq2bZsWLlyo0NDQTJ3DjGzYsEEtWrTQ1atXFRISok6dOunKlSvatWuXJk+ebH6PyEwcmTVz5kzlyZNHXbt2lZubm1555RWFh4dr+PDhad6vsvr+c/ToUdWqVUsBAQHq2bOnLl26JHt7exmGoeeee06LFy9WmTJlFBYWpuvXr+v7779X69atNXHiRA0ePNjcz7BhwzR+/HiVLFlSXbp0kaurq06fPq0NGzYoKirK/BrIzvOSWalJsaZNm6apCwkJUUREhNavX6/69etn+74zY8mSJerYsaNsbGzUpk0bFSlSRAcPHtSUKVO0YsUKbd261fx+vmzZMrVq1UoeHh5q06aN+b1iz549+vbbb9WvX79cOQYAQA4xAACPlWPHjhmSjJCQkDR177//viHJaNiwobnM39/f8Pf3t2j3wQcfGJKMtWvXpruP6tWrG5KMr776Kk3dyZMn08Qiyfj4448t2g0fPtyQZIwdOzbTx3b48GGjRIkShiQjMDDQuHTpUqa3Tc/OnTsNW1tbw97e3njppZeMJUuWGGfOnLnnNg0aNDAkGWXLljXi4uLM5XFxcUbZsmUNk8lkbN++3Vy+cuVK8+/j2rVr5vKUlBTj5ZdfNiQZP/744wO3X7p0qSHJCAgIMC5evGgRa3JyshEbG2te79GjhyHJKF68uMVxXrhwwfDw8DBcXV2NxMTEzJw68zn4t8jISEOSUbFixTR10dHRacquXr1qBAQEGO7u7sb169fN5amvnR49eqS7/6+++sqQZPTq1ctISkoylycmJhqtWrUyJBk7duy473EsXrzYkGS4uroar7/+urFixYo05/Hf0vubSXXz5k3j1KlTacr3799vuLi4GE2aNLEoDw8PNyQZ4eHh6faX+js7duyYuaxatWqGvb29ce7cuTTt7479fucwvX3fvHnTKFSokGFjY2P8+uuvaba5++87s3Hcz969e9O8Z3Xv3t2QZERFRaVp/yDvP++//36atrNmzTIkGQ0aNLB43Z84ccLInz+/YWtra/Ga9fT0NPz8/Cxep6nufi/KrvOS6uzZs+Y4M5I/f37DxcUl3bodO3YYkoxu3bplan/+/v5Gnjx5jA8++CDNMnfuXHO79GJK77Pj4sWLhpubm1GoUCHj+PHjFu3nzp1rSDL69+9vLmvfvr0hydi9e3ea2B7k/AEAHm4kpQDgMZP6RaxkyZLmLxJDhw416tWrZ0gyHB0djU2bNpnbZzUptXXrVkOSUb9+/UzHUrx4ceP27dvp1rVv3z5Tx3Xq1CnDx8fHKF68uPH9998bzs7ORkBAgHH27FmLdhs3bjQkGW+//Xam+p0zZ46RP39+85dXSUbhwoWNnj17ppvUSE1Kfffdd2nqvv322zRfsFq3bm1IMk6cOJGmfVxcnGEymYzQ0NAHbt+8eXNDkrFmzZr7HmtqguObb77JsG7v3r337ccw7nwh9fLyMr/G3nrrLaNly5aGyWQyXFxcjN9++y1T/RiGYUyYMMGQZKxbt85cdr+ESuXKlQ1nZ2fjxo0baepSkxyvv/56pvfv4uJi8RooWbKkERYWZvz1119p2t8rKXUvrVq1Muzt7S2SaA+alHJ2djYuX758z/09SFLq+++/NyQZ3bt3v+/xZDaO+xk4cKAhyZgzZ465LCoqypBkdO7c2aLtg7z/+Pr6pptsbdSokSHJ2Lp1a5q6MWPGGJKMUaNGmcs8PT2NYsWKGTdv3rznfrPrvKTKTFLKzs7OKFSoULp1f/31lyHJaN26dab25+/vb/G3cPfSpk0bc7vMJqUmTpxoSDJmz56d7v6qVatm5M+f37yempQ6fPhwpuIFADzauH0PAB5T0dHR5rmh7Ozs5OPjoy5duujtt99WQEDAA/e7bds2SenfJpKRp556Ks0Eu4ULF5YkxcXFZaqPIUOG6Ny5c9q4caNq166t/Pnzq0WLFqpfv76ioqJUtGhRSXcm/JWk6tWrZ6rfLl26qH379lq1apU2bNignTt3atOmTYqIiNDs2bM1depUvfzyy2m2q1evXoZld88dtGXLFjk7O2c4z5GTk5P+/PPPB26/bds2OTg4qEGDBpk6XkkKDAxMU5bV34ckXbp0yfwaS+Xi4qJVq1apVq1aadqfP39eH3/8sX799VedOHFC//zzj0V9ZieVv3Hjhvbt2yc/Pz+NGzcuTX1ycrIkWZynexkyZIj69u2r5cuXa9OmTdqxY4e2bt2qqVOnaubMmebbuTJr9+7dGj9+vDZs2KDY2FhzPKkuXrz4nyYv79Spk958801VqlRJXbp0UcOGDVW3bl25ubk9cJ+psvL3nR1xJCYm6rvvvpOrq6vatWtnLm/YsKGKFCmihQsX6sqVK+Zbux7k/adKlSqyt7dPU75r1y7lzZtXTz/9dJq6hg0bSrrzu0zVqVMnTZs2TZUqVVKnTp3UsGFDBQUFycnJyWLbnPz9WIuDg4Nu3ryZLX1t2bJFkrR161ZFR0enqb9586YuXryoixcvKn/+/OrUqZMWLFigWrVqqUuXLmrcuLHq1aun/PnzZ0s8AICHC0kpAHhMhYSEaPny5dneb3x8vCRl6RH16X0ZS51vKKOJu++WnJysBQsWqGLFiqpdu7YkqVGjRlqwYIHatm2revXqKSoqSqVLl9aiRYvk4uKikJCQTMfn6OioVq1aqVWrVpLufEn69NNP9d5772ngwIFq27atfH19LbZJbx6s1LLUcyTdmWPo1q1baZI3d7t+/foDt4+Pj1ehQoUy/VQt6b//PlKVLVvWnPiJi4vTokWL9Morr6hdu3basWOHxWvk8uXLqlGjhmJiYlSnTh01adJEHh4eypMnj3bv3q3Fixdn+il3V65ckWEYOn36dKbP0/24urqqQ4cO6tChg6Q75/Wdd97RtGnT1KdPH50+fTrdxMa/bdq0SY0aNZJ0J3FSunRpubi4yGQyadGiRdqzZ89/fprf0KFD5eXlpenTp2vChAn69NNPZWtrqxYtWmjSpEnmCc4fRFb+vrMjjkWLFunSpUvq1auXRXLHxsZGXbt21ccff6zIyEiFhYVlOb5UGc1Zl5CQkOHT/VKThgkJCeayyZMnq3jx4goPD9eHH36oDz/8UI6OjurYsaMmTJhgTprk5O8nI+7u7hbvO3dLPYbcetLi5cuXJUlTp069Z7vr168rf/786tChgxYtWqSJEydqxowZmjp1qkwmkxo2bKgJEyZkeu41AMCjgafvAQCyJHXC89OnT1ttnxcvXtStW7fk6upqUd68eXPNmzdPZ86cUf369fXdd99p6dKleuWVV8xP6nsQjo6OGj58uOrXr6+kpCRt3LgxTZtz585lWHb3lz83Nzd5eXnJuHPLfLrLsWPHHri9h4eHYmNjs+XJef+Fh4eHevbsqSlTpig2NtacREg1c+ZMxcTEaPTo0dqwYYO++OILjR49WiNGjEh3VNW9pCbVAgMD73me1q5d+8DH4+7urilTpsjf318XL17Uvn37MrXdmDFjlJiYqKioKC1ZskQTJkzQyJEjNWLEiDSJzQdlMpnUu3dvbd++XRcuXNDChQvVvn17LV68WC1btsxSYvHfsvL3nR1xzJw5U9Kdp8fd/YRAk8mkjz/+2KJNVuO7O870uLm56fz58+nWxcbGmtuksrW11dChQ3XgwAGdPn1akZGRqlevnmbPnq2uXbta7C+nfj8ZKV26tK5du2aO+26po0dLly6d7fvNjNRzuG/fvnv+vfr7+5u3adOmjdavX68rV67o119/1Ysvvqh169apWbNmWRrNCQB4+JGUAgCkkSdPHknpj5pJvdVl5cqVVosnf/78cnZ21t69e3XlyhWLunbt2unbb7/V+fPn1a1bN/n4+Gj48OHZst97JbZ+//33DMuqVq1qLqtZs6YuXbpk/mJ4P1lt//TTTysxMVHr16/PVPuc1rt3b1WrVk2LFy/Wpk2bzOWpt+20adMmzTbpnct7vQZdXV1Vvnx5HTp0KEe/oJpMJjk7O6cbW0aJhejoaHl6eqpu3boW5Tdu3NAff/yRbl9S1kao3c3Ly0tt27bV999/r0aNGungwYM6evToA/f9oH/f94ojIydOnNDq1avl4+OjPn36pLsUL15cu3btMt8Sm53vP1WrVtWNGzfMtwTeLfVpdhmNyvHz81Pnzp21fPlylSpVSlFRUWluR5Ue7Lw8iNTbd9M7LytWrLBoY201a9aUJG3evDnL27q6uqpZs2b66quv1LNnT507d05bt27N7hABALmIpBQAIA1PT09J0smTJ9PU1ahRQzVq1NBvv/2mr7/+Ok19ToygsrOz0wsvvKAbN26oW7duFrfUSFJQUJD5lpjk5OQMRz/827x587RmzRoZhpGmbsuWLVq7dq1sbW3THckzevRoi9tl4uPj9eGHH8pkMqlHjx7m8tdee03SnWTNpUuX0vQTGxurQ4cOPXD71BFJAwcONN8mk+rWrVvpjujKSSaTSR988IEk6b333jOXp46C2LBhg0X7yMhI/fLLL2n6yZcvn0wmU7qvQenOebpx44b69u2b7m16x44d0/Hjx+8b75dffqnt27enW7do0SIdOnRIHh4eqlSpkrnc09NTFy9eTHfOHX9/f125ckUHDhwwl92+fVtDhw7VhQsX0rS/199aRtatW5fmNZucnGz+/Ts6Okq6/zlMT+vWrVW4cGF999135mTG3e7++85sHBkJDw9XSkqKXnrpJf3vf/9Ld3n77bcl/d9oqex8/0n9Ox02bJjFvF8nT57UxIkTZWtrax4BlZiYaJFkTXX9+nVdu3ZNdnZ25lto/+t5eRC9evWSra2txowZY/G+tHv3bs2dO1fly5dPkyi1ll69esnV1VXvvvuuxd9Fqhs3bpjnnZKk3377Ld1Eaur7ek6cPwBA7mFOKQBAGg0bNpTJZNI777yjAwcOyN3dXR4eHurfv78kac6cOQoODla/fv307bffKigoSDdv3tSBAwe0a9eudJMp/9X48eO1e/duLVu2TGXLllWzZs3k6+urw4cPa9myZcqbN6/69eunr776Ss2aNdPmzZtVoECBe/a5ZcsWTZ48WYUKFVL9+vVVtGhRJSUl6dChQ1q5cqVSUlL08ccfpzt/TZkyZVSpUiWFhoZKkn766SedOnVKQ4YMsZhkvVmzZnrvvfc0evRolSpVSs2aNZO/v78uXbqko0eP6vfff9eHH36o8uXLP1D7Z599VkOHDtWnn36q0qVLq127dvL29tbp06e1evVqDR06VIMGDcqm30LmtG7dWoGBgVqzZo3Wr1+vBg0aqFu3bho3bpwGDBigtWvXyt/fX3v27NHq1avVvn17LViwwKIPFxcXc/KhW7duKl26tGxsbNStWzf5+/vrpZde0pYtWzRr1ixt3LhRTZo0kZ+fn86dO6c///xTW7duVWRkpIoVK3bPWH/99Ve9/PLLKlWqlOrUqSM/Pz9dv35du3bt0u+//y4bGxtNmzZNDg4O5m0aNWqkHTt2qHnz5qpXr57s7e1Vv3591a9fXwMGDNDKlStVt25ddezYUY6Ojlq3bp1Onz6t4OBg8wicVKkTZX/22We6cuWK+TV7r9F+bdu2lZubm2rVqiV/f38lJydr1apVOnjwoJ577jlzAvB+5zA9Dg4O+uGHH9SsWTM1b95czZo1U5UqVZSQkKDdu3frxo0b5lFLmY0jPSkpKeZb9nr27Jlhu+eff16DBg3SnDlz9Omnn8rR0THb3n+6deumBQsWaPHixapcubJatmyp69ev6/vvv9fly5c1YcIElShRQpL0zz//qE6dOipTpowCAwNVtGhRXbt2TT///LNiY2M1dOhQ82vkv5yXVH/++af59sXUEVh//vmnxbmKiIgw/1ymTBmNGDFCw4cPV5UqVRQaGqqrV69q3rx5kqSvv/46S/POZacCBQpo7ty56tChg6pUqaJmzZqpXLlySkxM1PHjx7V+/XrVrl3bPAfia6+9pjNnzqhu3boqVqyYTCaTNmzYoG3btqlWrVq5llwDAOSQHH++HwDAqlIfgx4SEpKp9hk93j4iIsIICAgwHBwcDElp2sTGxhoDBw40SpQoYdjb2xuenp5GzZo1jYkTJ6aJJaNH0us+jzn/t8TEROPTTz81qlWrZjg5ORmOjo5GhQoVjLffftuIjY01DMMwXn/9dUOSUaNGDePatWv37C8mJsb44osvjFatWhmlSpUynJ2dDXt7e6No0aJGhw4djNWrV6fZpkGDBoYk459//jHefPNNo0iRIoa9vb1RtmxZ4/PPPzdSUlLS3deqVauMVq1aGQUKFDDs7OwMX19fIygoyBg9erQRExPzn9v/9NNPRsOGDQ13d3fDwcHBKFasmNGtWzdj//795jY9evQwJBnHjh1Ls316j3K/F0lG2bJlM6xfunSpIcmoV6+euWz37t1G06ZNjXz58hmurq5GgwYNjKioKCM8PNyQZISHh1v0cfjwYePZZ581PDw8DJPJlG5833//vdGkSRMjX758hp2dnVGoUCEjODjYmDBhgnHhwoX7Hseff/5pjB8/3njmmWeM4sWLG46Ojoajo6NRsmRJo0ePHsaOHTvSbHP16lWjb9++RsGCBY08efIYkowPPvjAXP/jjz8a1apVM/LmzWvkz5/f6NixoxEdHZ3h+V+2bJlRo0YNw8nJyZBk3H15lt4206ZNM1q3bm34+/sbjo6OhpeXl/H0008b06dPN5KSkjJ9DjM674ZhGEePHjX69OljFC5c2LCzszO8vb2N4OBgY/bs2Q8Ux7+tWLEi03//Xbt2NSQZc+bMMZdlx/uPYRhGcnKy8emnn5rf61Jfl4sXL7Zol5SUZIwbN85o2rSpUbhwYcPe3t7w8fEx6tevb0RGRlr83f+X85Jq7dq15tdCRkt6vvvuO6N69eqGk5OT4e7ubjz77LPGzp07M7XPVP7+/oaDg8N926X3+7vX+8iff/5p9OnTx/D39zfs7e2NfPnyGQEBAcZrr71mbNu2zdxu3rx5RseOHY2SJUsaefPmNdzd3Y0qVaoY48aNM65evZqlYwEAPPxMhpHOPQsAACBdwcHBWr9+fbq3/AEAAADIPOaUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1TGnFAAAAAAAAKyOkVIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSACRJPXv2VLFixbKtvxEjRshkMunixYvZ1mduMJlMGjFiRLb0dfz4cZlMJkVERGRLf5k1fvx4lStXTikpKTm6n1q1aunNN9/M0X0AAJ5cxYoVU8uWLXM7jCcK10H3FhERIZPJpOPHj5vLgoODFRwcnOW+Us/Pp59+et+2qdfZj7ODBw/K1tZW+/fvz+1QkMNISgGPudQPy9TF0dFRZcqUUf/+/XXu3LkMt7tx44ZGjBihdevWWS/Yf1m6dKkaNGggb29v5c2bVyVKlFDHjh21fPlyc5szZ85oxIgR2r179wPv55dffsm2Cy5JioyM1GeffZZt/f0XCQkJGjdunN566y3Z2PzfW77JZFL//v3T3Sb1NbNjx44s7eutt97S1KlTFRsb+59iBgBkr3379um5556Tv7+/HB0dVahQIT3zzDP64osvcjs0q7r7eshkMsnNzU0NGjTQsmXLHrjPnPzM5zrov8voOgiZ88UXX8jd3V3JyclW33eFChXUokULvf/++1bfN6yLv0zgCTFq1Ch9++23mjJlimrXrq3p06crKChIN27ckCR9/fXXOnz4sLn9jRs3NHLkyFxLSn366adq3bq1TCaThg0bpkmTJik0NFRHjhzRvHnzzO3OnDmjkSNH/ueLsZEjR6Zb988//2j48OFZ6i+jizF/f3/9888/6tat24OE+UC++eYb3bp1S507d87xfbVp00Zubm6aNm1aju8LAJA5mzZtUvXq1bVnzx717dtXU6ZM0YsvvigbGxtNnjw5t8OzumeeeUbffvutZs+erTfffFNHjx5Vq1attGLFigfqL6cSMFwHZQ9rXgc9jpYtW6amTZvKzs4uV/b/8ssva+HChYqOjs6V/cM6bHM7AADW0bx5c1WvXl2S9OKLL8rLy0sTJ07U4sWL1blz51z7sEnPrVu3NHr0aD3zzDNauXJlmvrz589bLRZHR8ds6yt1pJo1hYeHq3Xr1lbZr42NjZ577jnNnj1bI0eOfOyHlQPAo2DMmDFyd3fX9u3b5eHhYVFnzc/Th0WZMmX0wgsvmNdDQ0NVoUIFTZ48WSEhIbkY2f/hOij7WPM66HFz48YNrV+/XtOnT8+1GJo0aaJ8+fJp1qxZGjVqVK7FgZzFSCngCdWoUSNJ0rFjxyRZzil1/PhxFShQQJLMyYV/zynw559/qmPHjipQoICcnJxUtmxZvfvuu2n2ExcXp549e8rDw0Pu7u7q1auXeXRWRi5evKiEhATVqVMn3Xpvb29J0rp161SjRg1JUq9evcxxps5V8Pvvv6tDhw4qWrSoHBwcVKRIEQ0ePFj//POPua+ePXtq6tSpkiyH9af693FfvXpVgwYNUrFixeTg4CBvb28988wz+uOPPyTdmUdg2bJlOnHihLmvu89renMp3O9c3m+fGTl27Jj27t2rJk2a3LPd/axbty7NLQ//PrZUzzzzjE6cOPGf/mMLAMg+0dHRqlixYpqElPR/n6epUm/tnjNnjsqWLStHR0cFBgbqt99+S7Pt6dOn1bt3b/n4+MjBwUEVK1bUN998k6ZdYmKiPvjgA5UqVcr8Wfzmm28qMTExTdvvvvtOTz/9tPLmzat8+fKpfv366SZlNmzYoKefflqOjo4qUaKEZs+enYUzYql8+fLKnz9/mpEYixcvVosWLeTn5ycHBweVLFlSo0eP1u3bt81t7vWZn9VjvxvXQTl/HTRv3jwFBgbK1dVVbm5uCggIMI8c/Pvvv2UymTRp0qQ0223atEkmk0lz5869577vlpSUpPfff1+BgYFyd3eXs7Oz6tWrp7Vr12a4zaRJk+Tv7y8nJyc1aNAg0/MqfffddwoMDJSTk5M8PT3VqVMnnTx50qLNkSNHFBoaKl9fXzk6Oqpw4cLq1KmT4uPjLdqtXr1aiYmJat68uSQpOTlZI0eOVOnSpeXo6CgvLy/VrVtXq1atstjuzz//1HPPPSdPT085OjqqevXqWrJkSZpY4+LiNHjwYPPvtXDhwurevbvFfLR2dnYKDg7W4sWLM3X8eDQxUgp4QqVefHl5eaWpK1CggKZPn65XXnlF7dq1U/v27SVJlStXliTt3btX9erVk52dnfr166dixYopOjpaS5cu1ZgxYyz66tixo4oXL66xY8fqjz/+0P/+9z95e3tr3LhxGcbm7e0tJycnLV26VAMGDJCnp2e67cqXL69Ro0bp/fffV79+/VSvXj1JUu3atSVJ8+fP140bN/TKK6/Iy8tL27Zt0xdffKFTp05p/vz5kqSXXnpJZ86c0apVq/Ttt9/e97y9/PLL+vHHH9W/f39VqFBBly5d0oYNG3To0CFVq1ZN7777ruLj43Xq1CnzxYyLi0uG/WXmXN5vnxnZtGmTJGXY5ubNm+lORH/t2jWL9fLly6c5N3FxcRoyZEiaLzSBgYGSpI0bN6pq1aoZxgYAsA5/f39t3rxZ+/fvV6VKle7bfv369fr+++/12muvycHBQdOmTVOzZs20bds28/bnzp1TrVq1zEmsAgUK6Ndff1WfPn2UkJCgQYMGSZJSUlLUunVrbdiwQf369VP58uW1b98+TZo0SX/99ZcWLVpk3u/IkSM1YsQI1a5dW6NGjZK9vb22bt2qNWvWqGnTpuZ2R48e1XPPPac+ffqoR48e+uabb9SzZ08FBgaqYsWKWT4/8fHxunLlikqWLGlRHhERIRcXFw0ZMkQuLi5as2aN3n//fSUkJOiTTz6RpHt+5mfl2P+N66CcvQ5atWqVOnfurMaNG5uvRw8dOqSNGzdq4MCBKlGihOrUqaM5c+Zo8ODBFtvOmTNHrq6uatOmzX3PVaqEhAT973//U+fOndW3b19dvXpVM2fOVEhIiLZt26annnrKov3s2bN19epVhYWF6ebNm5o8ebIaNWqkffv2ycfHJ8P9jBkzRu+99546duyoF198URcuXNAXX3yh+vXra9euXfLw8FBSUpJCQkKUmJioAQMGyNfXV6dPn9bPP/+suLg4ubu7m/v75ZdfFBgYaN7niBEjNHbsWL344ot6+umnlZCQoB07duiPP/7QM888I0k6cOCA6tSpo0KFCuntt9+Ws7OzfvjhB7Vt21Y//fST2rVrJ+nOtWa9evV06NAh9e7dW9WqVdPFixe1ZMkSnTp1Svnz5zfHERgYqMWLFyshIUFubm6ZPu94hBgAHmvh4eGGJCMqKsq4cOGCcfLkSWPevHmGl5eX4eTkZJw6dcowDMPo0aOH4e/vb97uwoULhiTjgw8+SNNn/fr1DVdXV+PEiRMW5SkpKeafP/jgA0OS0bt3b4s27dq1M7y8vO4b9/vvv29IMpydnY3mzZsbY8aMMXbu3Jmm3fbt2w1JRnh4eJq6GzdupCkbO3asYTKZLGIPCwszMno7/Pc5cHd3N8LCwu4Ze4sWLSzOZapjx46liTUz5zIz+0zP8OHDDUnG1atX09RJuu+yffv2dPtNSUkxWrZsabi4uBgHDhxIU29vb2+88sorWY4XAJD9Vq5caeTJk8fIkyePERQUZLz55pvGihUrjKSkpDRtU9//d+zYYS47ceKE4ejoaLRr185c1qdPH6NgwYLGxYsXLbbv1KmT4e7ubv78/fbbbw0bGxvj999/t2g3Y8YMQ5KxceNGwzAM48iRI4aNjY3Rrl074/bt2xZt7/489Pf3NyQZv/32m7ns/PnzhoODg/H666/f91xIMvr06WNcuHDBOH/+vLFjxw6jWbNmhiTjk08+sWib3jXESy+9ZOTNm9e4efOmuSyjz/zMHntGuA7KueuggQMHGm5ubsatW7cy3PbLL780JBmHDh0ylyUlJRn58+c3evToYS5Lvc4+duyYuaxBgwZGgwYNzOu3bt0yEhMTLfq/cuWK4ePjY3GdnHp+7r4+NwzD2Lp1qyHJGDx4sLks9To71fHjx408efIYY8aMsdjPvn37DFtbW3P5rl27DEnG/PnzMzz2VEWLFrX43VepUsVo0aLFPbdp3LixERAQYPE3kpKSYtSuXdsoXbq0uSz19b1gwYI0fdz9uzcMw4iMjDQkGVu3br1vzHg0cfse8IRo0qSJChQooCJFiqhTp05ycXHRwoULVahQoSz1c+HCBf3222/q3bu3ihYtalGX3hxCL7/8ssV6vXr1dOnSJSUkJNxzPyNHjlRkZKSqVq2qFStW6N1331VgYKCqVaumQ4cOZSpWJycn88/Xr1/XxYsXVbt2bRmGoV27dmWqj3/z8PDQ1q1bdebMmQfa/m6ZPZcPus9Lly7J1tY2w/9QtmnTRqtWrUqzvPHGG/fsd/To0fr5558VERGhChUqpKnPly9fuiOwAADW98wzz2jz5s1q3bq19uzZo/HjxyskJESFChVK95aaoKAg86hXSSpatKjatGmjFStW6Pbt2zIMQz/99JNatWolwzB08eJF8xISEqL4+HjzbVXz589X+fLlVa5cOYt2qVMIpN6+tGjRIqWkpOj9999P84S0f19bVKhQwTwiSLozurts2bL6+++/M3U+Zs6cqQIFCsjb21vVq1fX6tWr9eabb2rIkCEW7e6+hrh69aouXryoevXq6caNG/rzzz/vu5/MHntGuA7KuesgDw8PXb9+Pc1tZ3fr2LGjHB0dNWfOHHPZihUrdPHiRYs5yTIjT548sre3l3RnBN3ly5d169YtVa9ePd1bENu2bWtxff7000+rZs2a+uWXXzLcx4IFC5SSkqKOHTtavN58fX1VunRp8+stdSTUihUr7jmdxv79+xUTE6MWLVqYyzw8PHTgwAEdOXIk3W0uX76sNWvWqGPHjua/mYsXL+rSpUsKCQnRkSNHdPr0aUnSTz/9pCpVqphHTt3t33/z+fLlkySuLR9jJKWAJ8TUqVO1atUqrV27VgcPHtTff//9QBN6pl70ZeYWAElpLjJSP1iuXLly3207d+6s33//XVeuXNHKlSvVpUsX7dq1S61atdLNmzfvu31MTIx69uwpT09Pubi4qECBAmrQoIEkpblvPrPGjx+v/fv3q0iRInr66ac1YsSITF8I/1tmz2V27vNuhQsXVpMmTdIs6SWaUi1fvlwjR47UsGHDFBoamm4bwzCY5BwAHiI1atTQggULdOXKFW3btk3Dhg3T1atX9dxzz+ngwYMWbUuXLp1m+zJlyujGjRu6cOGCLly4oLi4OH311VcqUKCAxdKrVy9J/zcR95EjR3TgwIE07cqUKWPRLjo6WjY2Nvf8/En17+sK6c61RWauK6T/+4fMsmXLNGLECJlMJt24cSNNMuzAgQNq166d3N3d5ebmpgIFCpiTEZm5hsjssd8L10HZv09JevXVV1WmTBk1b95chQsXVu/evbV8+XKLNh4eHmrVqpUiIyPNZXPmzFGhQoXMicWsmDVrlipXrmyei6lAgQJatmxZur+HjP4Gjx8/nmH/R44ckWEYKl26dJrX3KFDh8yvt+LFi2vIkCH63//+p/z58yskJERTp05NE8eyZcvk4+NjfkiSdOdJ3nFxcSpTpowCAgL0xhtvaO/eveb6o0ePyjAMvffee2li+OCDDyRZ/s1n9ruEYRiS0v/nNx4PzCkFPCGefvppiw8Wa8mTJ0+65akfMJnh5uamZ555Rs8884zs7Ow0a9Ysbd261XxhlZ7bt2/rmWee0eXLl/XWW2+pXLlycnZ21unTp9WzZ0+lpKRk+VikO/85q1evnhYuXKiVK1fqk08+0bhx47RgwQLzRJDZ7UH36eXlpVu3bunq1atydXX9z3EcO3ZMXbt21TPPPKMPP/www3ZxcXEWcwEAAB4O9vb2qlGjhmrUqKEyZcqoV69emj9/vvkLY2akfn6+8MIL6tGjR7ptUuegTElJUUBAgCZOnJhuuyJFimTxCP77dUXqP2Qk6dlnn1X+/PnVv39/NWzY0DyHZlxcnBo0aCA3NzeNGjVKJUuWlKOjo/744w+99dZbmbqGyM5j5zooe6+DvL29tXv3bq1YsUK//vqrfv31V4WHh6t79+6aNWuWuV337t01f/58bdq0SQEBAVqyZIleffXVNAnM+/nuu+/Us2dPtW3bVm+88Ya8vb2VJ08ejR07Ns0E+w8qJSVFJpNJv/76a7p/I3ePFpswYYJ69uypxYsXa+XKlXrttdc0duxYbdmyRYULF5Z0Zz6pZs2aWSSC6tevr+joaPN2//vf/zRp0iTNmDFDL774ovk1NXTo0Az/8V2qVKksH1tqwplry8cXSSkA6crovxElSpSQpEw/BSS7Va9eXbNmzdLZs2clZRznvn379Ndff2nWrFnq3r27uTy9odpZ/c9LwYIF9eqrr+rVV1/V+fPnVa1aNY0ZM8Z8YZTZ/rJyLu+3z/SUK1dO0p1kUuoXhAf1zz//qH379vLw8NDcuXMzvCA7ffq0kpKSVL58+f+0PwBAzkr9R1Xq52mq9G7N+euvv5Q3b17zk3ldXV11+/bt+z7dtWTJktqzZ48aN258z8/GkiVLKiUlRQcPHkwz6XNOe+mllzRp0iQNHz5c7dq1k8lk0rp163Tp0iUtWLBA9evXN7dNfWLx3TI6rswee1ZxHZQ910H29vZq1aqVWrVqpZSUFL366qv68ssv9d5775kTJ82aNVOBAgU0Z84c1axZUzdu3FC3bt0ydWx3+/HHH1WiRAktWLDA4txklAzO6G/w3088vlvJkiVlGIaKFy9uHo13LwEBAQoICNDw4cO1adMm1alTRzNmzNCHH36ouLg4bdq0Sf3790+znaenp3r16qVevXrp2rVrql+/vkaMGKEXX3zR/Pu0s7PL1HtDZr9LHDt2TDY2Npk6LjyauH0PQLry5s0r6c5/C+9WoEAB1a9fX998841iYmIs6rIy+ulebty4oc2bN6db9+uvv0qSypYtK0lydnZON87U/xLdHZNhGObH/d4toz7+7fbt22mGN3t7e8vPz8/i8c7Ozs6ZGhafmXOZ2X2mJygoSJK0Y8eO+8ZyPy+//LL++usvLVy40HwLZnp27twp6f+e/AMAyF1r165N9/M5dX6a1M/TVJs3b7aY5+bkyZNavHixmjZtqjx58ihPnjwKDQ3VTz/9lO6XygsXLph/7tixo06fPq2vv/46Tbt//vlH169fl3RnDh0bGxuNGjUqzQie7Lq2yIitra1ef/11HTp0yPzY+fSuIZKSkjRt2rQ022f0mZ/ZY08P10E5ex106dIli3UbGxtz0uruPm1tbdW5c2f98MMPioiIUEBAwAP9ky+938XWrVsz/B0vWrTIPPeSJG3btk1bt269ZwKuffv2ypMnj0aOHJnmb8YwDPMxJyQk6NatWxb1AQEBsrGxMR/7ypUrJcniqZdS2vPm4uKiUqVKmbfz9vZWcHCwvvzyyzTJbsnyvSE0NFR79uzRwoUL07T7d/w7d+5UxYoVLZ4MiMcLI6UApMvJyUkVKlTQ999/rzJlysjT01OVKlVSpUqV9Pnnn6tu3bqqVq2a+vXrp+LFi+v48eNatmyZdu/e/Z/3fePGDdWuXVu1atVSs2bNVKRIEcXFxWnRokX6/fff1bZtW1WtWlXSnf+0eHh4aMaMGXJ1dZWzs7Nq1qypcuXKqWTJkho6dKhOnz4tNzc3/fTTT+nOOZE6oetrr72mkJAQ5cmTR506dUrT7urVqypcuLCee+45ValSRS4uLoqKitL27ds1YcIEi/6+//57DRkyRDVq1JCLi4tatWqV7rHe71xmdp/pKVGihCpVqqSoqCj17t070+f/35YtW6bZs2crNDRUe/futZg/wMXFRW3btjWvr1q1SkWLFjX/fgAAuWvAgAG6ceOG2rVrp3LlyikpKUmbNm3S999/r2LFipnngUpVqVIlhYSE6LXXXpODg4M5ETNy5Ehzm48//lhr165VzZo11bdvX1WoUEGXL1/WH3/8oaioKF2+fFmS1K1bN/3www96+eWXtXbtWtWpU0e3b9/Wn3/+qR9++EErVqxQ9erVVapUKb377rsaPXq06tWrp/bt28vBwUHbt2+Xn5+fxo4dm6PnqGfPnnr//fc1btw4tW3bVrVr11a+fPnUo0cPvfbaazKZTPr222/TTZBl9Jmf2WNPD9dBOXsd9OKLL+ry5ctq1KiRChcurBMnTuiLL77QU089lWakd/fu3fX5559r7dq1Gjdu3D33l5GWLVtqwYIFateunVq0aKFjx45pxowZqlChgq5du5amfalSpVS3bl298sorSkxM1GeffSYvLy+9+eabGe6jZMmS+vDDDzVs2DAdP35cbdu2laurq44dO6aFCxeqX79+Gjp0qNasWaP+/furQ4cOKlOmjG7duqVvv/3WnGyW7lz31a1bN00SqEKFCgoODlZgYKA8PT21Y8cO/fjjjxYjqqZOnaq6desqICBAffv2VYkSJXTu3Dlt3rxZp06d0p49eyRJb7zxhn788Ud16NBBvXv3VmBgoC5fvqwlS5ZoxowZqlKliiQpOTlZ69ev16uvvvpA5x6PCGs95g9A7kh9VO327dvv2a5Hjx5pHt+7adMmIzAw0LC3t0/zSOD9+/cb7dq1Mzw8PAxHR0ejbNmyxnvvvWeuT31U7YULF9KN5+5H5/5bcnKy8fXXXxtt27Y1/P39DQcHByNv3rxG1apVjU8++STNY3UXL15sVKhQwbC1tbV41PDBgweNJk2aGC4uLkb+/PmNvn37Gnv27EnzOOJbt24ZAwYMMAoUKGCYTCaLR+zefdyJiYnGG2+8YVSpUsVwdXU1nJ2djSpVqhjTpk2ziOfatWtGly5dDA8PD0OS+bym9yjk+53LzO4zIxMnTjRcXFzSPBZaUoaPV/73ayZ1Pb3l7tfM7du3jYIFCxrDhw/PVGwAgJz366+/Gr179zbKlStnuLi4GPb29kapUqWMAQMGGOfOnbNom/rZ8N133xmlS5c2HBwcjKpVqxpr165N0++5c+eMsLAwo0iRIoadnZ3h6+trNG7c2Pjqq68s2iUlJRnjxo0zKlasaDg4OBj58uUzAgMDjZEjRxrx8fEWbb/55hujatWq5nYNGjQwVq1aZa739/dP95H0DRo0MBo0aHDfc3Gvz74RI0YYkszHunHjRqNWrVqGk5OT4efnZ7z55pvGihUrLNoYRsaf+Vk99rtxHZSz10E//vij0bRpU8Pb29uwt7c3ihYtarz00kvG2bNn0+2jYsWKho2NjXHq1Kk0deld1/779ZiSkmJ89NFH5t9l1apVjZ9//jnNtXfq+fnkk0+MCRMmGEWKFDEcHByMevXqGXv27LHYb+p19r/99NNPRt26dQ1nZ2fD2dnZKFeunBEWFmYcPnzYMAzD+Pvvv43evXsbJUuWNBwdHQ1PT0+jYcOGRlRUlDlWb29vY/z48Wn6/vDDD42nn37a8PDwMJycnIxy5coZY8aMMZKSkizaRUdHG927dzd8fX0NOzs7o1ChQkbLli2NH3/80aLdpUuXjP79+xuFChUy7O3tjcKFCxs9evQwLl68aG7z66+/GpKMI0eOpIkHjw+TYeTwmFgAQK6Jj49XiRIlNH78ePXp0ydH97Vo0SJ16dJF0dHRKliwYI7uCwCQ/Uwmk8LCwjRlypTcDgXIFtlxHVS1alV5enpq9erV2Rzdw2fbtm2qWbOmDhw4kKmnYea0tm3bymQypXubHx4fzCkFAI8xd3d3vfnmm/rkk08e+Ek7mTVu3Dj179+fhBQAAHgo/NfroB07dmj37t0Wk8U/7j766KOHIiF16NAh/fzzzxo9enRuh4IcxkgpAAAAAIyUAv6//fv3a+fOnZowYYIuXryov//+W46OjrkdFvBYYqQUAAAAAAD/348//qhevXopOTlZc+fOJSEF5CBGSgEAAAAAAMDqGCkFAAAAAAAAqyMpBQAAAAAAAKuzze0AHgYpKSk6c+aMXF1dZTKZcjscAADwEDMMQ1evXpWfn59sbJ6c/+9xvQQAADIrs9dLJKUknTlzRkWKFMntMAAAwCPk5MmTKly4cG6HYTVcLwEAgKy63/USSSlJrq6uku6cLDc3t1yOBgAAPMwSEhJUpEgR8/XDk4LrJQAAkFmZvV4iKSWZh6C7ublxkQUAADLlSbuFjeslAACQVfe7XnpyJkIAAAAAAADAQ4OkFAAAAAAAAKyOpBQAAAAAAACsjqQUAAAAAAAArI6kFAAAAAAAAKyOpBQAAAAAAACsjqQUAAAAAAAArI6kFAAAAAAAAKyOpBQAAAAAAACsjqQUAAAAAAAArI6kFAAAAAAAAKyOpBQAAAAAAACsjqQUAAAAAAAArI6kFAAAAAAAAKyOpBQAAAAAAACsjqQUgCeSyWTSokWLHpp+AAAAAOBJY5vbAQBARlq1aqXk5GQtX748Td3vv/+u+vXra8+ePapcuXKW+z579qzy5cuX6fYjRozQokWLtHv37v/UD7JHkeBpuR0CHnIn172a2yEAAJCrdppMuR0CHnKBhpHbITBSCsDDq0+fPlq1apVOnTqVpi48PFzVq1fPckIqKSlJkuTr6ysHB4f/HGN29QMAAAAATxqSUgAeWi1btlSBAgUUERFhUX7t2jXNnz9fbdu2VefOnVWoUCHlzZtXAQEBmjt3rkXb4OBg9e/fX4MGDVL+/PkVEhIiKe1td2+99ZbKlCmjvHnzqkSJEnrvvfeUnJwsSYqIiNDIkSO1Z88emUwmmUwmc0z/7mffvn1q1KiRnJyc5OXlpX79+unatWvm+p49e6pt27b69NNPVbBgQXl5eSksLMy8LwB4EMWKFTO/P929hIWFSZJu3rypsLAweXl5ycXFRaGhoTp37lwuRw0AAJ50JKUAPLRsbW3VvXt3RUREyLhraOn8+fN1+/ZtvfDCCwoMDNSyZcu0f/9+9evXT926ddO2bdss+pk1a5bs7e21ceNGzZgxI919ubq6KiIiQgcPHtTkyZP19ddfa9KkSZKk559/Xq+//roqVqyos2fP6uzZs3r++efT9HH9+nWFhIQoX7582r59u+bPn6+oqCj179/fot3atWsVHR2ttWvXatasWYqIiLBIvI0YMULFihV7wLMG4Em0fft28/vT2bNntWrVKklShw4dJEmDBw/W0qVLNX/+fK1fv15nzpxR+/btczNkAAAA5pQC8HDr3bu3PvnkE61fv17BwcGS7ty6FxoaKn9/fw0dOtTcdsCAAVqxYoV++OEHPf300+by0qVLa/z48ffcz/Dhw80/FytWTEOHDtW8efP05ptvysnJSS4uLrK1tZWvr2+GfURGRurmzZuaPXu2nJ2dJUlTpkxRq1atNG7cOPn4+EiS8uXLpylTpihPnjwqV66cWrRoodWrV6tv376SpPz586tkyZJZO1EAnmgFChSwWP/4449VsmRJNWjQQPHx8Zo5c6YiIyPVqFEjSXfeR8uXL68tW7aoVq1auREyAAAAI6UAPNzKlSun2rVr65tvvpEkHT16VL///rv69Omj27dva/To0QoICJCnp6dcXFy0YsUKxcTEWPQRGBh43/18//33qlOnjnx9feXi4qLhw4en6ed+Dh06pCpVqpgTUpJUp04dpaSk6PDhw+ayihUrKk+ePOb1ggUL6vz58+b1/v37a/Xq1VnaNwCkSkpK0nfffafevXvLZDJp586dSk5OVpMmTcxtypUrp6JFi2rz5s0Z9pOYmKiEhASLBQAAIDuRlALw0OvTp49++uknXb16VeHh4eb//n/yySeaPHmy3nrrLa1du1a7d+9WSEiIeTLzVHcnidKzefNmde3aVc8++6x+/vln7dq1S++++26afrKLnZ2dxbrJZFJKSkqO7AvAk2fRokWKi4tTz549JUmxsbGyt7eXh4eHRTsfHx/FxsZm2M/YsWPl7u5uXooUKZKDUQMAgCcRSSkAD72OHTvKxsZGkZGRmj17tvm//xs3blSbNm30wgsvqEqVKipRooT++uuvLPe/adMm+fv7691331X16tVVunRpnThxwqKNvb29bt++fc9+ypcvrz179uj69evmso0bN8rGxkZly5bNclwA8CBmzpyp5s2by8/P7z/1M2zYMMXHx5uXkydPZlOEAAAAd5CUAvDQc3Fx0fPPP69hw4bp7Nmz5v/+ly5dWqtWrdKmTZt06NAhvfTSSw/0NKnSpUsrJiZG8+bNU3R0tD7//HMtXLjQok2xYsV07Ngx7d69WxcvXlRiYmKafrp27SpHR0f16NFD+/fv19q1azVgwAB169bNPJ9UZkyZMkWNGzfO8nEAwIkTJxQVFaUXX3zRXObr66ukpCTFxcVZtD137tw958lzcHCQm5ubxQIAAJCdSEoBeCT06dNHV65cUUhIiPm//8OHD1e1atUUEhKi4OBg+fr6qm3btlnuu3Xr1ho8eLD69++vp556Sps2bdJ7771n0SY0NFTNmjVTw4YNVaBAAc2dOzdNP3nz5tWKFSt0+fJl1ahRQ88995waN26sKVOmZCmeixcvKjo6OsvHAQDh4eHy9vZWixYtzGWBgYGys7OzmKvu8OHDiomJUVBQUG6ECQAAIEkyGXc/Z/0JlZCQIHd3d8XHx/NfQAB4BBQJnpbbIeAhd3LdqznW98N63ZCSkqLixYurc+fO+vjjjy3qXnnlFf3yyy+KiIiQm5ubBgwYIOnO7cuZ9bAeNwAgfTtNptwOAQ+5wBxMB2X2usE2xyIAAACA1URFRSkmJka9e/dOUzdp0iTZ2NgoNDRUiYmJCgkJ0bRpJHcBAEDuIikFAADwGGjatKkyGgDv6OioqVOnaurUqVaOCgAAIGPMKQUAAAAAAACrIykFAAAAAAAAqyMpBQAAAAAAAKsjKQXgkTBixAiZTCaLpVy5chZtgoOD07R5+eWX79nvtWvX1L9/fxUuXFhOTk6qUKGCZsyY8Z/7DQ4O1qBBg9KUR0REyMPDI1PHDAAAAACPMyY6B/DIqFixoqKioszrtrZp38L69u2rUaNGmdfz5s17zz6HDBmiNWvW6LvvvlOxYsW0cuVKvfrqq/Lz81Pr1q0fuF8AAAAAwL2RlALwyLC1tZWvr+892+TNm/e+be62adMm9ejRQ8HBwZKkfv366csvv9S2bdssklJZ7TezihUrphMnTqQpz+gJWgAAAADwuOD2PQCPjCNHjsjPz08lSpRQ165dFRMTk6bNnDlzlD9/flWqVEnDhg3TjRs37tln7dq1tWTJEp0+fVqGYWjt2rX666+/1LRp0//Ub2Zt375dZ8+e1dmzZ3Xq1CnVqlVL9erVy5a+AQAAAOBhxkgpAI+EmjVrKiIiQmXLltXZs2c1cuRI1atXT/v375erq6skqUuXLvL395efn5/27t2rt956S4cPH9aCBQsy7PeLL75Qv379VLhwYdna2srGxkZff/216tevb27zIP1K0rRp0/S///3PouzWrVtydHQ0rxcoUMD888CBA3X27Flt3749S+cGAAAAAB5FJKUAPBKaN29u/rly5cqqWbOm/P399cMPP6hPnz6S7tx6lyogIEAFCxZU48aNFR0drZIlS6bb7xdffKEtW7ZoyZIl8vf312+//aawsDD5+fmpSZMmD9yvJHXt2lXvvvuuRdmCBQv00UcfpWn71VdfaebMmdq0aZNFogoAAAAAHlckpQA8kjw8PFSmTBkdPXo0wzY1a9aUJB09ejTd5NE///yjd955RwsXLlSLFi0k3Ul47d69W59++qk5KZXVflO5u7urVKlSFmXe3t5p2q1du1YDBgzQ3LlzVbly5Qz7AwAAAIDHCXNKAXgkXbt2TdHR0SpYsGCGbXbv3i1JGbZJTk5WcnKybGws3wrz5MmjlJSUB+43K44eParnnntO77zzjtq3b/+f+wMAAACARwUjpQA8EoYOHapWrVrJ399fZ86c0QcffKA8efKoc+fOkqTo6GhFRkbq2WeflZeXl/bu3avBgwerfv36FqOPypUrp7Fjx6pdu3Zyc3NTgwYN9MYbb8jJyUn+/v5av369Zs+erYkTJ2ap3wfxzz//qFWrVqpatar69eun2NhYc11OPOkPAAAAAB4mJKUAPBJOnTqlzp0769KlSypQoIDq1q2rLVu2mOdfsre3V1RUlD777DNdv35dRYoUUWhoqIYPH27Rz+HDhxUfH29enzdvnoYNG6auXbvq8uXL8vf315gxY/Tyyy9nqd8Hce7cOf3555/6888/5efnZ1FnGMZ/7h8AAAAAHmYmg28+SkhIkLu7u+Lj4+Xm5pbb4QAA7qNI8LTcDgEPuZPrXs2xvp/U64Yn9bgB4FG102TK7RDwkAvMwXRQZq8bmFMKAAAAAAAAVkdSCgAAAAAAAFZHUgoAAAAAAABWR1IKAAAAAAAAVperSamxY8eqRo0acnV1lbe3t9q2bavDhw9btAkODpbJZLJYUp+KlSomJkYtWrRQ3rx55e3trTfeeEO3bt2y5qEAAAAAAAAgC2xzc+fr169XWFiYatSooVu3bumdd95R06ZNdfDgQTk7O5vb9e3bV6NGjTKv582b1/zz7du31aJFC/n6+mrTpk06e/asunfvLjs7O3300UdWPR4AAAAAAABkTq4mpZYvX26xHhERIW9vb+3cuVP169c3l+fNm1e+vr7p9rFy5UodPHhQUVFR8vHx0VNPPaXRo0frrbfe0ogRI2Rvb5+jxwA8TtpO/SO3Q8BDblFYtdwOAQAAAMBj4qGaUyo+Pl6S5OnpaVE+Z84c5c+fX5UqVdKwYcN048YNc93mzZsVEBAgHx8fc1lISIgSEhJ04MAB6wQOAAAAAACALMnVkVJ3S0lJ0aBBg1SnTh1VqlTJXN6lSxf5+/vLz89Pe/fu1VtvvaXDhw9rwYIFkqTY2FiLhJQk83psbGy6+0pMTFRiYqJ5PSEhIbsPBwAAAAAAAPfw0CSlwsLCtH//fm3YsMGivF+/fuafAwICVLBgQTVu3FjR0dEqWbLkA+1r7NixGjly5H+KFwAAAAAAAA/uobh9r3///vr555+1du1aFS5c+J5ta9asKUk6evSoJMnX11fnzp2zaJO6ntE8VMOGDVN8fLx5OXny5H89BAAAAAAAAGRBrialDMNQ//79tXDhQq1Zs0bFixe/7za7d++WJBUsWFCSFBQUpH379un8+fPmNqtWrZKbm5sqVKiQbh8ODg5yc3OzWAAAAAAAAGA9uXr7XlhYmCIjI7V48WK5urqa54Byd3eXk5OToqOjFRkZqWeffVZeXl7au3evBg8erPr166ty5cqSpKZNm6pChQrq1q2bxo8fr9jYWA0fPlxhYWFycHDIzcMDAAAAAABABnJ1pNT06dMVHx+v4OBgFSxY0Lx8//33kiR7e3tFRUWpadOmKleunF5//XWFhoZq6dKl5j7y5Mmjn3/+WXny5FFQUJBeeOEFde/eXaNGjcqtwwIAAAAAAMB95OpIKcMw7llfpEgRrV+//r79+Pv765dffsmusAAAAAAAAJDDHoqJzgEAAAAAAPBkISkFAAAAAAAAqyMpBQAAAAAAAKsjKQUAAAAAAACrIykFAAAAAAAAqyMpBQAAAAAAAKsjKQUAAAAAAACrIykFAAAAAAAAqyMpBQAAAAAAAKsjKQUAAAAAAACrIykFAAAAAAAAqyMpBQAAAAAAAKsjKQUAAPCIO336tF544QV5eXnJyclJAQEB2rFjh7neMAy9//77KliwoJycnNSkSRMdOXIkFyMGAAAgKQUAAPBIu3LliurUqSM7Ozv9+uuvOnjwoCZMmKB8+fKZ24wfP16ff/65ZsyYoa1bt8rZ2VkhISG6efNmLkYOAACedLa5HQAAAAAe3Lhx41SkSBGFh4eby4oXL27+2TAMffbZZxo+fLjatGkjSZo9e7Z8fHy0aNEiderUyeoxAwAASIyUAgAAeKQtWbJE1atXV4cOHeTt7a2qVavq66+/NtcfO3ZMsbGxatKkibnM3d1dNWvW1ObNmzPsNzExUQkJCRYLAABAdiIpBQAA8Aj7+++/NX36dJUuXVorVqzQK6+8otdee02zZs2SJMXGxkqSfHx8LLbz8fEx16Vn7Nixcnd3Ny9FihTJuYMAAABPJJJSAAAAj7CUlBRVq1ZNH330kapWrap+/fqpb9++mjFjxn/qd9iwYYqPjzcvJ0+ezKaIAQAA7iApBQAA8AgrWLCgKlSoYFFWvnx5xcTESJJ8fX0lSefOnbNoc+7cOXNdehwcHOTm5maxAAAAZCeSUgAAAI+wOnXq6PDhwxZlf/31l/z9/SXdmfTc19dXq1evNtcnJCRo69atCgoKsmqsAAAAd+PpewAAAI+wwYMHq3bt2vroo4/UsWNHbdu2TV999ZW++uorSZLJZNKgQYP04YcfqnTp0ipevLjee+89+fn5qW3btrkbPAAAeKKRlAIAAHiE1ahRQwsXLtSwYcM0atQoFS9eXJ999pm6du1qbvPmm2/q+vXr6tevn+Li4lS3bl0tX75cjo6OuRg5AAB40pGUAgAAeMS1bNlSLVu2zLDeZDJp1KhRGjVqlBWjAgAAuDfmlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDV2eZ2AE+Kt38xcjsEPMQ+ftaU2yEAAAAAAGBVjJQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1eVqUmrs2LGqUaOGXF1d5e3trbZt2+rw4cMWbW7evKmwsDB5eXnJxcVFoaGhOnfunEWbmJgYtWjRQnnz5pW3t7feeOMN3bp1y5qHAgAAAAAAgCzI1aTU+vXrFRYWpi1btmjVqlVKTk5W06ZNdf36dXObwYMHa+nSpZo/f77Wr1+vM2fOqH379ub627dvq0WLFkpKStKmTZs0a9YsRURE6P3338+NQwIAAAAAAEAm2ObmzpcvX26xHhERIW9vb+3cuVP169dXfHy8Zs6cqcjISDVq1EiSFB4ervLly2vLli2qVauWVq5cqYMHDyoqKko+Pj566qmnNHr0aL311lsaMWKE7O3tc+PQAAAAAAAAcA8P1ZxS8fHxkiRPT09J0s6dO5WcnKwmTZqY25QrV05FixbV5s2bJUmbN29WQECAfHx8zG1CQkKUkJCgAwcOWDF6AAAAAAAAZFaujpS6W0pKigYNGqQ6deqoUqVKkqTY2FjZ29vLw8PDoq2Pj49iY2PNbe5OSKXWp9alJzExUYmJieb1hISE7DoMAAAAAAAAZMJDM1IqLCxM+/fv17x583J8X2PHjpW7u7t5KVKkSI7vEwAAAAAAAP/noUhK9e/fXz///LPWrl2rwoULm8t9fX2VlJSkuLg4i/bnzp2Tr6+vuc2/n8aXup7a5t+GDRum+Ph483Ly5MlsPBoAAAAAAADcT64mpQzDUP/+/bVw4UKtWbNGxYsXt6gPDAyUnZ2dVq9ebS47fPiwYmJiFBQUJEkKCgrSvn37dP78eXObVatWyc3NTRUqVEh3vw4ODnJzc7NYAAAAAAAAYD25mpQKCwvTd999p8jISLm6uio2NlaxsbH6559/JEnu7u7q06ePhgwZorVr12rnzp3q1auXgoKCVKtWLUlS06ZNVaFCBXXr1k179uzRihUrNHz4cIWFhcnBwSE3Dw8AAMAqRowYIZPJZLGUK1fOXH/z5k2FhYXJy8tLLi4uCg0NTTPSHAAAwNpyNSk1ffp0xcfHKzg4WAULFjQv33//vbnNpEmT1LJlS4WGhqp+/fry9fXVggULzPV58uTRzz//rDx58igoKEgvvPCCunfvrlGjRuXGIQEAAOSKihUr6uzZs+Zlw4YN5rrBgwdr6dKlmj9/vtavX68zZ86offv2uRgtAABALj99zzCM+7ZxdHTU1KlTNXXq1Azb+Pv765dffsnO0AAAAB4ptra26c6nGR8fr5kzZyoyMlKNGjWSJIWHh6t8+fLasmWLefQ5AACAtT0UE50DAADgvzly5Ij8/PxUokQJde3aVTExMZKknTt3Kjk5WU2aNDG3LVeunIoWLarNmzdn2F9iYqISEhIsFgAAgOxEUgoAAOARV7NmTUVERGj58uWaPn26jh07pnr16unq1auKjY2Vvb29PDw8LLbx8fFRbGxshn2OHTtW7u7u5qVIkSI5fBQAAOBJk6u37wEAAOC/a968ufnnypUrq2bNmvL399cPP/wgJyenB+pz2LBhGjJkiHk9ISGBxBQAAMhWjJQCAAB4zHh4eKhMmTI6evSofH19lZSUpLi4OIs2586dS3cOqlQODg5yc3OzWAAAALITSSkAAIDHzLVr1xQdHa2CBQsqMDBQdnZ2Wr16tbn+8OHDiomJUVBQUC5GCQAAnnTcvgcAAPCIGzp0qFq1aiV/f3+dOXNGH3zwgfLkyaPOnTvL3d1dffr00ZAhQ+Tp6Sk3NzcNGDBAQUFBPHkPAADkKpJSAAAAj7hTp06pc+fOunTpkgoUKKC6detqy5YtKlCggCRp0qRJsrGxUWhoqBITExUSEqJp06blctQAAOBJR1IKAADgETdv3rx71js6Omrq1KmaOnWqlSICAAC4P+aUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1ZGUAgAAAAAAgNWRlAIAAAAAAIDVkZQCAAAAAACA1dlmdYNjx47p999/14kTJ3Tjxg0VKFBAVatWVVBQkBwdHXMiRgAAAAAAADxmMp2UmjNnjiZPnqwdO3bIx8dHfn5+cnJy0uXLlxUdHS1HR0d17dpVb731lvz9/XMyZgAAAAAAADziMpWUqlq1quzt7dWzZ0/99NNPKlKkiEV9YmKiNm/erHnz5ql69eqaNm2aOnTokCMBAwAAAAAA4NGXqaTUxx9/rJCQkAzrHRwcFBwcrODgYI0ZM0bHjx/PrvgAAAAAAADwGMpUUupeCal/8/LykpeX1wMHBAAAAAAAgMdflp++16hRI40cOTJN+ZUrV9SoUaNsCQoAAAAAAACPtyw/fW/dunXat2+fdu3apTlz5sjZ2VmSlJSUpPXr12d7gAAAAAAAAHj8ZHmklCRFRUUpNjZWtWrVYv4oAAAAAAAAZNkDJaUKFiyo9evXKyAgQDVq1NC6deuyOSwAAAAAAAA8zrKclDKZTJLuPHEvMjJSAwcOVLNmzTRt2rRsDw4AAAAAAACPpyzPKWUYhsX68OHDVb58efXo0SPbggIAAAAAAMDjLctJqWPHjil//vwWZaGhoSpbtqx27tyZbYEBAAAAAADg8ZXlpJS/v3+65ZUqVVKlSpX+c0AAAAAAAAB4/GU6KdW+fftMtVuwYMEDBwMAAAAAAIAnQ6aTUu7u7hbrkZGRatWqlVxdXbM9KAAAAAAAADzeMp2UCg8Pt1j/8ccfNX78eJUoUSLbgwIAAAAAAMDjzSa3AwAAAAAAAMCTh6QUAAAAAAAArI6kFAAAAAAAAKwu03NKLVmyxGI9JSVFq1ev1v79+y3KW7dunT2RAQAAAAAA4LGV6aRU27Zt05S99NJLFusmk0m3b9/+z0EBAAAAAADg8ZbppFRKSkpOxgEAAAAAAIAnCHNKAQAAAAAAwOoylZTasmVLpju8ceOGDhw48MABAQAAAAAA4PGXqaRUt27dFBISovnz5+v69evptjl48KDeeecdlSxZUjt37szWIAEAAAAAAPB4ydScUgcPHtT06dM1fPhwdenSRWXKlJGfn58cHR115coV/fnnn7p27ZratWunlStXKiAgIKfjBgAAAAAAwCMsU0kpOzs7vfbaa3rttde0Y8cObdiwQSdOnNA///yjKlWqaPDgwWrYsKE8PT1zOl4AAAAAAAA8BjL99L1U1atXV/Xq1XMiFgAAAAAAADwhePoeAAAAAAAArI6kFAAAAAAAAKyOpBQAAAAAAACsjqQUAAAAAAAArC7LSam///47J+IAAABANvj4449lMpk0aNAgc9nNmzcVFhYmLy8vubi4KDQ0VOfOncu9IAEAAPQASalSpUqpYcOG+u6773Tz5s2ciAkAAAAPYPv27fryyy9VuXJli/LBgwdr6dKlmj9/vtavX68zZ86offv2uRQlAADAHVlOSv3xxx+qXLmyhgwZIl9fX7300kvatm1bTsQGAACATLp27Zq6du2qr7/+Wvny5TOXx8fHa+bMmZo4caIaNWqkwMBAhYeHa9OmTdqyZUsuRgwAAJ50WU5KPfXUU5o8ebLOnDmjb775RmfPnlXdunVVqVIlTZw4URcuXMiJOAEAAHAPYWFhatGihZo0aWJRvnPnTiUnJ1uUlytXTkWLFtXmzZutHSYAAIDZA090bmtrq/bt22v+/PkaN26cjh49qqFDh6pIkSLq3r27zp49m51xAgAAIAPz5s3TH3/8obFjx6api42Nlb29vTw8PCzKfXx8FBsbm2GfiYmJSkhIsFgAAACy0wMnpXbs2KFXX31VBQsW1MSJEzV06FBFR0dr1apVOnPmjNq0aZOdcQIAADy2jh49qhUrVuiff/6RJBmGkeltT548qYEDB2rOnDlydHTMtpjGjh0rd3d381KkSJFs6xsAAEB6gKTUxIkTFRAQoNq1a+vMmTOaPXu2Tpw4oQ8//FDFixdXvXr1FBERoT/++CMn4gUAAHhsXLp0SU2aNFGZMmX07LPPmkea9+nTR6+//nqm+ti5c6fOnz+vatWqydbWVra2tlq/fr0+//xz2draysfHR0lJSYqLi7PY7ty5c/L19c2w32HDhik+Pt68nDx58oGPEwAAID1ZTkpNnz5dXbp00YkTJ7Ro0SK1bNlSNjaW3Xh7e2vmzJn37eu3335Tq1at5OfnJ5PJpEWLFlnU9+zZUyaTyWJp1qyZRZvLly+ra9eucnNzk4eHh/r06aNr165l9bAAAACsbvDgwbK1tVVMTIzy5s1rLn/++ee1fPnyTPXRuHFj7du3T7t37zYv1atXV9euXc0/29nZafXq1eZtDh8+rJiYGAUFBWXYr4ODg9zc3CwWAACA7GSb1Q2OHDly3zb29vbq0aPHfdtdv35dVapUUe/evTN8LHGzZs0UHh5uXndwcLCo79q1q86ePatVq1YpOTlZvXr1Ur9+/RQZGXnf/QMAAOSmlStXasWKFSpcuLBFeenSpXXixIlM9eHq6qpKlSpZlDk7O8vLy8tc3qdPHw0ZMkSenp5yc3PTgAEDFBQUpFq1amXPgQAAADyALCelwsPD5eLiog4dOliUz58/Xzdu3MhUMipV8+bN1bx583u2cXBwyHBo+aFDh7R8+XJt375d1atXlyR98cUXevbZZ/Xpp5/Kz88v07EAAABY2/Xr1y1GSKW6fPlymn/E/ReTJk2SjY2NQkNDlZiYqJCQEE2bNi3b+gcAAHgQWb59b+zYscqfP3+acm9vb3300UfZEtTd1q1bJ29vb5UtW1avvPKKLl26ZK7bvHmzPDw8zAkpSWrSpIlsbGy0devWbI8FAAAgO9WrV0+zZ882r5tMJqWkpGj8+PFq2LDhA/e7bt06ffbZZ+Z1R0dHTZ06VZcvX9b169e1YMGCe84nBQAAYA1ZHikVExOj4sWLpyn39/dXTExMtgSVqlmzZmrfvr2KFy+u6OhovfPOO2revLk2b96sPHnyKDY2Vt7e3hbb2NraytPT876POE5MTDSv84hjAACQG8aPH6/GjRtrx44dSkpK0ptvvqkDBw7o8uXL2rhxY26HBwAAkKOynJTy9vbW3r17VaxYMYvyPXv2yMvLK7vikiR16tTJ/HNAQIAqV66skiVLat26dWrcuPED9zt27FiNHDkyO0IEAAB4YJUqVdJff/2lKVOmyNXVVdeuXVP79u0VFhamggUL5nZ4AAAAOSrLSanOnTvrtddek6urq+rXry9JWr9+vQYOHGiRRMoJJUqUUP78+XX06FE1btxYvr6+On/+vEWbW7du6fLly/d9xPGQIUPM6wkJCSpSpEiOxQ0AAJCemJgYFSlSRO+++266dUWLFs2FqAAAAKwjy0mp0aNH6/jx42rcuLFsbe9snpKSou7du+fInFJ3O3XqlC5dumT+z2FQUJDi4uK0c+dOBQYGSpLWrFmjlJQU1axZM8N+HBwcsnXyUAAAgAdRvHhxnT17Ns10BJcuXVLx4sV1+/btXIoMAAAg52U5KWVvb6/vv/9eo0eP1p49e+Tk5KSAgAD5+/tneefXrl3T0aNHzevHjh3T7t275enpKU9PT40cOVKhoaHy9fVVdHS03nzzTZUqVUohISGSpPLly6tZs2bq27evZsyYoeTkZPXv31+dOnXiyXsAAOChZxiGTCZTmvJr167J0dExFyICAACwniwnpVKVKVNGZcqU+U8737Fjh8WTZVJvqevRo4emT5+uvXv3atasWYqLi5Ofn5+aNm2q0aNHW4xymjNnjvr376/GjRubH3X8+eef/6e4AAAAclLqNY/JZNJ7772nvHnzmutu376trVu36qmnnsql6AAAAKwjy0mp27dvKyIiQqtXr9b58+eVkpJiUb9mzZpM9xUcHCzDMDKsX7FixX378PT0VGRkZKb3CQAAkNt27dol6c5IqX379sne3t5cZ29vrypVqmjo0KG5FR4AAIBVZDkpNXDgQEVERKhFixaqVKlSukPOAQAAkLG1a9dKknr16qXJkyfLzc0tlyMCAACwviwnpebNm6cffvhBzz77bE7EAwAA8MQIDw/P7RAAAAByzQNNdF6qVKmciAUAAOCJs2PHDv3www+KiYlRUlKSRd2CBQtyKSoAAICcZ5PVDV5//XVNnjz5nnNBAQAA4P7mzZun2rVr69ChQ1q4cKGSk5N14MABrVmzRu7u7rkdHgAAQI7K8kipDRs2aO3atfr1119VsWJF2dnZWdTzHz0AAIDM+eijjzRp0iSFhYXJ1dVVkydPVvHixfXSSy+pYMGCuR0eAABAjspyUsrDw0Pt2rXLiVgAAACeKNHR0WrRooWkO1MkXL9+XSaTSYMHD1ajRo00cuTIXI4QAAAg52Q5KcWEnAAAANkjX758unr1qiSpUKFC2r9/vwICAhQXF6cbN27kcnQAAAA5K8tzSknSrVu3FBUVpS+//NJ8IXXmzBldu3YtW4MDAAB4nNWvX1+rVq2SJHXo0EEDBw5U37591blzZzVq1CiXowMAAMhZWR4pdeLECTVr1kwxMTFKTEzUM888I1dXV40bN06JiYmaMWNGTsQJAADw2JkyZYpu3rwpSXr33XdlZ2enTZs2KTQ0VEOHDs3l6AAAAHJWlkdKDRw4UNWrV9eVK1fk5ORkLm/Xrp1Wr16drcEBAAA8zjw9PeXn5ydJsrGx0dtvv60ffvhBfn5+qlq1ai5HBwAAkLOynJT6/fffNXz4cNnb21uUFytWTKdPn862wAAAAB5XiYmJGjZsmKpXr67atWtr0aJFku7M3VmyZElNnjxZgwcPzt0gAQAAcliWb99LSUnR7du305SfOnVKrq6u2RIUAADA4+z999/Xl19+qSZNmmjTpk3q0KGDevXqpS1btmjChAnq0KGD8uTJk9thAgAA5Kgsj5Rq2rSpPvvsM/O6yWTStWvX9MEHH+jZZ5/NztgAAAAeS/Pnz9fs2bP1448/auXKlbp9+7Zu3bqlPXv2qFOnTiSkAADAEyHLI6UmTJigkJAQVahQQTdv3lSXLl105MgR5c+fX3Pnzs2JGAEAAB4rp06dUmBgoCSpUqVKcnBw0ODBg2UymXI5MgAAAOvJclKqcOHC2rNnj+bNm6e9e/fq2rVr6tOnj7p27Wox8TkAAADSd/v2bYv5OW1tbeXi4pKLEQEAAFhflpNS0p0LpxdeeCG7YwEAAHgiGIahnj17ysHBQZJ08+ZNvfzyy3J2drZot2DBgtwIDwAAwCqynJSaPXv2Peu7d+/+wMEAAAA8CXr06GGxzj/7AADAkyjLSamBAwdarCcnJ+vGjRuyt7dX3rx5SUoBAADcR3h4eG6HAAAAkOuy/PS9K1euWCzXrl3T4cOHVbduXSY6BwAAAAAAQKZkOSmVntKlS+vjjz9OM4oKAAAAAAAASE+2JKWkO5OfnzlzJru6AwAAAAAAwGMsy3NKLVmyxGLdMAydPXtWU6ZMUZ06dbItMAAAAAAAADy+spyUatu2rcW6yWRSgQIF1KhRI02YMCG74gIAAAAAAMBjLMtJqZSUlJyIAwAAAAAAAE+QbJtTCgAAAAAAAMisLI+UGjJkSKbbTpw4MavdAwAAAAAA4AmQ5aTUrl27tGvXLiUnJ6ts2bKSpL/++kt58uRRtWrVzO1MJlP2RQkAAAAAAIDHSpaTUq1atZKrq6tmzZqlfPnySZKuXLmiXr16qV69enr99dezPUgAAAAAAAA8XrI8p9SECRM0duxYc0JKkvLly6cPP/yQp+8BAAAAAAAgU7KclEpISNCFCxfSlF+4cEFXr17NlqAAAAAAAADweMtyUqpdu3bq1auXFixYoFOnTunUqVP66aef1KdPH7Vv3z4nYgQAAAAAAMBjJstzSs2YMUNDhw5Vly5dlJycfKcTW1v16dNHn3zySbYHCAAAAAAAgMdPlpNSefPm1bRp0/TJJ58oOjpaklSyZEk5Oztne3AAAAAAAAB4PGX59r1UZ8+e1dmzZ1W6dGk5OzvLMIzsjAsAAAAAAACPsSwnpS5duqTGjRurTJkyevbZZ3X27FlJUp8+ffT6669ne4AAAAC4t+nTp6ty5cpyc3OTm5ubgoKC9Ouvv5rrb968qbCwMHl5ecnFxUWhoaE6d+5cLkYMAADwAEmpwYMHy87OTjExMcqbN6+5/Pnnn9fy5cuzNTgAAADcX+HChfXxxx9r586d2rFjhxo1aqQ2bdrowIEDku5cvy1dulTz58/X+vXrdebMGR5QAwAAcl2W55RauXKlVqxYocKFC1uUly5dWidOnMi2wAAAAJA5rVq1slgfM2aMpk+fri1btqhw4cKaOXOmIiMj1ahRI0lSeHi4ypcvry1btqhWrVq5ETIAAEDWR0pdv37dYoRUqsuXL8vBwSFbggIAAMCDuX37tubNm6fr168rKChIO3fuVHJyspo0aWJuU65cORUtWlSbN2/OxUgBAMCTLstJqXr16mn27NnmdZPJpJSUFI0fP14NGzbM1uAAAACQOfv27ZOLi4scHBz08ssva+HChapQoYJiY2Nlb28vDw8Pi/Y+Pj6KjY3NsL/ExEQlJCRYLACeLCaTSYsWLXpo+gHw+Mny7Xvjx49X48aNtWPHDiUlJenNN9/UgQMHdPnyZW3cuDEnYgQAAMB9lC1bVrt371Z8fLx+/PFH9ejRQ+vXr3/g/saOHauRI0dmY4QAckKrVq2UnJyc7vy+v//+u+rXr689e/aocuXKWe777NmzypcvX6bbjxgxQosWLdLu3bv/Uz8AnhxZHilVqVIl/fXXX6pbt67atGmj69evq3379tq1a5dKliyZEzECAADgPuzt7VWqVCkFBgZq7NixqlKliiZPnixfX18lJSUpLi7Oov25c+fk6+ubYX/Dhg1TfHy8eTl58mQOHwGAB9GnTx+tWrVKp06dSlMXHh6u6tWrZzkhlZSUJEny9fXNlilasqsfAI+fLCWlkpOT1bhxY50/f17vvvuufvjhB/3yyy/68MMPVbBgwZyKEQAAAFmUkpKixMREBQYGys7OTqtXrzbXHT58WDExMQoKCspwewcHB7m5uVksAB4+LVu2VIECBRQREWFRfu3aNc2fP19t27ZV586dVahQIeXNm1cBAQGaO3euRdvg4GD1799fgwYNUv78+RUSEiIp7W13b731lsqUKaO8efOqRIkSeu+995ScnCxJioiI0MiRI7Vnzx6ZTCaZTCZzTP/uZ9++fWrUqJGcnJzk5eWlfv366dq1a+b6nj17qm3btvr0009VsGBBeXl5KSwszLwvAI+PLCWl7OzstHfv3pyKBQAAAA9g2LBh+u2333T8+HHt27dPw4YN07p169S1a1e5u7urT58+GjJkiNauXaudO3eqV69eCgoK4sl7wGPA1tZW3bt3V0REhAzDMJfPnz9ft2/f1gsvvKDAwEAtW7ZM+/fvV79+/dStWzdt27bNop9Zs2bJ3t5eGzdu1IwZM9Ldl6urqyIiInTw4EFNnjxZX3/9tSZNmiRJev755/X666+rYsWKOnv2rM6ePavnn38+TR/Xr19XSEiI8uXLp+3bt2v+/PmKiopS//79LdqtXbtW0dHRWrt2rWbNmqWIiAiLxNuIESNUrFixBzxrAB4WWb5974UXXtDMmTNzIhYAAAA8gPPnz6t79+4qW7asGjdurO3bt2vFihV65plnJEmTJk1Sy5YtFRoaqvr168vX11cLFizI5agBZJfevXsrOjraYh658PBwhYaGyt/fX0OHDtVTTz2lEiVKaMCAAWrWrJl++OEHiz5Kly6t8ePHq2zZsipbtmy6+xk+fLhq166tYsWKqVWrVho6dKi5HycnJ7m4uMjW1la+vr7y9fWVk5NTmj4iIyN18+ZNzZ49W5UqVVKjRo00ZcoUffvttzp37py5Xb58+TRlyhSVK1dOLVu2VIsWLSxGfObPn5/pY4DHQJYnOr9165a++eYbRUVFKTAwUM7Ozhb1EydOzLbgAAAAcH/3+4eho6Ojpk6dqqlTp1opIgDWVK5cOdWuXVvffPONgoODdfToUf3+++8aNWqUbt++rY8++kg//PCDTp8+raSkJCUmJipv3rwWfQQGBt53P99//70+//xzRUdH69q1a7p161aWb+09dOiQqlSpYvE9sk6dOkpJSdHhw4fl4+MjSapYsaLy5MljblOwYEHt27fPvN6/f/80o6sAPHqynJTav3+/qlWrJkn666+/LOpMJlP2RAUAAAAAyLQ+ffpowIABmjp1qsLDw1WyZEk1aNBA48aN0+TJk/XZZ58pICBAzs7OGjRokHky81T/Hmzwb5s3b1bXrl01cuRIhYSEyN3dXfPmzdOECRNy5Hjs7Ows1k0mk1JSUnJkXwByT6aTUn///beKFy+utWvX5mQ8AAAAAIAs6tixowYOHKjIyEjNnj1br7zyikwmkzZu3Kg2bdrohRdekHTnIQh//fWXKlSokKX+N23aJH9/f7377rvmshMnTli0sbe31+3bt+/ZT/ny5RUREaHr16+bE2EbN26UjY1NhrcNAnh8ZXpOqdKlS+vChQvm9eeff97inl8AAAAAQO5wcXHR888/r2HDhuns2bPq2bOnpDvf41atWqVNmzbp0KFDeumllx7oe1zp0qUVExOjefPmKTo6Wp9//rkWLlxo0aZYsWI6duyYdu/erYsXLyoxMTFNP127dpWjo6N69Oih/fv3a+3atRowYIC6detmvnUvM6ZMmaLGjRtn+TgAPFwynZS6+0kOkvTLL7/o+vXr2R4QAAAAACDr+vTpoytXrigkJER+fn6S7kxOXq1aNYWEhCg4OFi+vr5q27Ztlvtu3bq1Bg8erP79++upp57Spk2b9N5771m0CQ0NVbNmzdSwYUMVKFBAc+fOTdNP3rx5tWLFCl2+fFk1atTQc889p8aNG2vKlClZiufixYuKjo7O8nEAeLiYjH9nmzJgY2Oj2NhYeXt7S7rzONA9e/aoRIkSORqgNSQkJMjd3V3x8fFZnqgvs97+JVOnGU+oj599OOZjazv1j9wOAQ+5RWHVcjsESVKR4Gm5HQIecifXvZpjfVvjuuFh9KQeNwA8qnYy5zPuIzBz6aAHktnrhkyPlDKZTGkmMmdicwAAAAAAADyITE90bhiGevbsKQcHB0nSzZs39fLLL6d5SsOCBQuyN0IAAAAAAAA8djKdlOrRo4fFeurTGwAAAAAAAICsynRSKjw8PCfjAAAAAAAAwBMk03NKAQAAAAAAANmFpBQAAAAAPGGKFStmfpjV3UtYWJikO3MIh4WFycvLSy4uLgoNDdW5c+cy7C85OVlvvfWWAgIC5OzsLD8/P3Xv3l1nzpwxt1m3bl26+zSZTNq+fXuOHzOAhw9JKQAAAAB4wmzfvl1nz541L6tWrZIkdejQQZI0ePBgLV26VPPnz9f69et15swZtW/fPsP+bty4oT/++EPvvfee/vjjDy1YsECHDx9W69atzW1q165tsc+zZ8/qxRdfVPHixVW9evWcPWAAD6VcTUr99ttvatWqlfz8/GQymbRo0SKLesMw9P7776tgwYJycnJSkyZNdOTIEYs2ly9fVteuXeXm5iYPDw/16dNH165ds+JRAAAAAMCjpUCBAvL19TUvP//8s0qWLKkGDRooPj5eM2fO1MSJE9WoUSMFBgYqPDxcmzZt0pYtW9Ltz93dXatWrVLHjh1VtmxZ1apVS1OmTNHOnTsVExMjSbK3t7fYp5eXlxYvXqxevXrJZDJZ8/ABPCRyNSl1/fp1ValSRVOnTk23fvz48fr88881Y8YMbd26Vc7OzgoJCdHNmzfNbbp27aoDBw5o1apV+vnnn/Xbb7+pX79+1joEAAAAAHikJSUl6bvvvlPv3r1lMpm0c+dOJScnq0mTJuY25cqVU9GiRbV58+ZM9xsfHy+TySQPD49065csWaJLly6pV69e//UQADyiMv30vZzQvHlzNW/ePN06wzD02Wefafjw4WrTpo0kafbs2fLx8dGiRYvUqVMnHTp0SMuXL9f27dvNwz2/+OILPfvss/r000/l5+dntWMBAAAAgEfRokWLFBcXp549e0qSYmNjZW9vnyaZ5OPjo9jY2Ez1efPmTb311lvq3Lmz3Nzc0m0zc+ZMhYSEqHDhwv8lfACPsId2Tqljx44pNjbWIjvv7u6umjVrmrPzmzdvloeHh8X9x02aNJGNjY22bt2aYd+JiYlKSEiwWAAAAADgSTRz5kw1b9482/6pn5ycrI4dO8owDE2fPj3dNqdOndKKFSvUp0+fbNkngEfTQ5uUSs3A+/j4WJTfnZ2PjY2Vt7e3Rb2tra08PT3vmcEfO3as3N3dzUuRIkWyOXoAAAAAePidOHFCUVFRevHFF81lvr6+SkpKUlxcnEXbc+fOydfX9579pSakTpw4oVWrVmU4Sio8PFxeXl4WE6EDePI8tEmpnDRs2DDFx8ebl5MnT+Z2SAAAAABgdeHh4fL29laLFi3MZYGBgbKzs9Pq1avNZYcPH1ZMTIyCgoIy7Cs1IXXkyBFFRUXJy8sr3XaGYSg8PFzdu3eXnZ1d9h0MgEfOQ5uUSs3Anzt3zqL87uy8r6+vzp8/b1F/69YtXb58+Z4ZfAcHB7m5uVksAAAAAPAkSUlJUXh4uHr06CFb2/+bbtjd3V19+vTRkCFDtHbtWu3cuVO9evVSUFCQatWqZW5Xrlw5LVy4UNKdhNRzzz2nHTt2aM6cObp9+7ZiY2MVGxurpKQki/2uWbNGx44dsxidBeDJ9NAmpYoXLy5fX1+L7HxCQoK2bt1qzs4HBQUpLi5OO3fuNLdZs2aNUlJSVLNmTavHDAAAAACPiqioKMXExKh3795p6iZNmqSWLVsqNDRU9evXl6+vrxYsWGDR5vDhw4qPj5cknT59WkuWLNGpU6f01FNPqWDBguZl06ZNFtvNnDlTtWvXVrly5XLu4AA8EnL16XvXrl3T0aNHzevHjh3T7t275enpqaJFi2rQoEH68MMPVbp0aRUvXlzvvfee/Pz81LZtW0lS+fLl1axZM/Xt21czZsxQcnKy+vfvr06dOvHkPQAAAAC4h6ZNm8owjHTrHB0dNXXqVE2dOjXD7e/etlixYhn29W+RkZFZCxTAYytXk1I7duxQw4YNzetDhgyRJPXo0UMRERF68803df36dfXr109xcXGqW7euli9fLkdHR/M2c+bMUf/+/dW4cWPZ2NgoNDRUn3/+udWPBQAAAAAAAJmXq0mp4ODge2bTTSaTRo0apVGjRmXYxtPTk0w7AAAAAADAI+ahnVMKAAAAAAAAjy+SUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAAAAAsDqSUgAAAAAAALA6klIAAAAAAACwOpJSAAAAj7ixY8eqRo0acnV1lbe3t9q2bavDhw9btLl586bCwsLk5eUlFxcXhYaG6ty5c7kUMQAAAEkpAACAR9769esVFhamLVu2aNWqVUpOTlbTpk11/fp1c5vBgwdr6dKlmj9/vtavX68zZ86offv2uRg1AAB40tnmdgAAAAD4b5YvX26xHhERIW9vb+3cuVP169dXfHy8Zs6cqcjISDVq1EiSFB4ervLly2vLli2qVatWboQNAACecIyUAgAAeMzEx8dLkjw9PSVJO3fuVHJyspo0aWJuU65cORUtWlSbN2/OlRgBAAAYKQUAAPAYSUlJ0aBBg1SnTh1VqlRJkhQbGyt7e3t5eHhYtPXx8VFsbGy6/SQmJioxMdG8npCQkGMxAwCAJxNJKQAAgMdIWFiY9u/frw0bNvynfsaOHauRI0dmU1TAY6Z169yOAA+zJUtyOwLgkcHtewAAAI+J/v376+eff9batWtVuHBhc7mvr6+SkpIUFxdn0f7cuXPy9fVNt69hw4YpPj7evJw8eTInQwcAAE8gklIAAACPOMMw1L9/fy1cuFBr1qxR8eLFLeoDAwNlZ2en1atXm8sOHz6smJgYBQUFpdung4OD3NzcLBYAAIDsxO17AAAAj7iwsDBFRkZq8eLFcnV1Nc8T5e7uLicnJ7m7u6tPnz4aMmSIPD095ebmpgEDBigoKIgn7wEAgFxDUgoAAOARN336dElScHCwRXl4eLh69uwpSZo0aZJsbGwUGhqqxMREhYSEaNq0aVaOFAAA4P+QlAIAAHjEGYZx3zaOjo6aOnWqpk6daoWIAAAA7o85pQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUkpQAAAAAAAGB1JKUAAAAAAABgdSSlAAAAAAAAYHUPdVJqxIgRMplMFku5cuXM9Tdv3lRYWJi8vLzk4uKi0NBQnTt3LhcjBgAAAAAAQGY81EkpSapYsaLOnj1rXjZs2GCuGzx4sJYuXar58+dr/fr1OnPmjNq3b5+L0QIAAAAAACAzbHM7gPuxtbWVr69vmvL4+HjNnDlTkZGRatSokSQpPDxc5cuX15YtW1SrVi1rhwoAAAAAAIBMeuhHSh05ckR+fn4qUaKEunbtqpiYGEnSzp07lZycrCZNmpjblitXTkWLFtXmzZvv2WdiYqISEhIsFgAAAAAAAFjPQ52UqlmzpiIiIrR8+XJNnz5dx44dU7169XT16lXFxsbK3t5eHh4eFtv4+PgoNjb2nv2OHTtW7u7u5qVIkSI5eBQAAAAAAAD4t4f69r3mzZubf65cubJq1qwpf39//fDDD3JycnrgfocNG6YhQ4aY1xMSEkhMAQAAAAAAWNFDPVLq3zw8PFSmTBkdPXpUvr6+SkpKUlxcnEWbc+fOpTsH1d0cHBzk5uZmsQAAAAAAAOD/tXfnsVGV+x/HP0PBaYFOWR0oFGhkE1mUzRQRUKplsSkEkN2yCAKCQOWHYAQUlSoINCKCYBGbgGAUMVjBQAWRRRRENIplESxbKXuZAi21z+8Pr3PvWJZ6b3tm6fuVTNLznGW+M3l6+s1nTs9Yx69CKZfLpcOHD6tmzZpq1aqVypUrp7S0NPf69PR0ZWRkKCoqyotVAgAAAAAA4HZ8+t/3Jk2apNjYWNWtW1cnT57UjBkzFBQUpP79+yssLEzDhw9XQkKCqlSpIofDoXHjxikqKopv3gMAAAAAAPBxPh1KHT9+XP3799e5c+dUvXp1tW/fXt98842qV68uSZo/f77KlCmjXr16KTc3VzExMXr77be9XDUAAAAAAABux6dDqVWrVt1yfXBwsBYuXKiFCxdaVBEAAAAAAACKg1/dUwoAAAAAAACBgVAKAAAAAAAAliOUAgAAAAAAgOUIpQAAAAAAAGA5QikAAAA/t3XrVsXGxio8PFw2m01r1671WG+M0fTp01WzZk2FhIQoOjpaBw8e9E6xAAAA/0IoBQAA4OdycnLUokWLm34j8ezZs/Xmm29q8eLF2rVrlypUqKCYmBhdu3bN4koBAAD+ray3CwAAAMD/pmvXruratesN1xljlJSUpBdeeEFxcXGSpJSUFDmdTq1du1b9+vWzslQAAAA3rpQCAAAIYEeOHFFmZqaio6PdY2FhYbr//vu1c+fOm+6Xm5ur7OxsjwcAAEBxIpQCAAAIYJmZmZIkp9PpMe50Ot3rbiQxMVFhYWHuR0RERInWCQAASh9CKQAAABQydepUXbp0yf04duyYt0sCAAABhlAKAAAggNWoUUOSdPr0aY/x06dPu9fdiN1ul8Ph8HgAAAAUJ0IpAACAABYZGakaNWooLS3NPZadna1du3YpKirKi5UBAIDSjm/fAwAA8HMul0uHDh1yLx85ckQ//PCDqlSpojp16mjChAl65ZVX1KBBA0VGRmratGkKDw9Xjx49vFc0AAAo9QilAAAA/Nzu3bv10EMPuZcTEhIkSfHx8Vq+fLkmT56snJwcjRw5UhcvXlT79u21YcMGBQcHe6tkAAAAQikAAAB/16lTJxljbrreZrNp5syZmjlzpoVVAQAA3Br3lAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlCKUAAAAAAABgOUIpAAAAAAAAWI5QCgAAAAAAAJYjlAIAAAAAAIDlynq7gOKycOFCzZkzR5mZmWrRooUWLFigtm3berssAAAAn+Hz/dKSJd6uAL5s5EhvVwAAKGYBcaXU6tWrlZCQoBkzZuj7779XixYtFBMTo6ysLG+XBgAA4BPolwAAgK8JiFBq3rx5GjFihIYOHaomTZpo8eLFKl++vJYtW+bt0gAAAHwC/RIAAPA1fv/ve3l5edqzZ4+mTp3qHitTpoyio6O1c+fOG+6Tm5ur3Nxc9/KlS5ckSdnZ2SVWZ+4VU2LHhv/LzrZ5uwRJ0vWrLm+XAB9XkufJf6Ig/6q3S4CPK8m5+texjfGfv+3+0i/pKr/buAUf+RskSbp+3dsVwJf5yFyls8ft+EK/5Peh1NmzZ/XHH3/I6XR6jDudTv3666833CcxMVEvvfRSofGIiIgSqRG4nSRvFwAUUdj/ebsCoGjCwiaV+HNcvnxZYWFhJf48xYF+CQFhwgRvVwAUjZ/8bQCsmKu365f8PpT6b0ydOlUJCQnu5YKCAp0/f15Vq1aVzeYbV6wEsuzsbEVEROjYsWNyOBzeLge4KeYq/AVz1VrGGF2+fFnh4eHeLqVE0S95F7/X8BfMVfgL5qq1itov+X0oVa1aNQUFBen06dMe46dPn1aNGjVuuI/dbpfdbvcYq1SpUkmViJtwOBycDOAXmKvwF8xV6/jLFVJ/oV/yX/xew18wV+EvmKvWKUq/5Pc3Or/jjjvUqlUrpaWluccKCgqUlpamqKgoL1YGAADgG+iXAACAL/L7K6UkKSEhQfHx8WrdurXatm2rpKQk5eTkaOjQod4uDQAAwCfQLwEAAF8TEKFU3759debMGU2fPl2ZmZm69957tWHDhkI384RvsNvtmjFjRqF/CQB8DXMV/oK5iqKgX/Iv/F7DXzBX4S+Yq77JZvzp+4wBAAAAAAAQEPz+nlIAAAAAAADwP4RSAAAAAAAAsByhFAAAAAAAACxHKAWv6NSpkyZMmODtMgAAAHwW/RIAINARSqHYDBkyRDabTaNGjSq07umnn5bNZtOQIUMkSWvWrNHLL79scYUIJDt37lRQUJC6d+/u7VKAYvdPzqcA/Av9EqxEv4RARr8UGAilUKwiIiK0atUqXb161T127do1rVy5UnXq1HGPValSRaGhod4oEQEiOTlZ48aN09atW3Xy5MkSex5jjPLz80vs+MDNFPV8CsD/0C/BKvRLCHT0S/6PUArFqmXLloqIiNCaNWvcY2vWrFGdOnV03333ucf+fjl6vXr1NGvWLA0bNkyhoaGqU6eOlixZYmXp8CMul0urV6/W6NGj1b17dy1fvlySNGDAAPXt29dj2+vXr6tatWpKSUmRJBUUFCgxMVGRkZEKCQlRixYt9NFHH7m337Jli2w2m9avX69WrVrJbrdr27ZtOnz4sOLi4uR0OlWxYkW1adNGmzZt8niuU6dOqXv37goJCVFkZKRWrlypevXqKSkpyb3NxYsX9eSTT6p69epyOBx6+OGHtW/fvpJ5o+DXino+3bBhg9q3b69KlSqpatWqeuyxx3T48GH3+pSUFFWsWFEHDx50j40ZM0aNGzfWlStXrHkxADzQL8EK9EsoDeiX/B+hFIrdsGHD9N5777mXly1bpqFDh952v7lz56p169bau3evxowZo9GjRys9Pb0kS4Wf+vDDD9W4cWM1atRIgwYN0rJly2SM0cCBA7Vu3Tq5XC73tl988YWuXLminj17SpISExOVkpKixYsX6+eff9bEiRM1aNAgffXVVx7PMWXKFL322mvav3+/mjdvLpfLpW7duiktLU179+5Vly5dFBsbq4yMDPc+TzzxhE6ePKktW7bo448/1pIlS5SVleVx3D59+igrK0vr16/Xnj171LJlS3Xu3Fnnz58vwXcM/qoo59OcnBwlJCRo9+7dSktLU5kyZdSzZ08VFBRI+nNeduvWTQMHDlR+fr5SU1P17rvvasWKFSpfvrylrwfAv9EvoaTRL6G0oF/ycwYoJvHx8SYuLs5kZWUZu91ujh49ao4ePWqCg4PNmTNnTFxcnImPjzfGGNOxY0czfvx4975169Y1gwYNci8XFBSYO++80yxatMjiVwF/0K5dO5OUlGSMMeb69eumWrVqZvPmze6fU1JS3Nv279/f9O3b1xhjzLVr10z58uXNjh07PI43fPhw079/f2OMMZs3bzaSzNq1a29bxz333GMWLFhgjDFm//79RpL57rvv3OsPHjxoJJn58+cbY4z5+uuvjcPhMNeuXfM4zl133WXeeeedf/guIJD9k/Pp3505c8ZIMj/99JN77Pz586Z27dpm9OjRxul0mldffdWiVwLg7+iXYBX6JQQ6+qXAUNZ7cRgCVfXq1d2XCBtj1L17d1WrVu22+zVv3tz9s81mU40aNQp9agKkp6fr22+/1SeffCJJKlu2rPr27avk5GR16tRJjz/+uFasWKHBgwcrJydHn376qVatWiVJOnTokK5cuaJHHnnE45h5eXkel/dKUuvWrT2WXS6XXnzxRaWmpurUqVPKz8/X1atX3Z/8paenq2zZsmrZsqV7n/r166ty5cru5X379snlcqlq1aoex7569arH5cPAX4pyPj148KCmT5+uXbt26ezZs+5P/DIyMtS0aVNJUuXKlZWcnKyYmBi1a9dOU6ZMsfy1APBEv4SSRL+E0oR+yb8RSqFEDBs2TGPHjpUkLVy4sEj7lCtXzmPZZrO5TxbAX5KTk5Wfn6/w8HD3mDFGdrtdb731lgYOHKiOHTsqKytLGzduVEhIiLp06SJJ7svUU1NTVatWLY/j2u12j+UKFSp4LE+aNEkbN27UG2+8ofr16yskJES9e/dWXl5ekWt3uVyqWbOmtmzZUmhdpUqVinwclC63O5/Gxsaqbt26Wrp0qcLDw1VQUKCmTZsWmptbt25VUFCQTp06pZycHG6eDPgA+iWUFPollDb0S/6LUAolokuXLsrLy5PNZlNMTIy3y0GAyM/PV0pKiubOnatHH33UY12PHj30wQcfaNSoUYqIiNDq1au1fv169enTx93AN2nSRHa7XRkZGerYseM/eu7t27dryJAh7nstuFwuHT161L2+UaNGys/P1969e9WqVStJf37SeOHCBfc2LVu2VGZmpsqWLat69er9F+8ASqNbnU/PnTun9PR0LV26VA8++KAkadu2bYWOsWPHDr3++utat26dnnvuOY0dO1bvv/++JfUDuDn6JZQE+iWURvRL/otQCiUiKChI+/fvd/8MFIfPPvtMFy5c0PDhwxUWFuaxrlevXkpOTtaoUaM0YMAALV68WAcOHNDmzZvd24SGhmrSpEmaOHGiCgoK1L59e126dEnbt2+Xw+FQfHz8TZ+7QYMGWrNmjWJjY2Wz2TRt2jSPT6YbN26s6OhojRw5UosWLVK5cuX07LPPKiQkRDabTZIUHR2tqKgo9ejRQ7Nnz1bDhg118uRJpaamqmfPnoUugQekW59PK1eurKpVq2rJkiWqWbOmMjIyCl1qfvnyZQ0ePFjPPPOMunbtqtq1a6tNmzaKjY1V7969LXsdAAqjX0JJoF9CaUS/5L/49j2UGIfDIYfD4e0yEECSk5MVHR1dqMGS/myydu/erR9//FEDBw7UL7/8olq1aumBBx7w2O7ll1/WtGnTlJiYqLvvvltdunRRamqqIiMjb/nc8+bNU+XKldWuXTvFxsYqJibG434I0p9fJet0OtWhQwf17NlTI0aMUGhoqIKDgyX9+S8Wn3/+uTp06KChQ4eqYcOG6tevn37//Xc5nc7/8d1BILvZ+bRMmTJatWqV9uzZo6ZNm2rixImaM2eOxzbjx49XhQoVNGvWLElSs2bNNGvWLD311FM6ceKEJfUDuDn6JRQ3+iWUVvRL/slmjDHeLgIAAtHx48cVERGhTZs2qXPnzt4uBwAAwOfQLwGlG6EUABSTL7/8Ui6XS82aNdOpU6c0efJknThxQgcOHCh0Y1oAAIDSiH4JwH/inlIAUEyuX7+u559/Xr/99ptCQ0PVrl07rVixggYLAADgX+iXAPwnrpQCAAAAAACA5bjROQAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACxHKAUAAAAAAADLEUoBAAAAAADAcoRSAAAAAAAAsByhFAAAAAAAACz3/8kyAgWoiibyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "2YNlXm14fjKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-speech"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8KwcZNtmzN2",
        "outputId": "202075e2-17df-41ea-ef97-8dac2494232c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-speech in /usr/local/lib/python3.11/dist-packages (2.32.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.24.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-speech) (5.29.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.69.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-speech) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-speech) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Google STT"
      ],
      "metadata": {
        "id": "QE_d74UIwAUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "from google.cloud import speech\n",
        "import os\n",
        "import math\n",
        "import tempfile\n",
        "\n",
        "def transcribe_bilingual_audio_to_file(audio_path, output_path, chunk_sec=30):\n",
        "    audio = AudioSegment.from_file(audio_path)\n",
        "    audio = audio.set_channels(1)  # Convert to mono\n",
        "    chunk_ms = chunk_sec * 1000  # in milliseconds\n",
        "    total_ms = len(audio)\n",
        "    n_chunks = math.ceil(total_ms / chunk_ms)\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "    client = speech.SpeechClient()\n",
        "\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        for i in range(n_chunks):\n",
        "            start_ms = i * chunk_ms\n",
        "            end_ms   = min((i + 1) * chunk_ms, total_ms)\n",
        "            chunk = audio[start_ms:end_ms]\n",
        "\n",
        "            # Export chunk to a temporary WAV file (Linear16)\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_wav:\n",
        "                chunk.export(tmp_wav.name, format=\"wav\")\n",
        "                tmp_path = tmp_wav.name\n",
        "\n",
        "            with open(tmp_path, \"rb\") as wav_f:\n",
        "                wav_bytes = wav_f.read()\n",
        "\n",
        "            audio_config = speech.RecognitionConfig(\n",
        "                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "                sample_rate_hertz=48000,\n",
        "                language_code=\"en-US\",\n",
        "                alternative_language_codes=[\"hi-IN\"],\n",
        "                enable_automatic_punctuation=True\n",
        "            )\n",
        "            audio_req = speech.RecognitionAudio(content=wav_bytes)\n",
        "\n",
        "\n",
        "            print(f\"Transcribing chunk {i+1}/{n_chunks} ({start_ms/1000:.0f}s–{end_ms/1000:.0f}s)...\")\n",
        "            response = client.recognize(config=audio_config, audio=audio_req)\n",
        "            for result in response.results:\n",
        "                out_f.write(result.alternatives[0].transcript.strip() + \"\\n\")\n",
        "            os.remove(tmp_path)\n",
        "\n",
        "    print(f\"✅ Full transcript saved to {output_path}\")\n",
        "\n",
        "transcribe_bilingual_audio_to_file(\n",
        "    audio_path=\"/content/drive/MyDrive/audio.mp3\",\n",
        "    output_path=\"/content/drive/MyDrive/base_transcript.txt\",\n",
        "    chunk_sec=30  # 30s chunks to stay well under the 10 MB limit\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtgaL_wRmzFa",
        "outputId": "ab2d44dd-73c9-45b1-d006-5cf069a450db"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing chunk 1/81 (0s–30s)...\n",
            "Transcribing chunk 2/81 (30s–60s)...\n",
            "Transcribing chunk 3/81 (60s–90s)...\n",
            "Transcribing chunk 4/81 (90s–120s)...\n",
            "Transcribing chunk 5/81 (120s–150s)...\n",
            "Transcribing chunk 6/81 (150s–180s)...\n",
            "Transcribing chunk 7/81 (180s–210s)...\n",
            "Transcribing chunk 8/81 (210s–240s)...\n",
            "Transcribing chunk 9/81 (240s–270s)...\n",
            "Transcribing chunk 10/81 (270s–300s)...\n",
            "Transcribing chunk 11/81 (300s–330s)...\n",
            "Transcribing chunk 12/81 (330s–360s)...\n",
            "Transcribing chunk 13/81 (360s–390s)...\n",
            "Transcribing chunk 14/81 (390s–420s)...\n",
            "Transcribing chunk 15/81 (420s–450s)...\n",
            "Transcribing chunk 16/81 (450s–480s)...\n",
            "Transcribing chunk 17/81 (480s–510s)...\n",
            "Transcribing chunk 18/81 (510s–540s)...\n",
            "Transcribing chunk 19/81 (540s–570s)...\n",
            "Transcribing chunk 20/81 (570s–600s)...\n",
            "Transcribing chunk 21/81 (600s–630s)...\n",
            "Transcribing chunk 22/81 (630s–660s)...\n",
            "Transcribing chunk 23/81 (660s–690s)...\n",
            "Transcribing chunk 24/81 (690s–720s)...\n",
            "Transcribing chunk 25/81 (720s–750s)...\n",
            "Transcribing chunk 26/81 (750s–780s)...\n",
            "Transcribing chunk 27/81 (780s–810s)...\n",
            "Transcribing chunk 28/81 (810s–840s)...\n",
            "Transcribing chunk 29/81 (840s–870s)...\n",
            "Transcribing chunk 30/81 (870s–900s)...\n",
            "Transcribing chunk 31/81 (900s–930s)...\n",
            "Transcribing chunk 32/81 (930s–960s)...\n",
            "Transcribing chunk 33/81 (960s–990s)...\n",
            "Transcribing chunk 34/81 (990s–1020s)...\n",
            "Transcribing chunk 35/81 (1020s–1050s)...\n",
            "Transcribing chunk 36/81 (1050s–1080s)...\n",
            "Transcribing chunk 37/81 (1080s–1110s)...\n",
            "Transcribing chunk 38/81 (1110s–1140s)...\n",
            "Transcribing chunk 39/81 (1140s–1170s)...\n",
            "Transcribing chunk 40/81 (1170s–1200s)...\n",
            "Transcribing chunk 41/81 (1200s–1230s)...\n",
            "Transcribing chunk 42/81 (1230s–1260s)...\n",
            "Transcribing chunk 43/81 (1260s–1290s)...\n",
            "Transcribing chunk 44/81 (1290s–1320s)...\n",
            "Transcribing chunk 45/81 (1320s–1350s)...\n",
            "Transcribing chunk 46/81 (1350s–1380s)...\n",
            "Transcribing chunk 47/81 (1380s–1410s)...\n",
            "Transcribing chunk 48/81 (1410s–1440s)...\n",
            "Transcribing chunk 49/81 (1440s–1470s)...\n",
            "Transcribing chunk 50/81 (1470s–1500s)...\n",
            "Transcribing chunk 51/81 (1500s–1530s)...\n",
            "Transcribing chunk 52/81 (1530s–1560s)...\n",
            "Transcribing chunk 53/81 (1560s–1590s)...\n",
            "Transcribing chunk 54/81 (1590s–1620s)...\n",
            "Transcribing chunk 55/81 (1620s–1650s)...\n",
            "Transcribing chunk 56/81 (1650s–1680s)...\n",
            "Transcribing chunk 57/81 (1680s–1710s)...\n",
            "Transcribing chunk 58/81 (1710s–1740s)...\n",
            "Transcribing chunk 59/81 (1740s–1770s)...\n",
            "Transcribing chunk 60/81 (1770s–1800s)...\n",
            "Transcribing chunk 61/81 (1800s–1830s)...\n",
            "Transcribing chunk 62/81 (1830s–1860s)...\n",
            "Transcribing chunk 63/81 (1860s–1890s)...\n",
            "Transcribing chunk 64/81 (1890s–1920s)...\n",
            "Transcribing chunk 65/81 (1920s–1950s)...\n",
            "Transcribing chunk 66/81 (1950s–1980s)...\n",
            "Transcribing chunk 67/81 (1980s–2010s)...\n",
            "Transcribing chunk 68/81 (2010s–2040s)...\n",
            "Transcribing chunk 69/81 (2040s–2070s)...\n",
            "Transcribing chunk 70/81 (2070s–2100s)...\n",
            "Transcribing chunk 71/81 (2100s–2130s)...\n",
            "Transcribing chunk 72/81 (2130s–2160s)...\n",
            "Transcribing chunk 73/81 (2160s–2190s)...\n",
            "Transcribing chunk 74/81 (2190s–2220s)...\n",
            "Transcribing chunk 75/81 (2220s–2250s)...\n",
            "Transcribing chunk 76/81 (2250s–2280s)...\n",
            "Transcribing chunk 77/81 (2280s–2310s)...\n",
            "Transcribing chunk 78/81 (2310s–2340s)...\n",
            "Transcribing chunk 79/81 (2340s–2370s)...\n",
            "Transcribing chunk 80/81 (2370s–2400s)...\n",
            "Transcribing chunk 81/81 (2400s–2400s)...\n",
            "✅ Full transcript saved to /content/drive/MyDrive/base_transcript.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def remove_filler_words_from_file(file_path, output_path):\n",
        "    english_fillers = r'\\b(um|uh|er|like|you know|so|basically|actually|literally|Um|Uh|So|Umm|Uhh|Like|Eh)\\b'\n",
        "    hindi_fillers = r'\\b(मतलब|ऐसे|वो|तो|ना|हां|अच्छा)\\b'\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        transcript = file.read()\n",
        "\n",
        "    cleaned_transcript = re.sub(english_fillers, '', transcript, flags=re.IGNORECASE)\n",
        "    cleaned_transcript = re.sub(hindi_fillers, '', cleaned_transcript)\n",
        "    cleaned_transcript = re.sub(r'\\s+', ' ', cleaned_transcript).strip()\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(cleaned_transcript)\n",
        "\n",
        "    print(f\"Filler words removed. Cleaned transcript saved to {output_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0H6hP1Pw1kUx"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_filler_words_from_file(\n",
        "    file_path=\"/content/drive/MyDrive/base_transcript.txt\",  # original transcript file\n",
        "    output_path=\"/content/drive/MyDrive/cleaned_base_transcript.txt\"  #  cleaned transcript\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2R3k--_1LOe",
        "outputId": "a0e10395-9e65-44e2-e318-bf16c53a7549"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filler words removed. Cleaned transcript saved to /content/drive/MyDrive/cleaned_base_transcript.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pesq pystoi jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAk_Iu9d1Mq0",
        "outputId": "adcb423b-7fe3-44e0-8b6e-7da30cef1a97"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pesq\n",
            "  Downloading pesq-0.0.4.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pystoi\n",
            "  Downloading pystoi-0.4.1-py2.py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pystoi) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pystoi) (1.14.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading pystoi-0.4.1-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pesq\n",
            "  Building wheel for pesq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pesq: filename=pesq-0.0.4-cp311-cp311-linux_x86_64.whl size=274949 sha256=1425ceb5f2a3cabca17e4f4de008b083b25b1d4ee31453f104efc6e376e167ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/f1/23/2698d0bf31eec2b2aa50623b5d93b6206c49c7155d0e31345d\n",
            "Successfully built pesq\n",
            "Installing collected packages: pesq, rapidfuzz, pystoi, jiwer\n",
            "Successfully installed jiwer-3.1.0 pesq-0.0.4 pystoi-0.4.1 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQG2zP1Z5CyH",
        "outputId": "ea0a556f-3dc3-4973-8b71-f2d415feba0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pesq import pesq\n",
        "from scipy.io import wavfile\n",
        "\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def convert_mp3_to_wav(input_path, output_path, target_sample_rate=16000):\n",
        "    audio = AudioSegment.from_mp3(input_path)\n",
        "    audio = audio.set_frame_rate(target_sample_rate).set_channels(1)  # mono, 16kHz\n",
        "    audio.export(output_path, format=\"wav\")\n",
        "\n",
        "# Example usage\n",
        "convert_mp3_to_wav(\"/content/temp_bengali_tts.mp3\", \"original.wav\")\n",
        "convert_mp3_to_wav(\"/content/bengali_with_my_voice.mp3\", \"generated.wav\")\n"
      ],
      "metadata": {
        "id": "h3lZZrdi2aXZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "from scipy.io import wavfile\n",
        "from pesq import pesq\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def convert_mp3_to_wav(input_path, output_path, target_sample_rate=16000):\n",
        "    audio = AudioSegment.from_mp3(input_path)\n",
        "    audio = audio.set_frame_rate(target_sample_rate).set_channels(1)\n",
        "    audio.export(output_path, format=\"wav\")\n",
        "\n",
        "def clip_to_10_minutes(audio_path, output_path, max_duration_ms=600000):\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    clipped = audio[:max_duration_ms]\n",
        "    clipped.export(output_path, format=\"wav\")\n",
        "\n",
        "def split_audio(audio_path, chunk_length_ms=50000):\n",
        "    audio = AudioSegment.from_wav(audio_path)\n",
        "    chunks = []\n",
        "    for i in range(0, len(audio), chunk_length_ms):\n",
        "        chunk = audio[i:i + chunk_length_ms]\n",
        "        chunk_path = f\"{audio_path}_chunk_{i//chunk_length_ms}.wav\"\n",
        "        chunk.export(chunk_path, format=\"wav\")\n",
        "        chunks.append(chunk_path)\n",
        "    return chunks\n",
        "\n",
        "def compute_pesq_on_chunks(ref_chunks, deg_chunks):\n",
        "    pesq_scores = []\n",
        "    for ref_path, deg_path in zip(ref_chunks, deg_chunks):\n",
        "        rate_ref, ref = wavfile.read(ref_path)\n",
        "        rate_deg, deg = wavfile.read(deg_path)\n",
        "        if rate_ref != 16000 or rate_deg != 16000:\n",
        "            print(f\"Skipping due to wrong sampling rate: {ref_path}\")\n",
        "            continue\n",
        "        try:\n",
        "            score = pesq(rate_ref, ref, deg, 'wb')\n",
        "            pesq_scores.append(score)\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chunk {ref_path}: {e}\")\n",
        "    return np.mean(pesq_scores) if pesq_scores else None\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rzuRW_sp5RAd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PESQ scores between gTTS Bengali speech and transformed (to my voice) Bengali speech"
      ],
      "metadata": {
        "id": "yRLddahm9SYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_mp3_to_wav(\"/content/temp_bengali_tts.mp3\", \"original_full.wav\")\n",
        "convert_mp3_to_wav(\"/content/bengali_with_my_voice.mp3\", \"generated_full.wav\")"
      ],
      "metadata": {
        "id": "Yn3pCEJx5pDO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_to_10_minutes(\"original_full.wav\", \"original.wav\")\n",
        "clip_to_10_minutes(\"generated_full.wav\", \"generated.wav\")"
      ],
      "metadata": {
        "id": "hsfHSu507RWx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_chunks = split_audio(\"original.wav\")\n",
        "generated_chunks = split_audio(\"generated.wav\")"
      ],
      "metadata": {
        "id": "57rZoUtc9d6B"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = min(len(original_chunks), len(generated_chunks))\n",
        "avg_pesq = compute_pesq_on_chunks(original_chunks[:min_len], generated_chunks[:min_len])\n",
        "\n",
        "print(f\"\\nAverage PESQ score (first 10 mins): {avg_pesq:.3f}\" if avg_pesq else \" No PESQ score calculated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCJYWvK89jcK",
        "outputId": "7c09f31e-9fba-44f3-efb6-77454d10b3e1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average PESQ score (first 10 mins): 1.041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PESQ scores between original lecturer's speech (English + Hindi) and transformed (to my voice) Bengali speech"
      ],
      "metadata": {
        "id": "nTLXbuv1-gFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_mp3_to_wav(\"/content/audio.mp3\", \"original_full.wav\")\n",
        "convert_mp3_to_wav(\"/content/bengali_with_my_voice.mp3\", \"generated_full.wav\")"
      ],
      "metadata": {
        "id": "rTH2xrbT9k50"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clip_to_10_minutes(\"original_full.wav\", \"original.wav\")\n",
        "clip_to_10_minutes(\"generated_full.wav\", \"generated.wav\")"
      ],
      "metadata": {
        "id": "Hi4cNanJ-oI-"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_chunks = split_audio(\"original.wav\")\n",
        "generated_chunks = split_audio(\"generated.wav\")"
      ],
      "metadata": {
        "id": "fDS-CDgs-tNf"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_len = min(len(original_chunks), len(generated_chunks))\n",
        "avg_pesq = compute_pesq_on_chunks(original_chunks[:min_len], generated_chunks[:min_len])\n",
        "\n",
        "print(f\"\\nAverage PESQ score (first 10 mins): {avg_pesq:.3f}\" if avg_pesq else \" No PESQ score calculated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLXy5HlZ-v3a",
        "outputId": "b5284851-57eb-4604-cd6d-147f2390e756"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average PESQ score (first 10 mins): 1.282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WER and CER between Google Cloud TTS transcription (base reference) and Whisper transcription"
      ],
      "metadata": {
        "id": "cSvSBPWkf_Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jiwer import wer, cer\n",
        "\n",
        "def compute_error_rates(reference_text, hypothesis_text):\n",
        "    word_error = wer(reference_text, hypothesis_text)\n",
        "    char_error = cer(reference_text, hypothesis_text)\n",
        "    return word_error, char_error\n",
        "\n",
        "# Example usage:\n",
        "with open(\"/content/drive/MyDrive/cleaned_base_transcript.txt\", \"r\", encoding=\"utf-8\") as ref_file:\n",
        "    reference = ref_file.read()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/cleaned_lecture_transcript.txt\", \"r\", encoding=\"utf-8\") as hyp_file:\n",
        "    hypothesis = hyp_file.read()\n",
        "\n",
        "wer_score, cer_score = compute_error_rates(reference, hypothesis)\n",
        "print(f\"WER: {wer_score:.4f}\")\n",
        "print(f\"CER: {cer_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHbDBYmp-w0D",
        "outputId": "d8375213-6267-421d-8cd4-c7950a779f8a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WER: 0.6751\n",
            "CER: 0.4673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xLnRU_ZE_aFQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}